var documenterSearchIndex = {"docs":
[{"location":"ForecastModel/SingleColumnModel/Forcing/#MUSC-Forcing","page":"Forcing","title":"MUSC Forcing","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/Forcing/","page":"Forcing","title":"Forcing","text":"From Eric Bazile: The fields SXXXFORC0001 –> SXXXFORC00NN in the initial file for MUSC are the atmospheric forcing without any rules for variables or advection etc ...","category":"page"},{"location":"ForecastModel/SingleColumnModel/Forcing/","page":"Forcing","title":"Forcing","text":"XXX = vertical levels\nNN  = number of forcing fields","category":"page"},{"location":"ForecastModel/SingleColumnModel/Forcing/","page":"Forcing","title":"Forcing","text":"So if you put for NN=1 the temperature  and QV in 2, and the geostrophic wind ug (3) and Vg (4) and you want  to force MUSC for 48h with a nudging for T and Q and a geostrophic wind you should add in the MUSC namelist ","category":"page"},{"location":"ForecastModel/SingleColumnModel/Forcing/","page":"Forcing","title":"Forcing","text":"NAMCT0\n   LSFORC=T\n   LSFROCS= FALSE ; default ONLY for surface forcing without SURFEX\n/   \n&NAMGFL\n   NGFL_FORC=4 ; number of atmospheric forcing fields\n/\n&NAMLSFORC\n  LGEOST_UV_FRC=.TRUE.,  ; geostrophic wind forcing\n  LMUSCLFA=.TRUE.,\n  NGEOST_U_DEB=3,   ; Ug is in position 3 in GFL_FORC\n  NGEOST_U_NUM=1,   ; ONLY 1 Ug available \n  NGEOST_V_DEB=4,   ; Vg is in position 4 in GFL_FORC\n  NGEOST_V_NUM=1,   ; ONLY one Vg available\n LT_NUDG=.TRUE.,    : Nudging for T\n LQV_NUDG=.TRUE.,   ; Nudging for Qv\n RELAX_TAUT=43200.  ; Relaxation time for Nudging for T\n RELAX_TAUQ=43200.  ; same  for Q\n NT_NUDG_NUM=1      ; Number of nudging profile for T \n NT_NUDG_DEB=1      ; Profile 1 used for the nudging of T\n NQV_NUDG_NUM=1     ; Number of nudging profile for Q\n NQV_NUDG_DEB=2     ; Profile 2 used for nudging Qv\n/","category":"page"},{"location":"ForecastModel/SingleColumnModel/Forcing/","page":"Forcing","title":"Forcing","text":"Here you can run MUSC for 1 day or 100 years with the same nudging profile and geostrophic wind !","category":"page"},{"location":"ForecastModel/SingleColumnModel/Forcing/","page":"Forcing","title":"Forcing","text":"So it is fully flexible BUT the user should know how the initial profile was created and which fields are in FORC00NN etc ....","category":"page"},{"location":"ForecastModel/SingleColumnModel/Forcing/","page":"Forcing","title":"Forcing","text":"After you can have several nudging profile (for several time) instead of one profile used for all the simulation. You just need to put the number of profile For ex you have 5 profiles for T for the nudging at 0, 6, 12 18 24. and if you put the T profile 0 in 1, etc ... the modified namelist","category":"page"},{"location":"ForecastModel/SingleColumnModel/Forcing/","page":"Forcing","title":"Forcing","text":"&NAMGFL\n   NGFL_FORC=8 ; number of atmospheric forcing fields\n/\n&NAMLSFORC\n  LGEOST_UV_FRC=.TRUE.,  ; geostrophic wind forcing\n  LMUSCLFA=.TRUE.,\n  NGEOST_U_DEB=7,   ; Ug is in position 3 in GFL_FORC\n  NGEOST_U_NUM=1,   ; ONLY 1 Ug available \n  NGEOST_V_DEB=8,   ; Vg is in position 4 in GFL_FORC\n  NGEOST_V_NUM=1,   ; ONLY one Vg available\n LT_NUDG=.TRUE.,    : Nudging for T\n LQV_NUDG=.TRUE.,   ; Nudging for Qv\n RELAX_TAUT=43200.  ; Relaxation time for Nudging for T\n RELAX_TAUQ=43200.  ; same  for Q\n NT_NUDG_NUM=5      ; Number of nudging profile for T \n NT_NUDG_DEB=1      ; Profile 1 used for the nudging of T\n NQV_NUDG_NUM=1     ; Number of nudging profile for Q\n NQV_NUDG_DEB=6     ; Profile 2 used for nudging Qv\n NL_T_NUDG_TIME(1) = 0\n NL_T_NUDG_TIME(2) = 21600\nNL_T_NUDG_TIME(3) = 43200\nNL_T_NUDG_TIME(4) = 64800\nNL_T_NUDG_TIME(5) = 86400\n/","category":"page"},{"location":"ForecastModel/SingleColumnModel/Forcing/","page":"Forcing","title":"Forcing","text":"and now you can not run MUSC more than 1 day ... if the time between the forcing profile is the same you can use *_FREQ  instead of TIME ...","category":"page"},{"location":"Overview/Content/#Harmonie-Content","page":"Content","title":"Harmonie Content","text":"","category":"section"},{"location":"Overview/Content/#Overview","page":"Content","title":"Overview","text":"","category":"section"},{"location":"Overview/Content/","page":"Content","title":"Content","text":"Harmonie is HIRLAM's adaptation of the LAM version of the IFS/ARPEGE project. The common code shared with the ALADIN program, Meteo France and ECMWF only contains the source code. Harmonie adds the build environment, scripts, support for a scheduler, and a number of diagnostics tools for file conversion and postprocessing. In summary a git clone of harmonie from github contains the following main directories","category":"page"},{"location":"Overview/Content/","page":"Content","title":"Content","text":"config-sh : Configuration and job submission files for different platforms.\nconst : A selected number of constant files for bias correction, assimilation and different internal schemes. A large number of data for climate generation and the RTTOV software is kept outside of the repository. See [wiki:HarmonieSystemDocumentation#Downloaddata].\necf : Directory for the main configuration file config_exp.h and the containers for the scheduler ECFLOW.\nsuites Scripts and suit definition files for ECFLOW, the scheduler for HARMONIE. \nnam : Namelists for different configurations.\nscr : Scripts to run the different tasks.\nsrc : The IFS/ARPEGE source code.\nutil : A number of utilities and support libraries.","category":"page"},{"location":"Overview/Content/#util","page":"Content","title":"util","text":"","category":"section"},{"location":"Overview/Content/","page":"Content","title":"Content","text":"The util directory contains the following main directories","category":"page"},{"location":"Overview/Content/","page":"Content","title":"Content","text":"auxlibs : Contains gribex, bufr, rgb and some dummy routines\nbinutils : https://www.gnu.org/software/binutils/\nchecknorms : Script for code norm checking\ngl_grib_api : Boundary file generator and file converter\nmakeup : HIRLAM style compilation tool\nmusc : MUSC scripts\nobsmon : Code to produce obsmon sqlite files\noffline : SURFEX offline code\noulan : Converts conventional BUFR data to OBSOUL format read by bator.\nRadarDAbyFA : Field alignment code","category":"page"},{"location":"Observations/Iasi/#IASI-radiances-(pre-)-processing","page":"IASI","title":"IASI radiances (pre-) processing","text":"","category":"section"},{"location":"Observations/Iasi/#Introduction","page":"IASI","title":"Introduction","text":"","category":"section"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"Typical IASI radiance data reception consists of a subset of 366 channels out of the full set of 8461. These cover the infrared absorption spectrum from 3.8 to 15.4 micrometers. In the context of NWP, the most useful IASI channels include (i) the temperature-sounding channels in the approximate channel index range 100-450, (ii) the humidity-sounding channels at 2800-3500 and 5000-5500 indices, and (iii) surface-sensing window channels at 500-1000. Most of the NWP impact from IASI is thought to come from group (i) and especially from the upper-tropospheric and lower-stratospheric channels in the range 200-300.","category":"page"},{"location":"Observations/Iasi/#Including-IASI-radiances-in-a-HARMONIE-run","page":"IASI","title":"Including IASI radiances in a HARMONIE run","text":"","category":"section"},{"location":"Observations/Iasi/#scr/include.ass","page":"IASI","title":"scr/include.ass","text":"","category":"section"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"scr/include.ass should be edited to \"switch on\" the use of AMSUA (AMSU-A), AMSUB (AMSU-B/MHS):","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"export IASI_OBS=1             # IASI\nexport ATOVS_SOURCE=mars       # local: EUMETCast;\nexport IASI_SOURCE=ears        # mars:MARS | else: file in $OBDIR\nexport IASI_RT_COEF=lblrtm     # genln2|kcarta|lblrtm\n[[  $IASI_OBS -eq 1  ]] && types_BASE=\"$types_BASE iasi\"","category":"page"},{"location":"Observations/Iasi/#Loading-the-IASI-radiances","page":"IASI","title":"Loading the IASI radiances","text":"","category":"section"},{"location":"Observations/Iasi/#Data-extracted-from-MARS","page":"IASI","title":"Data extracted from MARS","text":"","category":"section"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"      elif [ \"$base\" = iasi  ] ; then\n      # IASI\n        if [ \"$IASI_OBS\" -eq 1 ]; then\n          echo \"iasi     iasi     BUFR     iasi     \">>batormap\n          ln -sf \"${HM_LIB}\"/const/bator_param/param_bator.cfg.iasi param.cfg\n          if [ \"$IASI_SOURCE\" = mars ] ; then\n            ln -sf \"$WRK\"/splitObs/iasi ./BUFR.iasi\n          else\n            ln -sf $OBSDIR/iasi$DTG ./BUFR.iasi\n          fi\n        fi","category":"page"},{"location":"Observations/Iasi/#Locally-received-data","page":"IASI","title":"Locally received data","text":"","category":"section"},{"location":"Observations/Iasi/#Controlling-the-detection-of-cloud","page":"IASI","title":"Controlling the detection of cloud","text":"","category":"section"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"IASI radiances are strongly influenced by cloud. Because of inaccurate forward modelling, large background errors in cloud fields, and non-linear effects, success in the use of IASI requires careful screening and removal of cloud-affected data. In the HARMONIE data assimilation system, the screening for cloud follows the method of McNally and Watts (2003). The power of this method lies in the use of a large number of individual channels such that much of the disturbing instrument noise can be smoothed out and the cloud radiative effect is therefore more easily detected.","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"Even if the active use of IASI is limited to relatively small number of channels (such as the 55-channel subset in MetCoOp in early 2021), it is advisable to include more than 100 channels in the cloud detection channel list. Furthermore, it is important that all these channels are subjected to VarBC. To achieve the latter, one needs to make sure that blacklisting for the cloud detection channels uses the fail(EXPERIMENTAL) syntax rather than fail(CONSTANT). The following excerpt from src/blacklists/hirlam_blacklist.b.data_selection_after_20140601 illustrates the concept:","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"    if (SENSOR = iasi) then\n\n      ! remove channels that are not used in either cloud detection\n      ! or minimization\n      if PRESS notin (38,   49,   51,   55,   57,   61,   63,   83,   85,   87,\n                     104,  109,  111,  116,  122,  128,  135,  141,  146,  148,\n                     154,  159,  161,  167,  173,  179,  180,  185,  187,  193,\n                     199,  205,  207,  210,  212,  214,  217,  219,  222,  224,\n                     226,  230,  232,  236,  239,  242,  243,  246,  249,  252,\n                     254,  256,  258,  260,  262,  265,  267,  269,  275,  278,\n                     280,  282,  284,  286,  288,  290,  292,  294,  296,  299,\n                     306,  308,  310,  312,  314,  316,  318,  320,  323,  325,\n                     327,  329,  331,  333,  335,  341,  345,  347,  350,  352,\n                     354,  356,  358,  360,  362,  364,  366,  369,  371,  373,\n                     375,  377,  379,  381,  383,  386,  389,  398,  401,  404,\n                     407,  410,  414,  416,  426,  428,  432,  434,  439,  445,\n                     457,  515,  546,  552,  559,  566,  571,  573,  646,  662,\n                     668,  756,  867,  921, 1027, 1133, 1191, 1194, 1271, 1805,\n                    1884, 1946, 1991, 2094, 2239, 2701, 2819, 2910, 2919, 2991,\n                    2993, 3002, 3008, 3014, 3098, 3207, 3228, 3281, 3309, 3322,\n                    3438, 3442, 3484, 3491, 3499, 3506, 3575, 3582, 3658, 4032)\n         then fail(CONSTANT); endif;\n\n      if PRESS notin (38,   51,   63,   85,  104,  109,  167,  173,  180,  185,\n                     193,  199,  205,  207,  212,  224,  230,  236,  239,  242,\n                     243,  249,  296,  333,  337,  345,  352,  386,  389,  432,\n                    2701, 2819, 2910, 2919, 2991, 2993, 3002, 3008, 3014, 3098,\n                    3207, 3228, 3281, 3309, 3322, 3438, 3442, 3484, 3491, 3499,\n                    3506, 3575, 3582, 3658, 4032)\n         then fail(EXPERIMENTAL);\n      endif;\n\n    endif;       ! SENSOR = IASI","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"Here we provide two lists of IASI channels. The first list includes all those channels that are either used in the cloud detection, or are intended for active assimilation (or both). The second list is a subset of the first and includes just those intended for active assimilation. Only those channels included in the latter list will have a significant weight during the assimilation process.","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"The control for the cloud detection happens via namelist file at nam/IASI_CLDDET.NL. The format of the namelist file is illustrated below:","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"&NAMCLDDET\nN__Band_Size(1)=145\nN__Bands(1:145,1) =\n  38,   49,   51,   55,   57,   61,   63,   83,   85,   87,\n 104,  109,  111,  116,  122,  128,  135,  141,  146,  148,\n 154,  159,  161,  167,  173,  179,  180,  185,  187,  193,\n 199,  205,  207,  210,  212,  214,  217,  219,  222,  224,\n 226,  230,  232,  236,  239,  242,  243,  246,  249,  252,\n 254,  256,  258,  260,  262,  265,  267,  269,  275,  278,\n 280,  282,  284,  286,  288,  290,  292,  294,  296,  299,\n 306,  308,  310,  312,  314,  316,  318,  320,  323,  325,\n 327,  329,  331,  333,  335,  341,  345,  347,  350,  352,\n 354,  356,  358,  360,  362,  364,  366,  369,  371,  373,\n 375,  377,  379,  381,  383,  386,  389,  398,  401,  404,\n 407,  410,  414,  416,  426,  428,  432,  434,  439,  445,\n 457,  515,  546,  552,  559,  566,  571,  573,  646,  662,\n 668,  756,  867,  921, 1027, 1133, 1191, 1194, 1271, 1805,\n1884, 1946, 1991, 2094, 2239\n/","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"Here we specify a list of 145 channels to be included in \"band 1\" of the cloud detection, i.e., in the main cloud detection channel band. The setup of the cloud detection involves not just the channel list but several additional tuning parameters that can be modified to make the screening more or less conservative. The default settings are specified in src/arpifs/obs_preproc/cloud_detect_setup.F90. A comprehensive description of the cloud detection scheme, including explanations of the various tuning parameter values, is given at the NWPSAF web site https://nwp-saf.eumetsat.int/site/software/aerosol-and-cloud-detection/documentation/.","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"Log file of the Screening task will indicate whether the formatting of the namelist file is appropriate:","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":" READING CLOUD DETECTION FILE FOR IASI\n IASI  CLOUD DETECTION FILE READ OK","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"In case of an error, the following is printed instead:","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":" READING CLOUD DETECTION FILE FOR IASI\n PROBLEM READING IASI CLOUD DETECTION FILE: Using Default Values","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"The third possibility is that the namelist file does not appear in the working directory, in which case the printout statement is this:","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":" READING CLOUD DETECTION FILE FOR IASI\n NO IASI  CLOUD DETECTION FILE : Using Default Values","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"Please note that the use of the \"Default Values\" is generally not a desired outcome. This is because many of the cloud detection channels in the default list (see src/arpifs/obs_preproc/cloud_detect_setup.F90) are sensitive to higher stratosphere and therefore may be severely affected by the relatively low model top of limited-area HARMONIE systems.","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"References:","category":"page"},{"location":"Observations/Iasi/","page":"IASI","title":"IASI","text":"McNally, AP, and PD Watts, 2003: A cloud detection algorithm for high-spectral-resolution infrared sounders. Quarterly Journal of the Royal Meteorological Society, 129, 3411-3423, doi:10.1256/qj.02.208.","category":"page"},{"location":"Verification/Verification/#HARMONIE-Verification","page":"Verification","title":"HARMONIE Verification","text":"","category":"section"},{"location":"Verification/Verification/#Introduction","page":"Verification","title":"Introduction","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The verification package in HARMONIE is designed to be a self contained stand alone package dealing with pre-extracted model and observational data. The package calculates several standard verification scores such as:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Error as function of forecast lead time summarises the bias and rms­ error and their growth rate over a set of forecasts\nTime sequences and vertical profiles show how your data or error characteristic is distributed in time or in the vertical\nError charts and tables show how some error is distributed in space, and station­wise linear correlation\nScatter plots show the correspondence between forecast and observed values\nMean diurnal cycles show how your mean error changes in the course of the day\nHistograms show the correspondence between the distributions of forecast and observed values\nStudent t-test to show how reliable differences between different experiments are","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"In addition there are a number of scores based on contingency tables like:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Frequency bias (bias score): (h+fa)/(h+m); Compares the frequency of predicted events to the frequency of observed events. Range: 0 -> infinite. Perfect score: 1\nHit rate (probability of detection): h/(h+m); What fraction of the observed events were correctly forecast. Range: 0 to 1.  Perfect score: 1. \nFalse alarm ratio: fa/(h+fa); What fraction of the predicted events did not occur. Range: 0 to 1.  Perfect score: 0.\nFalse alarm rate: fa/((cn+fa); What fraction of the observed \"no\" events were incorrectly forecast as \"yes\". Range: 0 to 1. Perfect score: 0.\nThreat score: h/(h+m+fa); How well did the forecast \"yes\" events correspond to the observed \"yes\" events. Range: 0 to 1. Perfect scrore: 1.\nThe Equitable threat score takes into account the number of random hits (R) and is less sensitive to climatology: ETS=(h­R)/(h+m+fa­R),  R=(h+m)(h+fa)/(h+m+fa+cn). Often used in verification of precipitation. Range: -1/3 to 1, 0 indicates no skill.   Perfect score: 1. \nHansen­Kuipers score: (h/(h+m) ­ fa/(fa+cn)), How well did the forecast separate events from non­events. Range: -1 to 1, 0 indicates no skill. Perfect score: 1.\nExtreme Dependency Scores: What is the association between forecast and observed rare events? Range: -1 to 1, 0 indicates no skill. Perfect score: 1","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"A more detailed explanation about verification can found here","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The scores can be presented per station for the whole data set or filtered through different selection criteria based on e.g. a geographical domain or properties of the data itself. One key feature missing in earlier HIRLAM verification packages is that the comparison is done over exactly the same set of data ( in time and space ) when comparing different experiments or models. The scores are finally presented with a portable web interface, [#WebgraF WebgraF], that allows you to easily share the information with others. Since the verification is station based it is less suitable for moving platforms or fields.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Other examples on how products from the verification package looks like today can be found here:     * The monitor test data set     * FMI     * Mast verification     * HIRLAM verification portal","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"In the following we describe the different parts of the verification package. For preparation of verification data read more here.","category":"page"},{"location":"Verification/Verification/#Getting-and-compiling-the-code","page":"Verification","title":"Getting and compiling the code","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The verification code can be fetched from the hirlam code repository by","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"git clone git@github.com:Hirlam/Monitor.git\ncd Monitor","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"For a specific version do ","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":" git ls-remote\nFrom git@github.com:Hirlam/Monitor.git\n6ff572139f3ed438a073c1ca043f6f227a78a606\tHEAD\neceb5f6f9181a7bea4d811e6359b68ef3f82db9e\trefs/heads/gh-pages\n6ff572139f3ed438a073c1ca043f6f227a78a606\trefs/heads/master\n20dcc868000e94e3f3178e38cf661befff6d9e03\trefs/tags/v1.0\n2989b8ff40de90abb2dde1521350107e2770aef5\trefs/tags/v1.1\n...\n\ngit checkout vX.X\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The checkout gives you the following directories:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"cmastat: for assimilation monitoring, not covered here\nconfig: configuration files for different platforms\ndoc: README files\nmod: Different modules. module_data.f90 contains all namelist variables\nprg: Main programs\nrdr: Routines for reading data\nscr: Scripts\nsrc: The hard core verification routines\ntools: Compilation tools\nWebgraF: The web interface","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"You compile by finding the appropriate configuration file for your platform and type","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"gmake ARCH=YOUR_ARCH","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The package has no external dependencies but relies on gnuplot for generation of the graphics.","category":"page"},{"location":"Verification/Verification/#The-verification-step-by-step","page":"Verification","title":"The verification step by step","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The strategy in the verification is to separate the data input from the calculations of the different scores. This allows as to go through the data several times using different filtering criteria. Of course keeping everything in memory sets a limit on how much data one can handle at the same time. In the way it's used in HARMONIE a typical sequence is:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Read namelist\nCall my_choices\nRead observations\nRead model data\nPerform quality control\nLoop over all namelists\nSelection of data \nCalculate the scores and write data\nCheck for new namelist","category":"page"},{"location":"Verification/Verification/#Reading-the-data","page":"Verification","title":"Reading the data","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The program can handle several data sources. Which one you use is depending on the value of DATA_SOURCE and is controlled in the routine monitor/rdr/my_choices.f90. At namelist level we also control which experiments we should read, the period (SDATE,EDATE), interval between cycles (FCINT), which forecasts (FCLEN) and the interval of the observations (OBINT). We can also already at this point select which stations to use by specifying a station list (STNLIST). ","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The HARMONIE tools to extract data for verifiation are described in here.","category":"page"},{"location":"Verification/Verification/#A-general-input-format","page":"Verification","title":"A general input format","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"For the every day verification the model and observation data are read with the routines ","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"monitor/rdr/read_vfld.f90, monitor/rdr/read_vobs.f90, monitor/rdr/read_vfld_temp.f90, monitor/rdr/read_vobs_temp.f90 Where the two first are for surface data and are used when DATA_SOURCE=vfld and the two latter for temp data and are used when DATA_SOURCE=vfld_temp. During the evolution of the verification package the format of the input data has changed and we are now at version four. The new format allows an arbitrary number of different types of point data to be included in the model vfld- or observation vobs- files.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The generalized input format is defined as ","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"nstation_synop nstation_temp version_flag  # written in fortran format '(1x,3I6)' )\n# where version_flag == 4\n# If ( nstation_synop > 0 ) we read the variables in the file, their descriptors and\n# their accumulation time\n#\nnvar_synop\nDESC_1 ACC_TIME_1\n...\nDESC_nvar_synop ACC_TIME_nvar_synop\n# Station information and data N=nstation_synop times\nstid_1 lat lon hgt val(1:nvar_synop)\n...\nstid_N lat lon hgt val(1:nvar_synop)\n\n# If ( nstation_temp > 0 )\nnlev_temp\nnvar_temp\nDESC_1 ACC_TIME_1\n..\nDESC_nvar_temp ACC_TIME_nvar_temp\n# Station information and data nstation_temp times\n# and station data nlev_temp times for each station\nstid_1 lat lon hgt\npressure(1) val(1:nvar_temp)\n...\npressure(nlev_temp) val(1:nvar_temp)\nstid_2 lat lon hgt\n...","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The accumulation time allows us to e.g. easily include different precipitation accumulation intervals. Any variable can be included in the file and verified without any code changes. Once you have defined a variable in your data you have to describe its properties in the monitor/scr/plotdefs.pm described in the  parameter settings ","category":"page"},{"location":"Verification/Verification/#Quality-control","page":"Verification","title":"Quality control","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The quality control is activated by the namelist flag LQUALITY_CONTROL. It is mainly there as a gross error check to remove the unrealistic observations. The check has the following features:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The forecast lengths used for quality control can be set by namelist variable QC_FCLEN. If QC_FCLEN is not set The forecasts < FORECAST_INTERVAL will be used.\nAn observation is accepted if ABS(mod - exp) < err_limit for ANY experiment used in the verification. \nThe quality control limits can be set explicitly in namelist by VARPROP%LIM for any variable or by QC_LIM for all variables. See the section about parameter settings for further instructions.\nBy setting ESTIMATE_QC_LIMIT the QC_LIM will be set as SCALE_QC_LIM * STDV for the forecasts in QC_FCLEN.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"QC diagnostic information output may be controlled by PRINT_QC={0,1,2}. It is also possible to blacklist stations through the STNLIST_BL parameter.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"In the HARMONIE implementation all quality control levels are estimated on the fly with a limit where the standard deviation is scaled by 5.","category":"page"},{"location":"Verification/Verification/#The-selection-process","page":"Verification","title":"The selection process","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Before we run through the actual comparison of data we can select a subset of the data depending on different criteria.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Initial time of the forecast (INI_HOURS)\nSelect data that are valid at a certain hour (SHOW_TIMES) \nPick only some of the forecast lengths in memory (USE_FCLEN)\nSelection using a station list (STNLIST)\nSelect to produce statistics for some selected stations in addition to the overall statistics (STNLIST_PLOT)\nDefine a geographical box through the definition of the corners (CBOX%ACTIVE,CBOX%S,CBOX%W,CBOX%N,CBOX%E).\nUse an area defined by a polygon (LPOLY,POLYFILE)\nSelect by station height (HGT_LLIM,HGT_ULIM)\nConditions given by the data (described later)\nReverse all selections but the conditional (REVERSE_SELECTION)","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Of course all the selections can be combined.","category":"page"},{"location":"Verification/Verification/#The-verification-loop","page":"Verification","title":"The verification loop","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"For each station we loop over all selected stations, initial times, forecast lengths for the given period and accumulate the statistics. Data is stored by:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Statistic by forecast length or time of day, for each station and accumulated for all stations and is used to generate error maps(MAP), vertical profiles(VERT), standard forecast length verification(GEN) and daily variation scores (DAYVAR)\nTime serie arrays for each station and accumulated for all stations (TIME)\nOne array containing all the selected data used for scatter plots (SCAT) and contingency table calculations (CONT). From the contingency table we can calculate several different scores described later.","category":"page"},{"location":"Verification/Verification/#Output-format","page":"Verification","title":"Output format","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Early versions of the package was based on the ECMWF graphics package MAGICS. Due to the poor portability of MAGICS the package now a days produces text files that are parsed through monitor/scr/verobs2gnuplot.pl that produces plots using gnuplot. It may not be the most elegant graphics package, but it is available almost everywhere. Some verification are also produced in form of tables. The contingency tables are parsed through monitor/scr/contingency2gnuplot.pl to produce skill scores.","category":"page"},{"location":"Verification/Verification/#HARMONIE-user-interface","page":"Verification","title":"HARMONIE user interface","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"In HARMONIE a set of scripts is build around the code for generation of plots and building the web page. There are two main scripts monitor/scr/Run_verobs_surface for verification of surface variables and monitor/scr/Run_verobs_temp for verification of radio sonde data. Both of them need a configuration file, Env_exp, as input:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"./Run_verobs_surface MY_ENV_EXP\n./Run_verobs_temp MY_ENV_EXP","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"There is also a master script monitor/scr/Run_verobs_all which cleans the webpage, runs through both types of verification and creates a tar file suitable to add to an existing WebgraF page. It is used in the same way like:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"./Run_verobs_all MY_ENV_EXP","category":"page"},{"location":"Verification/Verification/#The-main-configuration-file","page":"Verification","title":"The main configuration file","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"For most of the cases you can configure your verification by just editing the configuration file, monitor/scr/Env_exp. The above mentioned script can take files with any name so it's a good idea to have different files for different sets of experiments. First you have to identify your experiments and their location. ","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"# Experiment names and paths,should be space separated\nDBASE=/scratch/ms/dk/nhz/oprint\nEXP=\"RCR C22\"\nDISPLAY_EXP=\"$EXP\"\nOBSPATH=$DBASE/OBS2/\nP1=$DBASE/RCR/\nP2=$DBASE/C22/\nMODPATH=\"$P1 $P2\"","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The experiment name should of course match the name on the vfld files. At the moment there is an upper limit of ten experiments, but already at five the plots starts to get pretty messy. It is possible to disply more meaningful names by setting DISPLAY_EXP to something different.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"From harmonie-40h1.1.1.rc1 the output from the verification for the initial setup has changed so that ","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"vfldEXPYYYYMMDDHH represents analysis data\nvfldEXPYYYYMMDDHHLL represents output from the forecast model at +00","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"To handle this USE_ANALYSIS has been introduced to allow different choices and combination of the new and old convention.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n# Use analysis at +00h, set per experiment\nUSE_ANALYSIS=\".FALSE.,.TRUE.,.FALSE.,.TRUE.,.TRUE.\"","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"We should now defined the name on the WebgraF page and write some text ( in simple html syntax ) describing the experiments.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n# Project name, will be the name on the web page\nPROJECT=monitor\n\n# Explanation on webpage\nHELP=\"Observation verification comparison between \\\n      <br> FMI(RCR) \\\n      <br> SMHI(C22) \\\n     \"","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"By having several configuration files with different PROJECT names you can gather all your verification plots under one web page. In next section we define the verification period. We can also say if we would like to verify the full period in one go or in monthly pieces by setting PERIOD_TYPE.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"# Date handling\n# PERIOD_TYPE 1 : SDATE - EDATE,\n#             2 : SDATE - EDATE in monthly pieces\n#\n# IDATE is the very first date for PERIOD_TYPE=2 it determines the\n# lentgh of the date menu in WebgraF\n#\nPERIOD_TYPE=1\n\nSDATE=20080901\nEDATE=20080905\nIDATE=$SDATE","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"For operational runs it might be useful to set PERIOD_TYPE=2 like FMI has done.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"If you would like to monitor some special stations you can list them by station number.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n#\n# Single stations can be defined with comma separated\n# station number and a text for the web page\n#\n# STNLIST_PLOT=\"00002574,00006348\"\n# STNLIST_PLOT_TXT=\"NORRKOPING,CABAUW\"\n#\nSTNLIST_PLOT=-1\nSTNLIST_PLOT_TXT=-1\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Note that the zeros in the name matters since the plots are created with station id as eight digit numbers. A list of parameters to be verified are selected through SURFPAR and TEMPPAR respectively. ","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n######################\n# Surface parameters #\n######################\n#\n# Change in the file plotdefs.pm for text and limits\n#\n# PS : Mslp\n# TT : T2m\n# TTHA : T2m, adjusted for model and observation station height differences\n# TN : Min T2m\n# TX : Max T2m\n# TD : Td2m\n# FF : Wind speed\n# FX : Max wind speed\n# GG : Wind gust\n# GX : Max wind gust\n# DD : Wind direction\n# QQ : Specific humidity\n# RH : Relative humidity\n# PE : Precipitation\n# NN : Total Cloud cover\n# VI : Visibility, not in vfld files yet\n#\n\n# Active parameters\nSURFPAR=\"PS FF FX GG GX DD TT TN TX TD RH QQ NN PE\"\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Note that not all parameters are available in the vfld files for HARMONIE yet. The number of levels to be used for TEMP is set in LEV_LST.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Comment in the code fldextr_pp.f explains TTHA, the moist adiabatic adjustment:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"c adiabatic height correction of station values\nc T2M_corr=T2M+[(STATION_HEIGHT-MODEL_HEIGHT)*ADIABATIC_LAPSE_RATE]\nc ex: STATION_HEIGHT = 400 masl; MODEL_HEIGHT = 500 masl T2M = 10\nc     T2M_corr=10+(400-500)*(-0.0065)=10+0.6=10.6","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"By setting SURFPLOT and TEMPPLOT we choose what kind of statistics we would like to produce. ","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"# Things to plot:\n# GEN    : General forcast length statistics\n# TIME   : Timeserie statistics\n# SCAT   : Scatterplot\n# MAP    : Bias maps\n# FREQ   : Frequency plots\n# DAYVAR : Daily variation\n# XML    : Station statistics in xml format\n# CONT   : Contingency tables\n# VERT   : Vertical profiles only available for TEMP data\n# SEAS   : Seasonal cycle\n#\nSURFPLOT=\"GEN TIME MAP FREQ SCAT CONT XML DAYVAR\"","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"From the contingency tables we are also able to produce a number of skill scores either defined by their classes or thresholds.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"# Select skill scores to be plotted if CONT is activated in SURFPLOT\n# Frequency     : Frequency\n# Frequencybias : Frequency bias\n# POD           : Probability of detection ( hit rate )\n# FAR           : False alarm ratio\n# FA            : False alarm rate\n# TS            : Threath score\n# WILSON        : Wilson diagram, a combination of POD, TS, FAR and frequency bias\n# KSS           : Hansen-Kupiers skill score\n# AI            : Area index\n# EDS           : Extreme Dependency Score\n# SEDS          : Symmetric Extreme Dependency Score\n# EDI           : Extremal Dependency Index\n# SEDI          : Symmetric Extremal Dependency Index\n# ETS           : Equitable threat score\n\nSCORELIST=\"WILSON KSS Frequency\"\n\n# Select whether skill scores are based on classes and/or thresholds (CONT must be activated)\nSCORETYPES=\"classes thresholds\"\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The meaning of the different abbreviations will be given in next section.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"A selection of data is done by SURFSELECTION and TEMPSELECTION. The name in these list refers to definitions in monitor/scr/selection.pm. We also select time and forecast interval by the OBINT, FCINT and FCLEN parameters.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"At the end you would possibly like to change the graphics format of the output files.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n#####################\n# GRAPHICS and misc #\n#####################\n\n# Select output_type\n# 1  Postscript + PNG\n# 2  PNG\n# 3  JPEG\n# 4  SVG\nOUTPUT_TYPE=2\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The difference between the first OUTPUT_TYPE and the others is that in the first case gnuplot will produce postscript files that will be converted to PNG files and both files will be available on the web page. The SVG format (Scalable Vector Graphics) should allow plots with zoom functionality, but this does not yet work in the web interface. ","category":"page"},{"location":"Verification/Verification/#Setting-parameters-for-different-types-of-plots","page":"Verification","title":"Setting parameters for different types of plots","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"If the settings in the main configuration file does not cover your needs you go to next level of the definition files. The namelists defining your verification run is build by monitor/scr/Build_namelist.pl by using your configuration file and three perl modules defining different parts. The logics behind the GEN, MAP, TIME switches are hidden in monitor/scr/maindefs.pm. The first part defines the reading part:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n%nameread=(\n 'read_section' => {\n    'SDATE'   => $ENV{SDATE},\n    'EDATE'   => $ENV{EDATE},\n    'NEXP'    => $nexp,\n    'EXPNAME' => $exp,\n    'MODPATH' => $modpath,\n    'OBSPATH' => '\\''.$ENV{OBSPATH}.'\\'',\n    'LQUALITY_CONTROL' => 'T',\n    'ESTIMATE_QC_LIMIT'=> 'T',\n    'MAXSTN'           => 5000,\n    'STNLIST'          => 0,\n    'STNLIST_PLOT'     => $ENV{STNLIST_PLOT},\n    'LVERIFY'          => 'F',\n    'PRINT_READ'       => 1,\n    'PERIOD_TYPE'      => $ENV{PERIOD_TYPE},\n    'OUTPUT_TYPE'      => $ENV{OUTPUT_TYPE},\n    'OUTPUT_MODE'      => 2,\n },\n) ;\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Any new variable added here will also be set in the reading part of the namelist. The next part, def, defines values that are reset every time we loop through the verification and reads a new namelist. In the selectionloop part  we find the magic switches for the different SURFPLOT/TEMPPLOT keywords. The first one SEAS is only interesting if you run with a few parameters over several seasons. In the normal case several years of data doesn't fit in the memory so this is left for the experienced user.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The next one is GEN:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n 'GEN' => {\n # Fclen plots\n 'LSTAT_GEN'  => 'T',\n 'LSIGN_TEST' => 'T',\n 'SIGN_TIME_DIFF' => '-1',\n 'LPLOT_STAT' => 'T',\n 'SHOW_BIAS' => 'T',\n 'SHOW_RMSE' => 'T',\n 'SHOW_VAR'  => 'T',\n },\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Here we set things for standard forecast length verification. We can choose to show bias, rmse and stdv by the SHOW_* variable. If SHOW_VAR is set plots comparing the model variability to the observed one will be produced. The significance of the difference between different experiments can be shown by setting LSIGN_TEST.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The next section handles the production of bias maps","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n 'MAP' => {\n # Map plots\n 'PLOT_BIAS_MAP' => 'T',\n 'PLOT_RMSE_MAP' => 'T',\n 'LSTAT_GEN'     => 'T',\n 'LPLOT_STAT'    => 'F',\n 'LFCVER'        => 'F',\n 'SHOW_TIMES'    => '00,12',\n 'USE_FCLEN' => join(',',split(' ',$ENV{FCLEN_MAP})),\n },\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Here we can show decide to show any of bias,rmse and stdv maps. The bias intervals for a given parameter are defined in monitor/scr/plotdefs.pm discussed later. Here we have chosen to show only the 00 and 12 UTC maps.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Time serie statistics of the observed values and departures are produced by the TIME section.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n 'TIME' => {\n  # Timeseries\n  'LTIMESERIE_STAT'=> 'T',\n  'USE_FCLEN' => join(',',split(' ',$ENV{FCLEN_TIME})),\n },\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Note that we explicitly set the forecast lengths we use. As for the GEN part the activation of bias, rmse and stdv plots are controlled by the SHOW_* parameters. The averaging period for time series are controlled per variable through the TWIND_SURF and TWIND_TEMP parameters in monitor/scr/plotdefs.pm.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Scatter plots and contingency tables are set in","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":" 'scat_ver' => {\n  # Scatterplots and freq,\n  'LPREP_XML' => 'T',\n  'LPLOT_FREQ'=> 'T',\n  'LPLOT_SCAT'=> 'T',\n  'USE_FCLEN' => join(',',split(' ',$ENV{FCLEN_SCAT})),\n },\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"By setting LPREP_XML we will get a list of stations sorted by decreasing rmse on the web page. This allows you to find the worst stations for different variables. The contingency part of this is defined in monitor/scr/plotdefs.pm. It is possible to create cross variable scatter plots where we compare different model parameters against each other or the observations. This is however not a part of the script system but be defined on the low level. Read more` in here.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"In some cases it's interesting to see how the model handles the daily cycle. In DAYVAR we define the flags to get this. The LFCVER tell the program that we should organize the statistics by time of day rather than by forecast length. We have also chosen to allow for a special set of forecast length here through the environment variable FCLEN_DAYVAR set in your configuration file","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n 'DAYVAR' => {\n  # Daily variation\n  'LPLOT_STAT' => 'T',\n  'LSTAT_GEN'  => 'T',\n  'LFCVER'     => 'F',\n  'USE_FCLEN'  => join(',',split(' ',$ENV{FCLEN_DAYVAR})),\n  'SHOW_OBS'   => 'T',\n  'SHOW_VAR'   => 'F',\n },\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The final part of monitor/scr/maindefs.pm deals with the vertical profiles. LPLOT_VERT is the flag telling us that we are doing a vertical profile. The major difference between this and GEN is that here we have chosen to split between night and daytime soundings by setting SHOW_TIMES.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Any valid namelist variable added to these sections will be picked up and used in the verification. ","category":"page"},{"location":"Verification/Verification/#Settings-for-different-meteorological-parameters","page":"Verification","title":"Settings for different meteorological parameters","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The different treatment of the different meteorological variables are done in monitor/scr/plotdefs.pm. In the first section we define the default values for all variables.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n 'def'=>{\n   'TWIND_SURF' => 06,\n   'TWIND_TEMP' => 12,\n   'QC_LIM_SCALE' => 5.,\n   'MAP_BIAS_INTERVAL'=> '7*-1',\n },\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The TWIND_* parameter sets the time averaging window for time series for surface data and temp respectively. QC_LIM_SCALE is the scaling factor for the stdv used in the quality control. The TEXT variables sets the text for the title in the plots and the web page. For e.g. the cloud cover we have defined the classes for the contingency tables by setting CONT_CLASS and CONT_LIM. MAP_BIAS_INTERVAL set, as the name indicates, the intervals for the bias maps.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n'NN'=>{\n   'TEXT'        => 'Cloud cover',\n   'CONT_CLASS'  => 7,\n   'CONT_LIM'    => '1.,2.,3.,4.,5.,6.,7.',\n   'PRE_FCLA'    => '1.,2.,3.,4.,5.,6.,7.',\n   'MAP_BIAS_INTERVAL'=> '-6.,-4.,-2.,0.,2.,4.,6.',\n },\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"It is also possible to set the time window for timeseries separately for each variables like it is done for e.g. precipitation (PE). For accumulated and max/min parameters we also need to set the accumulation period. E.g. the maximum temperature for the past 12 hours is defined as","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n 'TX'=>{\n   'TWIND_SURF'  => 12,\n   'TEXT'        => 'Max T2m',\n   'MAP_BIAS_INTERVAL'=> '-6.,-4.,-2.,0.,2.,4.,6.',\n   'ACC'         => 12,\n   'ACCTYPE'     => 3,\n },\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Where ACCTYPE defines if it's an accumulated (1), minimum (2) or maximum (3)  parameter.","category":"page"},{"location":"Verification/Verification/#Defining-a-new-verification-parameter","page":"Verification","title":"Defining a new verification parameter","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"As described in the [#Ageneralinputformat input format] section you can add any variable to the verification. Let's say you would like to verify precipiation accunulated over three hours and that you have called it PE3H in your data files.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n 'PE3H'=>{\n   'TEXT' => 'Precipitation 3h',\n   'ACC'  => 3,\n   'UNIT' => 'mm/3h',\n },\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Where TEXT is the description to be displayed on the plot and the webpage, ACC is the accumulation period in hours and UNIT is the unit to be written to the plot. Of course the above mentioned properties can be set as well. Remember to add PE3H to the SURFPAR list in your definition file.","category":"page"},{"location":"Verification/Verification/#Selection-options","page":"Verification","title":"Selection options","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"It easy to select a subset of your data for verification and we have already discussed how it can be done by setting a list of stations or select different forecast lengths. In monitor/selection.pm a number of different kind of selections have been defined. Several of them are just a list of stations like e.g. the well known (but perhaps not so well defined) EWGLAM list. An example of how a box can be defined is found for the Netherlands.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n 'Netherland' => {\n   'CBOX%ACTIVE' => 'T',\n   'CBOX%SLAT' => '51.',\n   'CBOX%WLON' => '1.5',\n   'CBOX%NLAT' => '54.5',\n   'CBOX%ELON' => '9.',\n   }\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"We may also define our area of selection through a polygon.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n 'BalticSea' => {\n   'STNLIST'=> 0,\n   'LPOLY'               => 'T',\n   'POLYFILE'            => '\\'Baltic_sea.poly\\'',\n   },\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"A more specialized case is the selection of stations by station height where the upper and lower limits are given.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n  'masl_300m' => {\n   'STNLIST'=> 0,\n   'LSTN_HGT_CHECK'=>'.T.',\n   'HGT_ULIM'=>1.e6,\n   'HGT_LLIM'=>300.,\n   },\n","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Finally we can also do the selection based on meteorological criteria. In the example below we are interested in cases with low temperatures and weak winds.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"\n  'temp_and_wind_limit' => {\n    'COND%IND' => 'FF,TT',\n    'COND%ULIM' => ' 5.,-10.',\n    'COND%LLIM' => '-1.,-55.',\n    'COND%LOBS' => 'T,F',\n    'COND%ALL_MOD' => 'T,F',\n  },","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Example of conditional selection","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"COND%IND sets the parameters\nCOND%LLIM sets the lower limits\nCOND%ULIM sets the upper limits\nCOND%LOBS T means apply condition on observations, F applies condition on model data\nCOND%ALL_MOD, T means condition is required for ALL models, F for ANY model","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"All the above mentioned selections can of course be combined in any way you can imagine.","category":"page"},{"location":"Verification/Verification/#WebgraF","page":"Verification","title":"WebgraF","text":"","category":"section"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"One idea with the HARMONIE verification packages is that it should be easy to share you results with others. This is where WebgraF comes in. It was originally written to mimic the ECMWF \"chart\" facility like here. The ECMWF solution is a perl based server solution and needs some installation and WebgraF is a javascript running locally which makes it more portable. The idea with WebgraF is that each page is defined by a simple definition file which spans the space of the menu axes on the page. ","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Examples :       * GLAMEPS Definition file       * Daily maps Definition file","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"At the end of both of the scripts Run_verobs_surface/Run_verobs_temp there is a call to monitor/scr/Create_ver_js.pl that builds the webpage depending on your configuration file. It is also possible to (re)generate the webpage directly by running:","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Create_ver_js YOUR_CONFIG_FILE","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The WebgraF page is controlled by monitor/WebgraF/bin/WebgraF. It has commands to e.g. list, add, remove the content of a page. To start mastering your own page you first have to let the script know the location of the page by setting the environment variable WEBGRAF_BASE ","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"# in bash\nexport WEBGRAF_BASE=SOME_PATH/monitor/WebgraF\n# or in tcsh\nsetenv WEBGRAF_BASE SOME_PATH/monitor/WebgraF","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Now you can list the content of you page by","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"WebgraF/bin/WebgraF -l ","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"A more comprehensive list of commands can be found in monitor/doc/README_WebgraF. The rules and functions available for your definition file is found monitor/WebgraF/src/input.html.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Two useful tools is the export and transport commands. Both creates an portable extraction of your verification page but in two different ways.","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"The export.tar file is a stand alone web page that you can untar anywhere and open in your browser.\nThe transport.tar file is suitable to add to an already existing WebgraF page by  WebgraF -a TARFILE","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Both are accessible through the script monitor/scr/Transport_ver which is used like","category":"page"},{"location":"Verification/Verification/","page":"Verification","title":"Verification","text":"Transport_ver YOUR_CONFIG_FILE","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Experiment-configuration","page":"Experiment","title":"Experiment configuration","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Introduction","page":"Experiment","title":"Introduction","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"There are several levels on configuration available in HARMONIE. The highest level of configuration is done in ecf/config_exp.h. It includes the environment variables, which are used to control the experimentation. In the following we describe the meaning of the different variables and are described in the order they appear in ecf/config_exp.h.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Host specific paths and environment variables for your system are defined in Env_system. Read more here.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Build-options","page":"Experiment","title":"Build options","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** Build and bin paths ****\n# Definitions about Build, should fit with hm_rev\nBUILD=${BUILD-yes}              # Turn on or off the compilation and binary build (yes|no)\nBUILD_WITH=${BUILD_WITH-makeup} # Which build system to use (makeup|cmake)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"BUILD is a switch for compiling HARMONIE code (yes|no) and BUILD_WITH controls which build system to use when compiling HARMONIE-AROME.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"BINDIR=${BINDIR-$HM_DATA/bin}                 # Binary directory","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"BINDIR is the location of where your HARMONIE binaries will be installed. You can use this to point to binaries outside of your experiment. A few other options for non default configurations exists as well:","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"COMPILE_ENKF=${COMPILE_ENKF-\"no\"}             # Compile LETKF code (yes|no)\nCOMPILE_DABYFA=${COMPILE_DABYFA-\"no\"}         # Compile FA/VC code (yes|no)\nSURFEX_OFFLINE_BINARIES=\"no\"                  # Switch to compile and use offline SURFEX binaries","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#General-settings","page":"Experiment","title":"General settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** Misc, defined first because it's used later ****\n\nCNMEXP=HARM                             # Four character experiment identifier\nWRK=$HM_DATA/$CYCLEDIR                  # Work directory","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"CNMEXP: experiment identifier used by MASTERODB\nWRK is the work directory. The suggested path on ECMWF.atos is $SCRATCH/hm_home/${EXP}/$CYCLEDIR","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Archive-settings-(ECMWF)","page":"Experiment","title":"Archive settings (ECMWF)","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Since $SCRATCH is cleaned regularly on ECMWF some files are transferred to ECFS for a more permanent storage by the scripts scr/Archive_host1 and scr/Archive_ecgate. ","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** Paths to archive ****\n# We need to define ARCHIVE early since it might be used further down\n\nARCHIVE_ROOT=$HM_DATA/archive           # Archive root directory\nECFSLOC=ectmp                           # Archiving site at ECMWF-ECFS: \"ec\" or ECFS-TMP \"ectmp\"\nECFSGROUP=hirald                        # Group in which to chgrp the ECMWF archive, \"default\" or \"hirald\"\nEXTRARCH=$ARCHIVE_ROOT/extract          # Archive for fld/obs-extractions","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"ARCHIVE_ROOT is the path to forecast file archive. Note that at ECMWF this directory is not a permanent storage\nEXTRARCH is the path to field extraction archive. Note that at ECMWF this directory is not a permanent storage\nECFSLOC Archiving site at ECMWF-ECFS  (ectmp|ec) Note that files archived on ectmp will be lost after 90 days. If you wish your files to stay longer you should set ECFSLOC=ec. \nECFSGROUP Group in which to chgrp the ECMWF archive, (hirald|default)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Running-Mode","page":"Experiment","title":"Running Mode","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** Running mode ****\nRUNNING_MODE=research                   # Research or operational mode (research|operational)\n                                        # operational implies that the suite will continue even if e.g.\n                                        # observations are missing or assimilation fails\n\nSIMULATION_TYPE=nwp                     # Type of simulation (nwp|climate)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"RUNNING_MODE can be research or operational. Operational is more forgiving in the error handling and e.g. the assimilation will be skipped if Bator doesn't find any observations. Exceptions handled by the operational mode are written to $HM_DATA/severe_warnings.txt\nSIMULATION_TYPE Switch between nwp and climate type of simulation. The climate simulations are still in an experimental stage. ","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Model-domain-settings","page":"Experiment","title":"Model domain settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Horizontal domain settings. Further information is available here","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"\n# **** Model geometry ****\nDOMAIN=DKCOEXP                          # See definitions in scr/Harmonie_domains.pm\nTOPO_SOURCE=gmted2010                   # Input source for orography. Available are (gmted2010|gtopo30)\nGRID_TYPE=LINEAR                        # Type of grid (LINEAR|QUADRATIC|CUBIC)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"DOMAIN defines your domain according to the settings in scr/Harmonie_domains.pm (DKCOEXP). The spectral truncation for your domain is determined from NLON and NLAT by scr/Harmonie_domains.pm. Further information on model domains are available here\nTOPO_SOURCE: Defines input source for model orography (gmted2010|gtopo30). Further information available here: hi-res topography\nGRID_TYPE: This variable is used to define the spectral truncation used (LINEAR|QUADRATIC|CUBIC). GRID_TYPE is used in scr/Climate and scr/Forecast","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Vertical-levels","page":"Experiment","title":"Vertical levels","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Set the number vertical levels to use. Further information is available here","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"VLEV=65                                 # Vertical level definition name\n                                        # HIRLAM_60, MF_60,HIRLAM_40, or\n                                        # BOUNDARIES = same number of levs as on boundary file.\n                                        # See the other choices from scr/Vertical_levels.pl","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"VLEV is the name of the vertical levels defined in scr/Vertical_levels.pl (65). Further information is available here. If you intend to run upper air assimilation you must select the same domain and level definition for which you have derived structure functions. Read more Structure Functions","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Forecast-model","page":"Experiment","title":"Forecast model","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Higher level forecast model settings.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** High level forecast options ****\nNAMELIST_BASE=\"harmonie\"                # Input for namelist generation (harmonie|alaro1)\n                                        #   harmonie : The default HARMONIE namelist base nam/harmonie_namelists.pm\n                                        #   alaro1   : For ALARO-1 baseline with only a few configurations available\n                                        #              nam/alaro1_namelists.pm\nDYNAMICS=\"nh\"                           # Hydrostatic or non-hydrostatic dynamics (h|nh)\nVERT_DISC=vfd                           # Discretization in the vertical (vfd,vfe)\n                                        # Note that vfe does not yet work in non-hydrostatic mode\nPHYSICS=\"arome\"                         # Main model physics flag (arome|alaro)\nSURFACE=\"surfex\"                        # Surface flag (old_surface|surfex)\nDFI=\"none\"                              # Digital filter initialization (idfi|fdfi|none)\n                                        # idfi : Incremental dfi\n                                        # fdfi : Full dfi\n                                        # none : No initialization (AROME case)\nLSPBDC=no                               # Spectral boundary contions option off(no) | on(yes)\nLGRADSP=yes                             # Apply Wedi/Hortal vorticity dealiasing\nLUNBC=yes                               # Apply upper nested boundary condition","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"NAMELIST_BASE: Two different namelist sets are available (harmonie|alaro).\nDYNAMICS: Hydrostatic or non-hydrostatic dynamics (h|nh)\nVERT_DISC: Vertical discretization (vfd,vfe)\nPHYSICS: HARMONIE uses either AROME or ALARO for its forecast model physics (arome|alaro)\nSURFACE: Surface physics flag to use either the SURFEX or the ALADIN surface scheme(surfex|old_surface)\nDFI: Digital filter initialization switch (idfi|fdfi|none). idfi - incremental dfi, fdfi - full dfi, none - no initialization. See Digital filter for more information\nLSPBDC: Specify whether the boundary conditions are spectral or not (yes|no)\nLGRADSP: Switch to apply vorticity dealiasing (yes|no)\nLUNBC: Switch to apply upper boundary conditions (yes|no)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Physics","page":"Experiment","title":"Physics","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Physics options.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# Highlighted physics switches\nCISBA=\"3-L\"                             # Type of ISBA scheme in SURFEX. Options: \"3-L\" and \"2-L\".\nCROUGH=\"NONE\"                           # SSO scheme used in SURFEX \"NONE\"|\"'Z01D'\"|\"'BE04'\"\nSURFEX_SEA_ICE=\"none\"                   # Treatment of sea ice in surfex (none|sice)\nMASS_FLUX_SCHEME=edmfm                  # Version of EDMF scheme (edkf|edmfm)\n                                        # Only applicable if PHYSICS=arome\n                                        # edkf is the AROME-MF version\n                                        # edmfm is the KNMI implementation of Eddy Diffusivity Mass Flux scheme for Meso-scale\nHARATU=\"yes\"                            # Switch for HARATU turbulence scheme (no|yes)\nALARO_VERSION=0                         # Alaro version (1|0)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"CISBA: If SURFACE is set to surfex this selects the type of ISBA scheme to use in SURFEX. (3-L|2-L). See src/surfex_namelists.pm  Namelists\nCROUGH: If SURFACE is set to surfex this selects the sub-grid scale orography scheme used in SURFEX. (NONE|Z01D|BE04). See src/surfex_namelists.pm  Namelist\nSURFEX_SEA_ICE: Treatment of sea ice in surfex (none|sice). See nam/surfex_namelists.pm\nMASS_FLUX_SCHEME: If PHYSICS is set to arome choose the mass flux scheme to be used by AROME; edkf to use the AROME-MF scheme or edmfm to use the KNMI developed scheme\nHARATU: Switch to use the HARATU turbulence scheme\nALARO_VERSION: If PHYSICS is set to alaro select version of ALARO to use (0|1)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Near-Real-Time-aerosols","page":"Experiment","title":"Near Real Time aerosols","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Use of near real time aerosols from CAMS. Options can be set in namelist NAMNRTAER.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"BDAERO=cams                     # Import near-real-time aerosol via boundaries: cams | none\n                                # in case BDAERO=cams, change NAERO=11 in NAMNRTAER for dates previous to 2019071000\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Assimilation","page":"Experiment","title":"Assimilation","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Data assimilation settings. More assimilation related settings, in particular what observations to assimilate, can be found in src/include.ass","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** Assimilation ****\nANAATMO=3DVAR                           # Atmospheric analysis (3DVAR|4DVAR|blending|none)\nANASURF=CANARI_OI_MAIN                  # Surface analysis (CANARI|CANARI_OI_MAIN|CANARI_EKF_SURFEX|none)\n                                        # CANARI            : Old style CANARI\n                                        # CANARI_OI_MAIN    : CANARI + SURFEX OI\n                                        # CANARI_EKF_SURFEX : CANARI + SURFEX EKF ( experimental )\n                                        # none              : No surface assimilation\nANASURF_MODE=\"before\"                   # When ANASURF should be done\n                                        # before            : Before ANAATMO\n                                        # after             : After ANAATMO\n                                        # both              : Before and after ANAATMO (Only for ANAATMO=4DVAR)\nINCV=\"1,1,1,1\"                          # Active EKF control variables. 1=WG2 2=WG1 3=TG2 4=TG1\nINCO=\"1,1,0\"                            # Active EKF observation types (Element 1=T2m, element 2=RH2m and element 3=Soil moisture) \n\nMAKEODB2=no                             # Conversion of ODB-1 to ODB-2 using odb_migrator\n\nSST=BOUNDARY                            # Which SST fields to be used in surface analysis\n                                        # BOUNDARY          : SST interpolated from the boundary file. ECMWF boundaries utilize a special method.\n                                        #                     HIRLAM and HARMONIE boundaries applies T0M which should be SST over sea.\nLSMIXBC=no                              # Spectral mixing of LBC0 file before assimilation\n[ \"$ANAATMO\" = 3DVAR] && LSMIXBC=yes\nJB_INTERPOL=no                          # Interpolation of structure functions from a pre-defined domain to your domain\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"ANAATMO: Atmospheric analysis (3DVAR|4DVAR|blending|none)\nANASURF: Surface analysis (CANARI|CANARIOIMAIN|CANARIEKFSURFEX|none). See nam/surfex_namelists.pm\nANASURF_MODE:When the surface should be called (before|after|both)\nINCV: Active EKF control variables. 1=WG2 2=WG1 3=TG2 4=TG1 (0|1)\nINCO: Active EKF observation types (Element 1=T2m, element 2=RH2m and element 3=Soil moisture) (0|1)\nMAKEODB2: Option to convert ODB-1 databases to ODB-2 files for DA monitoring\nSST: which sea surface temperature field to use in the surface analysis\nLSMIXBC Spectral mixing of LBC0 file before assimilation (no|yes)\nJB_INTERPOL Interpolation of structure functions from a pre-defined domain to your domain (no|yes). Note that this has to be used with some caution.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Observations","page":"Experiment","title":"Observations","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** Observations ****\nOBDIR=$HM_DATA/observations             # Observation file directory\nRADARDIR=$HM_DATA/radardata             # Radar observation file directory\nSINGLEOBS=no                            # Run single obs experiment with observation created by scr/Create_single_obs (no|yes)\n\nUSE_MSG=no                              # Use MSG data for adjustment of inital profiles, EXPERIMENTAL! (no|yes)\nMSG_PATH=$SCRATCH/CLOUDS/               # Location of input MSG FA file, expected name is MSGcloudYYYYMMDDHH","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"OBDIR: Defines the directory that your (BUFR) observation files (obYYYYMMDDHH) are to read from\nRADARDIR: Defines the directory that your (OPERA HDF5) radar observation files are to be read from. BALTRAD OPERA HDF5, MF BUFR and LOCAL files are treated in scr/Prepradar\nSINGLEOBS Run single obs experiment with synthetic observation created by scr/Create_single_obs scr/Create_single_obs (no|yes)\nUSE_MSG: Use MSG data for adjustment of inital profiles, EXPERIMENTAL! (no|yes)\nMSG_PATH:  Location of input MSG FA file, expected name is MSGcloudYYYYMMDDHH. Note that the pre-processing software to generate input files is not yet included in HARMONIE","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#DVAR-settings","page":"Experiment","title":"4DVAR settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"4DVAR settings (experimental)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** 4DVAR ****\nNOUTERLOOP=1                            # 4DVAR outer loops, need to be 1 at present\nILRES=2,2                               # Resolution (in parts of full) of outer loops\nTSTEP4D=360,360                         # Timestep length (seconds) of outer loops TL+AD\nTL_TEST=yes                             # Only active for playfile tlad_tests\nAD_TEST=yes                             # Only active for playfile tlad_tests","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"NOUTERLOOP: Number of outer loops, need to be 1 at present\nILRES:  Resolution (in parts of full) of outer loops\nTSTEP4D: Timestep length (seconds) of outer loops TL+AD\nTL_TEST: Only active for playfile tlad_tests (yes|no)\nAD_TEST: Only active for playfile tlad_tests (yes|no)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Digital-filter-settings","page":"Experiment","title":"Digital filter settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Digital filter initialization settings if DFI is not equal to \"none\"","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** DFI setting ****\nTAUS=5400                               # cut-off frequency in second\nTSPAN=5400                              # 7200s or 5400s","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"TAUS cut-off frequency in seconds \nTSPAN length of DFI run in seconds","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Boundaries-and-initial-conditions","page":"Experiment","title":"Boundaries and initial conditions","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Settings for generation of lateral boundaries conditions for HARMONIE. Further information is available here","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** Lateral boundary conditions ****\nHOST_MODEL=\"ifs\"                        # Host model (ifs|hir|ald|ala|aro)\n                                        # ifs : ecmwf data\n                                        # hir : hirlam data\n                                        # ald : Output from aladin physics\n                                        # ala : Output from alaro physics\n                                        # aro : Output from arome physics\n\nHOST_SURFEX=\"no\"                        # yes if the host model is run with SURFEX\nSURFEX_INPUT_FORMAT=lfi                 # Input format for host model run with surfex (lfi|fa)\n\nNBDMAX=12                               # Number of parallel interpolation tasks\nBDLIB=ECMWF                             # Boundary experiment, set:\n                                        # ECMWF to use MARS data\n                                        # RCRa  to use RCRa data from ECFS\n                                        # Other HARMONIE/HIRLAM experiment\n\nBDDIR=$HM_DATA/${BDLIB}/archive/@YYYY@/@MM@/@DD@/@HH@   # Boundary file directory,\n                                                        # For more information, read in scr/Boundary_strategy.pl\nINT_BDFILE=$WRK/ELSCF${CNMEXP}ALBC@NNN@                 # Interpolated boundary file name and location\n\nBDSTRATEGY=simulate_operational # Which boundary strategy to follow\n                                # as defined in scr/Boundary_strategy.pl\n                                #\n                                # available            : Search for available files in BDDIR, try to keep forecast consistency\n                                #                        This is ment to be used operationally\n                                # simulate_operational : Mimic the behaviour of the operational runs using ECMWF LBC,\n                                #                        i.e. 6 hour old boundaries\n                                # same_forecast        : Use all boundaries from the same forecast, start from analysis\n                                # analysis_only        : Use only analysises as boundaries\n                                # era                  : As for analysis_only but using ERA interim data\n                                # latest               : Use the latest possible boundary with the shortest forecast length\n                                # RCR_operational      : Mimic the behaviour of the RCR runs, ie\n                                #                        12h old boundaries at 00 and 12 and\n                                #                        06h old boundaries at 06 and 18\n                                # enda                 : use ECMWF ENDA data for running ensemble data assimilation\n                                #                        or generation of background statistic.\n                                #                        Note that only LL up to 9h is supported\n                                #                        with this you should set your ENSMSEL members\n                                # eps_ec               : ECMWF EPS members (on reduced gaussian grid)\n                                #                      : Only meaningful with ENSMSEL non-empty, i.e., ENSSIZE > 0\n\nBDINT=1                         # Boundary interval in hours\n\nSURFEX_PREP=\"yes\"                # Use offline surfex prep facility (Alt. gl + Fullpos + prep )","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"HOST_MODEL defines the host model that provides the lateral boundaries conditions for your experiment\nhir for HIRLAM.\nald for ALADIN \nala for ALARO\naro for AROME\nifs for ECMWF-IFS. \nHOST_SURFEX Set to yes if host model runs with SURFEX. (no|yes)\nSURFEX_INPUT_FORMAT Input format for host model run with surfex (lfi|fa)\nBDLIB is the experiment to be used as boundaries. Possible values, ECMWF for IFS from MARS (default), RCRa for HIRLAM-RCR from ECFS or other HARMONIE experiment. \nBDDIR is the boundary file directory. The possible date information in the path must be given by using UPPER CASE letters (@YYYY@=year,@MM@=month,@DD@=day,@HH@=hour,@FFF@=forecast length).  \nBDSTRATEGY Which boundary strategy to follow i.e. How to find the right boundaries with the right age and location. Read more\nBDINT is boundary interval in hours.\nBDCLIM is the path to climate files corresponding the boundary files, when nesting HARMONIE to HARMONIE.\nINT_BDFILE is the name and location of the interpolated boundary files. These files are removed every cycle, but if you wish to save them you can specify a more permanent location here. By setting INT_BDFILE=ARCHIVE the interpolated files will be stored in your archive directory.\nNBDMAX Number of parallel boundary interpolation tasks in mSMS. The current default value is 12.\nSURFEX_PREP Use SURFEX tool PREP instead of gl+FULLPOS to prepare SURFEX initial conditions. This is now the default. The gl+FULLPOS version is still working but will not be maintained in the future (no|yes)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Read more about the boundary file preparation here.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Ensemble-mode-settings","page":"Experiment","title":"Ensemble mode settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# *** Ensemble mode general settings. ***\n# *** For member specific settings use msms/harmonie.pm ***\nENSMSEL=                                # Ensemble member selection, comma separated list, and/or range(s):\n                                        # m1,m2,m3-m4,m5-m6:step    mb-me == mb-me:1 == mb,mb+1,mb+2,...,me\n                                        # 0=control. ENSMFIRST, ENSMLAST, ENSSIZE derived automatically from ENSMSEL.\nENSINIPERT=                             # Ensemble perturbation method (bnd). Not yet implemented: etkf, hmsv.\nENSCTL=                                 # Which member is my control member? Needed for ENSINIPERT=bnd. See harmonie.pm.\nENSBDMBR=                               # Which host member is used for my boundaries? Use harmonie.pm to set.\nENSMFAIL=0                              # Failure tolerance for all members.\nENSMDAFAIL=0                            # Failure tolerance for members doing own DA. Not implemented.\nSLAFK=1.0                               # best set in harmonie.pm\nSLAFLAG=0                               # --- \" ---\nSLAFDIFF=0                              # --- \" ---\n\n# *** This part is for EDA with observations perturbation\nPERTATMO=none                           # ECMAIN  : In-line observation perturbation using the default IFS way.\n                            \t\t\t# CCMA    : Perturbation of the active observations only (CCMA content)\n\t                            \t\t#           before the Minimization, using the PERTCMA executable.\n                            \t\t\t# none    : no perturbation of upper-air observations\n\nPERTSURF=none                           # ECMA    : perturb also the surface observation before Canari (recommended\n                            \t\t\t#         : for EDA to have full perturbation of the initial state).\n                                        # model   : perturb surface fields in grid-point space (recursive filter)\n\t\t\t                            # none    : no perturbation for surface observations.\n\nFESTAT=no                               # Extract differences and do Jb calculations (no|yes)\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"ENSMSEL  Ensemble member selection, comma separated list, and/or range(s):\n # m1,m2,m3-m4,m5-m6:step    mb-me == mb-me:1 == mb,mb+1,mb+2,...,me\n  # 0=control. ENSMFIRST, ENSMLAST, ENSSIZE derived automatically from ENSMSEL.\nENSINIPERT Ensemble perturbation method (bnd). Not yet implemented: etkf, hmsv, slaf.\nENSMFAIL Failure tolerance for all members. Not yet implemented.\nENSMDAFAIL Failure tolerance for members doing own DA. Not yet implemented.\nENSCTL Which member is my control member? Needed for ENSINIPERT=bnd. See harmonie.pm.\nENSBDMBR Which host member is used for my boundaries? Use harmonie.pm to set.\nSLAFK Perturbation coefficients for SLAF, experimental\nSLAFLAG Time lag for boundaries in SLAG, experimental","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"For member dependent settings see msms/harmonie.pm.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"PERTATMO Observation perturbation with three options \nECMA : In-line observation perturbation using the default IFS way.\nCCMA : Perturbation of the active observations only (CCMA content) before the Minimization, using the PERTCMA executable.\nnone : no perturbation of upper-air observations\nPERTSURF Perturbation of surface observations before Canari (recommended for EDA to have full perturbation of the initial state) (no|yes).","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"FESTAT Extract differences and do Jb calculations (no|yes). Read more about the procedure here.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Climate-file-settings","page":"Experiment","title":"Climate file settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Climate file generation settings. Further information is available here","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** Climate files ****\nCREATE_CLIMATE=${CREATE_CLIMATE-yes}    # Run climate generation (yes|no)\nCLIMDIR=$HM_DATA/climate                # Climate files directory\nBDCLIM=$HM_DATA/${BDLIB}/climate        # Boundary climate files (ald2ald,ald2aro)\n                                        # This should point to intermediate aladin \n                                        # climate file in case of hir2aro,ifs2aro processes.\n\n# Physiography input for SURFEX\nECOCLIMAP_VERSION=2.2                   # Version of ECOCLIMAP for surfex (1,2)\n                                        # Available versions are 1.1-1.5,2.0-2.2\nSOIL_TEXTURE_VERSION=FAO                # Soil texture input data FAO|HWSD_v2","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"CREATE_CLIMATE: Run climate generation (yes|no). If you already have a full set of climate files generated in CLIMDIR you can set this flag to no for a faster run.\nCLIMDIR: path to the generated climate files for your specific domain. The input data for the climate generation is defined by HM_CLDATA defined in Env_system -> config-sh/config.YOURHOST\nBDCLIM: path to intermediate climate files\nECOCLIMAP_VERSION is the version of ECOCLIMAP to be used with SURFEX. Available versions are 1.1-1.5,2.0,2.1,2.2. See surfex_namelists.pm Namelist\nSOIL_TEXTURE_VERSION Soil texture input data (FAO|HWSD_v2). See surfex_namelists.pm more info.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Archiving-settings","page":"Experiment","title":"Archiving settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** Archiving settings ****\nARCHIVE_ECMWF=yes                       # Archive to $ECFSLOC at ECMWF (yes|no)\n# Archiving selection syntax, settings done below\n#\n# [fc|an|pp]_[fa|gr|nc] : Output from\n#  an : All steps from upper air and surface analysis\n#  fc : Forecast model state files from upper air and surfex\n#  pp : Output from FULLPOS and SURFEX_LSELECT=yes (ICMSHSELE+NNNN.sfx)\n# in any of the formats if applicable\n#  fa : FA files\n#  gr : GRIB[1|2] files\n#  nc : NetCDF files\n# sqlite|odb|VARBC|bdstrategy : odb and sqlite files stored in odb_stuff.tar\n# fldver|ddh|vobs|vfld : fldver/ddh/vobs/vfld files\n# climate : Climate files from PGD and E923\n# Some macros\n# odb_stuff=odb:VARBC:bdstrategy:sqlite\n# verif=vobs:vfld\n# fg : Required files to run the next cycle","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Forecast-output","page":"Experiment","title":"Forecast output","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** Cycles to run, and their forecast length ****\n\nTFLAG=\"h\"                               # Time flag for model output. (h|min)\n                                        # h   = hour based output\n                                        # min = minute based output\n\n\n# The unit of HWRITUPTIMES, FULLFATIMES, ..., SFXFWFTIMES should be:\n#   - hours   if TFLAG=\"h\"\n#   - minutes if TFLAG=\"min\"\n\n# Writeup times of # history,surfex and fullpos files\n# Comma separated list, and/or range(s) like:\n# t1,t2,t3-t4,t5-t6:step    tb-te == tb-te:1 == tb,tb+1,tb+2,...,te\n\nif [ -z \"$ENSMSEL\"] ; then\n  # Standard deterministic run\n  HH_LIST=\"00-21:3\"                       # Which cycles to run, replaces FCINT\n  LL_LIST=\"12,3\"                          # Forecast lengths for the cycles [h], replaces LL, LLMAIN\n                                          # The LL_LIST list is wrapped around if necessary, to fit HH_LIST\n  HWRITUPTIMES=\"00-21:3,24-60:6\"          # History file output times\n  FULLFAFTIMES=$HWRITUPTIMES              # History FA file IO server gather times\n  PWRITUPTIMES=\"00-60:3\"                  # Postprocessing times\n  PFFULLWFTIMES=-1                        # Postprocessing FA file IO server gathering times\n  VERITIMES=\"00-60:1\"                     # Verification output times, may change PWRITUPTIMES\n  SFXSELTIMES=$HWRITUPTIMES               # Surfex select file output times\n                                          # Only meaningful if SURFEX_LSELECT=yes\n  SFXSWFTIMES=-1                          # SURFEX select FA file IO server gathering times\n  SWRITUPTIMES=\"00-06:3\"                  # Surfex model state output times\n  SFXWFTIMES=$SWRITUPTIMES                # SURFEX history FA file IO server gathering times\n  if [ \"$SIMULATION_TYPE\" == climate]; then  #Specific settings for climate simulations\n    HWRITUPTIMES=\"00-760:6\"                 # History file output times\n    FULLFAFTIMES=\"00-760:24\"                # History FA file IO server gather times\n    PWRITUPTIMES=$HWRITUPTIMES              # Postprocessing times\n    VERITIMES=$HWRITUPTIMES                 # Verification output times, may change PWRITUPTIMES\n    SFXSELTIMES=$HWRITUPTIMES               # Surfex select file output times - Only meaningful if SURFEX_LSELECT=yes\n    SWRITUPTIMES=\"00-760:12\"                # Surfex model state output times\n    SFXWFTIMES=$SWRITUPTIMES                # SURFEX history FA file IO server gathering times\n  fi\n\n  ARSTRATEGY=\"climate:fg:verif:odb_stuff: \\\n              [an|fc]_fa:pp_grb\"          # Files to archive on ECFS, see above for syntax\n\nelse\n  # EPS settings\n  HH_LIST=\"00-21:3\"                       # Which cycles to run, replaces FCINT\n  LL_LIST=\"36,3,3,3\"                      # Forecast lengths for the cycles [h], replaces LL, LLMAIN\n  HWRITUPTIMES=\"00-06:3\"                  # History file output times\n  FULLFAFTIMES=$HWRITUPTIMES              # History FA file IO server gather times\n  PWRITUPTIMES=\"00-48:1\"                  # Postprocessing times\n  PFFULLWFTIMES=-1                        # Postprocessing FA file IO server gathering times\n  VERITIMES=\"00-60:3\"                     # Verification output times, may change PWRITUPTIMES\n  SFXSELTIMES=$HWRITUPTIMES               # Surfex select file output times\n                                          # Only meaningful if SURFEX_LSELECT=yes\n  SFXSWFTIMES=-1                          # SURFEX select FA file IO server gathering times\n  SWRITUPTIMES=\"00-06:3\"                  # Surfex model state output times\n  SFXWFTIMES=$SWRITUPTIMES                # SURFEX history FA file IO server gathering times\n\n  ARSTRATEGY=\"climate:fg:verif:odb_stuff: \\\n              an_fa:pp_grb\"               # Files to archive on ECFS, see above for syntax\n\nfi\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"The writeup times of model output can be defined as a space separated list or as a fixed frequency for model history files, surfex files and postprocessed files respectively. The unit of the steps of WRITUPTIMES, SWRITUPTIMES, PWRITUPTIMES and OUTINT should be in hours or minutes depending on the TFLAG Regular output interval can be switched on by setting OUTINT>0. Consequently, OUTINT will override the WRITUPTIMES lists!","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"TFLAG: Time flag for model output. Hourly or minute-based output (h|min)\nHWRITUPTIMES:  Output list for history files. Default is 00-21:3,24-60:6 which will output files every 3 hours for 00-21 and every 6 hours for 24-60.\nVERITIMES:  Output list for verification files. Default is 00-60:1 which will produce file every 1 hour for 00-60\nSWRITUPTIMES  Output list for surfex files. Default is 00-06:3 which output a  SURFEX file every 3 hours for 00-06.\nPWRITUPTIMES  Output list for fullpos (post-processed) files. Default is 00-21:3,24-60:6 which will output files every 3 hours for 00-21 and every 6 hours for 24-60.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"SURFEX_LSELECT=\"yes\"                    # Only write selected fields in surfex outpute files. (yes|no)\n                                        # Check nam/surfex_selected_output.pm for details.\n                                        # Not tested with lfi files.\nINT_SINI_FILE=$WRK/SURFXINI.fa          # Surfex initial file name and location\n\n# **** Postprocessing/output ****\nIO_SERVER=yes                           # Use IO server (yes|no). Set the number of cores to be used\n                                        # in your Env_submit\nIO_SERVER_BD=yes                        # Use IO server for reading of boundary data\nPOSTP=\"inline\"                          # Postprocessing by Fullpos (inline|offline|none).\n                                        # See Setup_postp.pl for selection of fields.\n                                        # inline: this is run inside of the forecast\n                                        # offline: this is run in parallel to the forecast in a separate task\n\nFREQ_RESET_TEMP=3                       # Reset frequency of max/min temperature values in hours, controls NRAZTS\nFREQ_RESET_GUST=1                       # Reset frequency of max/min gust values in hours, controls NXGSTPERIOD\n                                        # Set to -1 to get the same frequency _AND_ reset behaviour as for min/max temperature\n                                        # See yomxfu.F90 for further information.\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"SURFEX_LSELECT: Switch to write a selection of fields in SURFEX output files (yes|no). See surfex_selected_output.pm for more info. Namelist\nINT_SINI_FILE: name and location of the initial SURFEX file\nARCHIVE_ECMWF: archive files to ECFSLOC at ECMWF (yes|no)\nIO_SERVER: Use IO server (yes|no). If set to \"yes\" changes may be required in Env_submit -> config-sh/submit.YOURHOUST\nPOSTP: Postprocessing by Fullpos (inline|offline|none).\nFREQ_RESET_[TEMP|GUST]: Reset frequency of max/min values in hours, controls NRAZTS. Default is every 3/1 hours","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** GRIB ****\nCONVERTFA=yes                           # Conversion of FA file to GRIB/nc (yes|no)\nARCHIVE_FORMAT=GRIB1                    # Format of archive files (GRIB1|GRIB2|nc). nc format yet only available in climate mode\nNCNAMES=nwp                             # Nameing of NetCDF files follows (climate|nwp) convention.\nRCR_POSTP=no                            # Produce a subset of fields from the history file for RCR monitoring\n                                        # Only applicable if ARCHIVE_FORMAT=GRIB\nMAKEGRIB_LISTENERS=1                    # Number of parallel listeners for Makegrib\n                                        # Only applicable if ARCHIVE_FORMAT=GRIB\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"More options on fullpos postprocessing can be found in scr/Select_posp.pl","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"CONVERTFA: Conversion of FA files to GRIB or NetCDF (yes|no)\nARCHIVE_FORMAT: Format of archive files (GRIB1|nc). NetCDF format yet only available in climate mode\nRCR_POSTP: Produce a subset of fields from the history file for RCR monitoring (yes|no). This is only applicable if ARCHIVE_FORMAT=GRIB1|GRIB2\nMAKEGRIB_LISTENERS: Number of parallel listeners for Makegrib. Only applicable if ARCHIVE_FORMAT=GRIB1|GRIB2","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"More options on file conversion can be found in scr/Makegrib","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Verification-and-monitoring","page":"Experiment","title":"Verification and monitoring","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# **** Verification extraction ****\nOBSEXTR=yes                             # Extract observations from BUFR (yes|no)\nFLDEXTR=yes                             # Extract model data for verification from model files (yes|no)\nFLDEXTR_TASKS=1                         # Number of parallel tasks for field extraction\nVFLDEXP=$EXP                            # Experiment name on vfld files","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"OBSEXTR: Extract observations for verification from BUFR (yes|no)\nFLDEXTR: Extract model data for verification from model files (yes|no)\n*FLDEXTR_TASKS: Number of parallel tasks for field extraction\nVFLDEXP:","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Read more about the verification package here","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Field-verification","page":"Experiment","title":"Field verification","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# *** Field verification ***\nFLDVER=no                               # Main switch for field verification (yes|no)\nFLDVER_HOURS=\"06 12 18 24 30 36 42 48\"  # Hours for field verification\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"FLDVER Main switch for field verification (yes|no). The field verification extracts some selected variables for calculation of bias, rmse, stdv and averages on the model grid.\nFLDVER_HOURS Hours for field verification","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"More options on field verification can be found in scr/Fldver and scr/AccuFldver","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Observation-monitoring-and-general-diagnostics","page":"Experiment","title":"Observation monitoring and general diagnostics","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# *** Observation monitoring ***\nOBSMONITOR=obstat                       # Create Observation statistics plots\n                                        # Format: OBSMONITOR=Option1:Option2:...:OptionN\n                                        # obstat: Daily usage maps and departures\n                                        # no: Nothing at all\n                                        #\n                                        # obstat is # only active if ANAATMO != none\nOBSMON_SYNC=no                          # Sync obsmn sqlite tables to ecgate (yes|no)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"OBSMONITOR Selection for observation statistics plots","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"obstat Observations usage. Read more here.\nno No monitoring","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"Note that this is only active if ANAATMO != none","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"# Recipient(s) to send mail to when a task aborts\nMAIL_ON_ABORT=                          # you@work,you@home","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/","page":"Experiment","title":"Experiment","text":"If MAIL_ON_ABORT is set, a mail will be sent to the specified address(es) from head.h. Some tuning in Env_system may be needed for it to work on local hosts.","category":"page"},{"location":"DataAssimilation/MTEN/#Moist-Total-Energy-Norm-(MTEN)-diagnostic","page":"MTEN","title":"Moist Total Energy Norm (MTEN) diagnostic","text":"","category":"section"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"MTEN shows the sensitivity of the forecast model to different observations withdrawn from the full analysis system.  There are two ways of computing the MTEN diagnostic: A special branch was created in CY40 (see below) where the MTEN diagnostic can be requested. This approach uses Harmonie ensemble system to perform series of observation denial independent runs. This means that the following settings are used in msms/harmonie.pm","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"    'ENSBDMBR' => [ 0 ],\n    'ENSCTL'   => [ '000',  '001',  '002',  '003', '004', '005', '006', '007' ],\n    'AIRCRAFT_OBS' => [ 0, 1, 1, 1, 1, 1, 1, 1],\n    'BUOY_OBS'     => [ 1, 0, 1, 1, 1, 1, 1, 1],\n    'AMSUA_OBS'    => [ 1, 1, 0, 1, 1, 1, 1, 1],\n    'AMSUB_OBS'    => [ 1, 1, 1, 0, 1, 1, 1, 1],\n    'POL_OBS'      => [ 1, 1, 1, 1, 0, 1, 1, 1],\n    'HRW_OBS'      => [ 1, 1, 1, 1, 1, 0, 1, 1],\n    'TEMP_OBS'     => [ 1, 1, 1, 1, 1, 1, 0, 1],\n    'IASI_OBS'     => [ 1, 1, 1, 1, 1, 1, 1, 0],","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"In this particular example, we are interested in the impact of aircraft, Buoy, amsu-a, amsu-b/mhs, polar winds, high-resolution geowinds, radiosonde, and iasi observations. This setting is activated in config.exp with the following choice:","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"export  REFEXP DOMTEN\nexport SYNOP_OBS=1             # All synop\nexport AIRCRAFT_OBS=1          # AMDAR, AIREP, ACARS\nexport BUOY_OBS=1              # Buoy\nexport POL_OBS=1               # Satob polar winds\nexport GEO_OBS=0               # Satob geo winds\nexport HRW_OBS=1               # Satob HRWind\nexport TEMP_OBS=1              # TEMP, TEMPSHIP\nexport PILOT_OBS=1             # Pilot, Europrofiler\nexport SEVIRI_OBS=0            # Seviri radiances\nexport AMSUA_OBS=1             # AMSU-A\nexport AMSUB_OBS=1             # AMSU-B, MHS\nexport IASI_OBS=1              # IASI\nexport PAOB_OBS=0              # PAOB not defined everywhere\nexport SCATT_OBS=0             # Scatterometer data not defined everywhere\nexport LIMB_OBS=0              # LIMB observations, GPS Radio Occultations\nexport RADAR_OBS=0             # Radar\nexport GNSS_OBS=0              # GNSS","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"Where REFEXP is the reference experiment (see below), and DOMTEN (yes,no) is activate the MTEN choice when fetching the First-guess and the VarBC files for the MTEN computation, as follows: in /scr/Fetch_assim_data:","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"if [ ${DOMTEN} = \"yes\" ]; then\n  HM_REFEXP=/sbt/harmonie/$REFEXP\n  adir=${ECFSLOC}:${HM_REFEXP}/$YY/$MM/$DD/$HH\nelse\n  adir=$( ArchDir $HM_EXP $YY $MM $DD $HH )\nfi","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"in scr/FirstGuess (be careful this happens twice in the script)","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"if [ ${DOMTEN} = \"yes\" ]; then\n  HM_REFEXP=/sbt/harmonie/$REFEXP\n  adir=${ECFSLOC}:${HM_REFEXP}/$FGYY/$FGMM/$FGDD/$FGHH\nelse\n  adir=$( ArchDir $HM_EXP $FGYY $FGMM $FGDD $FGHH )\nfi","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"The MTEN can be also computed using a deterministic system. In this case, you need to take care of the First-guess and the VarBC files, which should come from the reference experiment. You need to carefully set the choice of the observations to be tested in scr/include.ass. In this case, you need to adapt the above Fetch_assim_data and FirstGuess scripts accordingly.","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"The MTEN diagnostic, similarly to DFS, is case sensitive, so it's better to male the computation with times and dates enough distant (by 5 days or more).","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"The MTEN can be computed the example below:","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"  for EXP in EXP1 EXP2;\n    for RANGE in 06 12 18 24 30 36 42 48;\n    do\n\n       YY=`echo $DTG | cut -c 1-4`\n       mm=`echo $DTG | cut -c 5-6`\n       dd=`echo $DTG | cut -c 7-8`\n       hh=`echo $DTG | cut -c 9-10`\n       # -- Get the FA files\n       # ===================\n       ecp ec:/$USER/harmonie/$REFEXP/$YY/$mm/$dd/$hh/ICMSHHARM+00$RANGE ./FAREF$RANGE\n       ecp ec:/$USER/harmonie/${EXP}/$YY/$mm/$dd/$hh/ICMSHHARM+00$RANGE ./${EXP}$RANGE\n       $MTEN_BIN/MTEN ./FAREF$RANGE ./${EXP}$RANGE\n\n    done\n  done\n","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"See Storto & Randriamampianina, 2010 for more details.","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"The harmonie-40h1_DA branch can be used to calculate MTEN values for selected (40h1.2/trunk) forecasts by using the ensemble functionality to carry multiple observation withdrawal experiments. Follow these instructions:","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"Check out the DA branch:\nmkdir -p $SCRATCH/harmonie_releases/branches\ncd $SCRATCH/harmonie_releases/branches\nsvn co https://svn.hirlam.org/branches/harmonie-40h1_DA\nSet up your (MTEN) experiment:\nmkdir -p $HOME/hm_home/mtenEXP\ncd $HOME/hm_home/mtenEXP\n$SCRATCH/harmonie_releases/branches/harmonie-40h1_DA/config-sh/Harmonie setup -r $SCRATCH/harmonie_releases/branches/harmonie-40h1_DA\nYou will need to edit the ecf/config_exp.h  file and msms/harmonie.pm file to produce the required observation withdrawal experiments.","category":"page"},{"location":"DataAssimilation/MTEN/","page":"MTEN","title":"MTEN","text":"... More to follow ...","category":"page"},{"location":"DataAssimilation/Surface/CANARI_EKF_SURFEX/#Surface-variables-assimilated-/-read-in-EKF_MAIN","page":"CANARI EKF SURFEX","title":"Surface variables assimilated / read in EKF_MAIN","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_EKF_SURFEX/","page":"CANARI EKF SURFEX","title":"CANARI EKF SURFEX","text":"From cycle 37 EKF is implemented in research/development mode. The following tiles and variables are modified:","category":"page"},{"location":"DataAssimilation/Surface/CANARI_EKF_SURFEX/#NATURE","page":"CANARI EKF SURFEX","title":"NATURE","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_EKF_SURFEX/#WG2/WG1/TG2/TG1","page":"CANARI EKF SURFEX","title":"WG2/WG1/TG2/TG1","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_EKF_SURFEX/","page":"CANARI EKF SURFEX","title":"CANARI EKF SURFEX","text":"The uppermost two levels in ISBA of soil moisture and temperature are assimilated. With CANARI/CANARI_OI_MAIN by an OI method, by CANARI_SURFEX_EKF by an Extended Kalman Filter (EKF).","category":"page"},{"location":"DataAssimilation/Surface/CANARI_EKF_SURFEX/","page":"CANARI EKF SURFEX","title":"CANARI EKF SURFEX","text":"For 2012 it is planned to have a re-writing of OI_MAIN/EKF_MAIN to be the same binary in order to be able to apply the work done for OI_MAIN in EKF_MAIN and thus reduce the maintainance costs.","category":"page"},{"location":"DataAssimilation/StructureFunctions/#Derivation-of-Structure-Functions","page":"Structure functions","title":"Derivation of Structure Functions","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/#General","page":"Structure functions","title":"General","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"For each new model domain, in order to carry out upper air data assimilation (3DVAR or 4DVAR) one needs to generate background error covariances (generally referred to as structure functions).  Within the HARMONIE community the derivation has been based on data generated with ensemble HARMONIE forecasts which downscale from ECMWF EPS runs. To alleviate spin-up issues the HARMONIE  forecasts are run up to 6 hours. Using the ECMWF LBC data, 6h HARMONIE ensemble forecasts are initiated from ECMWF 6h forecasts daily from 00 UTC and 12 UTC, with ECMWF forecasts as initial  and lateral boundary conditions. To obtain stable statistics, it seems appropriate to run 4 ensembles for the chosen episode (s). Ideally the episodes should sample different seasons.  Therefore it is recommended to run for one winter month and one summer month, for example June 2016 and January 2017. These periods are chosen so as to benefit from the latest upgrade to ECMWF's EDA syste,. Thereby we sample both seasonal (January, July)  and daily (00 UTC and 12 UTC)  variations. After running of the ensembles the archived results (6h forecasts) are processed to generate structure functions by running a program called 'festat'. ","category":"page"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"This documentation describes a new environment for generating structure functions and was introduced in cy38h1.2. The main changes in the new environment as compared with the old environment are: (1) the ensemble forecasts are all run under on single experiment within the HARMONIE mini-sms system, (2) a software 'FEMARS' is run to generate GRIB files of forecasts differences after each cycle have completed in HARMONIE mini-sms and (3) a parallel version of the 'festat' is run as a last step of the single mini-sms experiment, after that the forecast difference files have been generated for the entire period. For earlier versions the old procedure for generating structure functions should be used (wiki:HarmonieSystemDocumentation/Structurefunctions). ","category":"page"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"Note that under certain circumstances there is a method to technically run 3/4DVAR using structure functions derived from another HARMONIE model domain. That requires that your HARMONIE model domain is embedded in the original domain from which structure function has been derived, and with same vertical coordinate. Thus the derivation of structure function is usually the first step to go before any VAR experiment can be done. There is a program developed for converting structure functions from one area to another but it is recommended to be used for technical tests only. This procedure is described in the section [#Interpolationtests] and it is only intended for technical assimilation tests.","category":"page"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"The procedure for generating structure functions from an ensemble of forecasts is described below for a AROME setup with 2.5 km horizontal resolution, 65 vertical level and a domain covering Denmark. The experiment is run for a winter-period of 10 days followed by a summer-period of 10 days on the ECMWF ecgb/cca computing system. Forecasts are run twice a day. In the section below detailed instructions on how to generate the structure functions are give..The other sections deals with how to diagnose the structure functions and how to interface the newly generated structure functions into the data assimilation. Finally something about recent and ongoing work and future development plans.","category":"page"},{"location":"DataAssimilation/StructureFunctions/#Generating-background-error-statistics-(using-40h1)","page":"Structure functions","title":"Generating background error statistics (using 40h1)","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"The following instructions are valid for trunk and any 40h1.2 tags that have been created. These instructions will only work at ECMWF on ecgate and cca.","category":"page"},{"location":"DataAssimilation/StructureFunctions/#New-domain-setup","page":"Structure functions","title":"New domain setup","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"If you are creating structure functions for a new (or you are not sure):","category":"page"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"Create a new experiment on ecgate:\nmkdir -p $HOME/hm_home/newDomJb\ncd $HOME/hm_home/newDomJb\n~hlam/Harmonie setup -c AROME_JB -r /home/ms/spsehlam/hlam/harmonie_release/git/develop\n~hlam/Harmonie co scr/Harmonie_domains.pm\nEdit scr/Harmonie_domains.pm and add your new domain definition. New domain creation is described in ModelDomain which links to the useful Domain Creation Tool here \nThe ensemble that will be used to generate the structure functions needs to be defined in msms/harmonie.pm. An edited ensemble configuration file should define a four member ensemble that only varies the boundary memeber input (ENSBDMBR) as follows:\n%env = (\n#   'ANAATMO'  => { 0 => '3DVAR' },\n#   'HWRITUPTIMES' => { 0 => '00-21:3,24-60:6' },\n#   'SWRITUPTIMES' => { 0 => '00-06:3' },\n#   'HH_LIST' => { 0 => '00-21:3' },\n#   'LL_LIST' => { 0 => '36,3' },\n#   'LSMIXBC'  => { 0 => 'yes' },\n#   'ANASURF'  => { 0 => 'CANARI_OI_MAIN' },\n    'ENSCTL'   => [ '001', '002', '003', '004'],\n#   'OBSMONITOR' => [ 'obstat'],\n# SLAFLAG: Forecast length to pick your perturbation end point from\n# SLAFDIFF: Hours difference to pick your perturbation start point from\n# SLAFLAG=24, SLAFDIFF=6 will use +24 - +18\n# SLAFDIFF=SLAFLAG will retain the original SLAF construction\n# SLAFK should be tuned so that all members have the same perturbation size\n   'ENSBDMBR' => [ 1,2,3,4],\n#   'SLAFLAG'  => [    0,    6,     6,    12,    12,  18,     18,   24,    24,    30,    30],\n#   'SLAFDIFF' => [    0,    6,     6,     6,     6,    6,     6,    6,     6,     6,     6],\n#   'SLAFK'    => ['0.0','1.75','-1.75','1.5','-1.5','1.2','-1.2','1.0','-1.0','0.9','-0.9'],\n# When using ECMWF ENS the members should be defined\n#   # 'ENSBDMBR' => [ 0, 1..10],\n\n### Normally NO NEED to change the settings below\nRun for two one-month (30 day) periods:\ncd $HOME/hm_home/newDomJb\n~hlam/Harmonie start DTG=2016060100 DTGEND=2016070100\n#\n#~hlam/Harmonie start DTG=2017010100 DTGEND=2017013100\nGenerate the statistics using festat offline:\nCopy Festat_offline to cca\nEdit the script to reflect your user and experiment details\nSubmit job with:\nqsub Festat_offline","category":"page"},{"location":"DataAssimilation/StructureFunctions/#Existing-domain-setup","page":"Structure functions","title":"Existing domain setup","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"Create a new experiment on ecgate:\nmkdir -p $HOME/hm_home/newDomJb\ncd $HOME/hm_home/newDomJb\n~hlam/Harmonie setup -c AROME_JB -r /home/ms/spsehlam/hlam/harmonie_release/trunk -d DOMAIN # where domain is the name of your domain\nThe ensemble that will be used to generate the structure functions needs to be defined in msms/harmonie.pm. An edited ensemble configuration file should define a four member ensemble that only varies the boundary memeber input (ENSBDMBR) as follows:\n%env = (\n#   'ANAATMO'  => { 0 => '3DVAR' },\n#   'HWRITUPTIMES' => { 0 => '00-21:3,24-60:6' },\n#   'SWRITUPTIMES' => { 0 => '00-06:3' },\n#   'HH_LIST' => { 0 => '00-21:3' },\n#   'LL_LIST' => { 0 => '36,3' },\n#   'LSMIXBC'  => { 0 => 'yes' },\n#   'ANASURF'  => { 0 => 'CANARI_OI_MAIN' },\n   'ENSCTL'   => [ '001', '002', '003', '004'],\n#   'OBSMONITOR' => [ 'obstat'],\n# SLAFLAG: Forecast length to pick your perturbation end point from\n# SLAFDIFF: Hours difference to pick your perturbation start point from\n# SLAFLAG=24, SLAFDIFF=6 will use +24 - +18\n# SLAFDIFF=SLAFLAG will retain the original SLAF construction\n# SLAFK should be tuned so that all members have the same perturbation size\n   'ENSBDMBR' => [ 1,2,3,4],\n#   'SLAFLAG'  => [    0,    6,     6,    12,    12,  18,     18,   24,    24,    30,    30],\n#   'SLAFDIFF' => [    0,    6,     6,     6,     6,    6,     6,    6,     6,     6,     6],\n#   'SLAFK'    => ['0.0','1.75','-1.75','1.5','-1.5','1.2','-1.2','1.0','-1.0','0.9','-0.9'],\n# When using ECMWF ENS the members should be defined\n#   # 'ENSBDMBR' => [ 0, 1..10],\n\n### Normally NO NEED to change the settings below\nRun for two one-month (30 day) periods:\ncd $HOME/hm_home/newDomJb\n~hlam/Harmonie start DTG=2016060100 DTGEND=2016070100\n#\n#~hlam/Harmonie start DTG=2017010100 DTGEND=2017013100\nGenerate the statistics using festat offline:\nCopy Festat_offline to cca\nEdit the script to reflect your user and experiment details\nSubmit job with:\nqsub Festat_offline","category":"page"},{"location":"DataAssimilation/StructureFunctions/#Generating-background-error-statistics-with-EDA-cycling-(using-38h1)","page":"Structure functions","title":"Generating background error statistics with EDA cycling (using 38h1)","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"Preparation of one mini-sms ensemble experiment for running 8 EDA member 6h forecast cycling with 3DVAR \nOn ecgb $HOME directory create ~/hm_home/your_exp directory. Then cd $HOME/hm_home/your_exp.\nCreate experiment by typing ~hlam/Harmonie setup -r ~hlam/harmonie_release/tags/harmonie-38h1.2 if you e.g. run harmonie-38h1.2.\nedit ecf/config_exp.h  as follows:\nset LSMIXBC=no\nset FCINT=6\nset BDSTRATEGY=enda\nset BDINT=3 \nset FESTAT=yes\nset ENSMSEL=1,2,3,4,5,6,7,8\nadd export ENSSIZE=8 \nset PERTATMO=CCMA\nset PERTSURF=\"yes\" \nEdit the msms/harmonie.pm file as follows: \nset 'LSMIXBC'  => { 0 => 'no', 1 => 'no' },\nset 'ENSBDMBR' => [ 1, 2, 3, 4, 5, 6, 7, 8],\nset 'PHYSICS'  => [ 'arome','arome','arome','arome','arome','arome','arome','arome'],\nset 'ENSCTL'   => [ '001',  '002',  '003',  '004','005','006','007','008'],\nset 'TSTEP'    => [  '75',  '75',  '75',  '75','75',  '75',  '75',  '75'],\nin $HOME/hm_home/your_exp, edit Env_submit. Edit this file\nset $nproc_festat=320=NPD*ND*NMB; where NPD is number of runs per day (4) x ND is number of days (10) x NMB is number of members (8).\nLaunch of mini-sms ensemble experiment\n~hlam/Harmonie start DTG=2013081500 DTGEND=2013082418 LL=6\nAfter the EDA runs have finished, the resulting background error statistics (structure functions) will be found on cca:$SCRATCH/hm_home/your_exp/archive/extract/. The name of the files are stab_your_exp_2013081500_320.bal.gz, stab_your_exp_2013081500_320.cv.gz and stab_your_exp_2013081500_320.cvt.gz. ","category":"page"},{"location":"DataAssimilation/StructureFunctions/#Background-error-statistics-on-cca-with-EDA-cycling-for-big-areas","page":"Structure functions","title":"Background error statistics on cca with EDA cycling for big areas","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"If you want to compute the B-matrix (structure functions) for a big integration domain using FESTAT program you need to adjust the resources in Env_submit and in the script Festat","category":"page"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"It is recommended to set the number of tasks to the actual number of  cases (integrations)\nIn order to set the resources apart from defining the PBS variables it is necessary to set this trough the command line that it is how the resources are actually passed to the FESTAT program\nExample settings for a big area (1152x864) and 240 cases:","category":"page"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"#PBS -l EC_total_tasks=240\n#PBS -l EC_tasks_per_node=12\n#PBS -l EC_threads_per_task=1\n#PBS -l EC_hyperthreads=1\n#PBS -l EC_memory_per_task=5000MB\n\n\nMPPEXEC=\"aprun -N 12 -n 240 -d 1 -j 1\"\n","category":"page"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"It is also possible to run FESTAT offline once the integrations have ended\nFestat_offline, ","category":"page"},{"location":"DataAssimilation/StructureFunctions/#Diagnosis-of-background-error-statistics","page":"Structure functions","title":"Diagnosis of background error statistics","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"Diagnosis of background error statistics is a rather complicated task. To get an idea of what the correlations and covariances should look like take a look in the article: Berre, L., 2000: Estimation of synoptic and meso scale forecast error covariances in a limited area model. Mon. Wea. Rev., 128, 644-667. Software for investigating and graphically illustrate different aspects of the background error statistics has been developed and statistics generated for different domains has been investigated (see attached report below by Nils Gustafsson). With this software you can also compare your newly generated background error statistics with the one generated for other HARMONIE domains. This will give you and idea if your statistics seems reasonable. For diagnosing the newly derived background error statistics follow these instructions:\nGet the code and scripts:\n# on ecgate\ncd $SCRATCH\necp ec:/smx/Jb_wiki_ml/jbdiagconv_ensys.tar .\ntar -xvf jbdiagconv_ensys.tar # produces $SCRATCH/jbdiagconv\ncd jbdiagconv \nIn jbdiagconv you will find:\ndata (containing background error statistics for different domains\ndiagXXarome directories containing output-statistics for different domains (for example diagDKarome and  diagFIarome)\ntwo .f90 files - jbconv.f90  jbdiagnose.f90\nthree scripts for diagnostics - jbconv.sh, jbdiagnose.sh and ???\nCompile the fortran programs (with gfortran):\ncd $SCRATCH/jbdiagconv\n./Compile\nThe Compile script should produce two executables, jbdiagnose.x and  jbconv.x. \nNext, fetch your background error statistics files:\ncd $SCRATCH/jbdiagconv/data\necp ec:/$uid/jbdata/stab_your_exp_2012070106_240* .\ngunzip stab_your_exp_2012070106_240.*.gz\nRun Jb diagnostics script:\nmkdir ${SCRATCH}/jbdiaconv/diag # for output\n### edit 'jbdiagnose.ksh' to read your data and adjust namelist to your domain settings (horizontal resolution and vertical levels). \n### Finally generate statistics \n./jbdiagnose.ksh'\n### A lot of statistics files appears in directory ${SCRATCH}/jbdiagconv/diag\ncp -r ${SCRATCH}/jbdiagconv/diag ${SCRATCH}/jbdiagconv/diagEXP\nPlaced in ${SCRATCH}/jbdiagconv plot statistics of your newly derived background error statistics with ./plotbaloper_arome.sh, plotspdens_arome.sh and plotvercor.sh. This will generate a lot of postscript files in directory ${SCRATCH}/jbdiagconv. The statistics is for newly generated structure functions (named EXP) as well as for structure functions of other domains, plotted together. The meaning of these can be found in attached document by Gustafsson (missing?). Note that the plot scripts can be adjusted to plot different vertical levels instead of the currently chosen level.\nAnother way to diagnose the background error statistics is to investigate the analysis increments of various types of data assimilation experiments. For carrying out such experiments we however first need to introduce our newly generated background error statistics into a data assimilation experiment. That procedure is describe below for a full scale data assimilation experiment (utilizing in this example the domain placed over Denmark (named DKCOEXP), for which the statistics was derived).","category":"page"},{"location":"DataAssimilation/StructureFunctions/#Test-3DVAR-with-the-new-background-error-statistics","page":"Structure functions","title":"Test 3DVAR with the new background error statistics","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"create hm_home/38h12_assim directory. Then cd $HOME/hm_home/38h12_assim.\ncreate experiment by typing ~hlam/Harmonie setup -r ~hlam/harmonie_release/tags/harmonie-38h1.2 -h ecgb-cca.\nCheck out the file include.ass by typing ~hlam/Harmonie co scr/include.ass \nIn include.ass set JBDIR=ec:/$uid/jbdata (uid being your userid, in this example ec:/smx/jbdata) and  f_JBCV= name of your .cv file in ec:/$uid/jbdata (without .gz) and f_JBBAL is 'name of your .bal file in ec:/$uid/jbdata  (without .gz)  (in this example ,f_JBCV=stab_your_exp_2012070106_160.cv, stab_your_exp_2012070106_160.bal).  Add these three lines instead of the three lines in include.ass that follows right after the elif statement: elif [ \"$DOMAIN\" = DKCOEXP]; then. If domain is other than DKCOEXP one has to look for the alternative name of the domain. \nFrom $HOME/hm_home/38h12_assim launch experiment by typing\n~hlam/Harmonie start DTG=2012061003 DTGEND=2012061006 LL=03\nThe resulting analysis file be found on c2a (ssh c2a) under $SCRATCH/hm_home/38h12_assim/archive/2012/06/10/06 and it will be called MXMIN1999+0000 and on and ectmp:/smx/harmonie/38h12_assim/YYYY/MM/DD/06. To diagnose the 3D-VAR analysis increments of the 38h12_sinob-experiment, copy the files MXMIN1999+0000 (analysis) and ICMSHHARM+0003 (fg) to $SCRATCH. The first guess (background) file can be found on $SCRATCH/hm_home/38h12_assim/archive/2012/06/10/03 and ectmp:/smx/harmonie/38h12_assim/YYYY/MM/DD/03.  Convert from FA-file format to GRIB with the gl-software ($SCRATCH/hm_home/38h12_assim/bin/gl) by typing ./gl -p MXMIN1999+0000 and ./gl -p ICMSHANAL+0000. Then plot the difference between files file with your favourite software. Plot horizontal and vertical cross-sections of temperature and other variables using your favourite software (MetgraF or cross for example).\nNow you have managed to insert the newly generated background error statistics to the assimilation system and managed to carry out a full scale data assimilation system and plot the analysis increments. The next natural step to further diagnose the background error statistics is to carry out a single observation impact experiment, utilizing your newly generated background error statistics. Note the variables REDNMC and REDZONE in include.ass. REDNMC is the scaling factor for the background error statistics (default value 0.6/0.9 ","category":"page"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"for DKCOEXP/NEW_DOMAIN. REDZONE described how far from the lateral boundaries (in km) the observations need to be located to be assimilated (default value 150/100) for DKCOEXP/NEW_DOMAIN.","category":"page"},{"location":"DataAssimilation/StructureFunctions/#Interpolation-tests","page":"Structure functions","title":"Interpolation tests","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"Interpolation of background error statistics between different domains for technical data assimilation tests:","category":"page"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"A tool has been developed to interpolate background error statistics derived for one domain, to another domain. The purpose is to make it easier to get started and to carry out technical data assimilation tests for new domains. The functionality of the tool 'jbconv' is documented in detail in an attached report by Nils Gustafsson. One constraint worth mentioning is that the number of vertical levels of the two domains should be the same. In addition, wave number 1 in the spectral representation of errors should be larger for the input domain than for the output. In practice this mean that the length (in km) of the longest side of the domain should be longer for the input domin than for the output domain. By doing some settings in the HARMONIE system background error statistics can automatically be interpolated from one area to another in HARMONIE data assimilation experiments. This procedure is described below and it has been tested on HARMONIE cy38b1.1.beta1, from which version it is applicable. In the illustrative example background error statistics are automatically generated for the MEDITERRANEAN domain from the DENMARK domain in a HARMONIE assimilation experiment carried out over the  MEDITERRANEAN domain.\ncreate hm_home/38h12_jbint directory. Then cd $HOME/hm_home/38h12_jbint.\ncreate experiment by typing ~hlam/Harmonie setup -r ~hlam/harmonie_release/tags/harmonie-38h1.2.beta.1 -h ecgb-cca.\nEdit ecf/config_exp.h  as follows:\nset DOMAIN=MEDITERRANEAN,\nset JB_INTERPOL=yes,\nCheck out the file jbconv.sh by typing ~hlam/Harmonie co scr/jbconv.sh.\nReplace the file jbconv.sh with the corrected one attached at the bottom of this page. \nCheck out the file domain_prop.F90 by typing ~hlam/Harmonie co util/gl/prg/domain_prop.F90.\nReplace the file domain_prop.F90 with the corrected one attached at the bottom of this page.\nLaunch the single observation impact experiment by standing in hmhome/38h12jbint typing:\n~hlam/Harmonie start DTG=2012061003 DTGEND=2012061006 LL=03\nThe resulting analysis file be found on cca (ssh cca) under $SCRATCH/hm_home/38h12_jbint/archive/2012/06/10/06 and it will be called MXMIN1999+0000 and on and ectmp:/smx/harmonie/38h12_jbint/YYYY/MM/DD/06. To diagnose the 3D-VAR analysis increments of the 38h12_sinob-experiment, copy the files MXMIN1999+0000 (analysis) and ICMSHHARM+0003 (fg) to $SCRATCH. The first guess (background) file can be found on $SCRATCH/hm_home/38h12_jbint/archive/2012/06/10/03 and ectmp:/smx/harmonie/38h12_jbint/YYYY/MM/DD/03.  Convert from FA-file format to GRIB with the gl-software ($SCRATCH/hm_home/38h12_jbint/bin/gl) by typing ./gl -p MXMIN1999+0000 and ./gl -p ICMSHHARM+0003. Then plot the difference between files file with your favourite software. Plot horizontal and vertical cross-sections of temperature and other variables using your favourite software (MetgraF or cross for example). The interpolated background error statistics for the MEDITERRANEAN domain will appear on cca in $SCRATCH/hm_home/38h12_jbint/lib/const/jb_data as stabfiltn_MEDITERRANEAN_65_jbconv.cv and stabfiltn_MEDITERRANEAN_65_jbconv.bal.","category":"page"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"Note that you can change the area you want to interpolate the structure functions from by editing in the script jbconv.sh.","category":"page"},{"location":"DataAssimilation/StructureFunctions/#Recent-work-and-future-developments","page":"Structure functions","title":"Recent work & future developments","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"Recent and on-going work as well as plans for future developments:","category":"page"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"Present work regarding structure functions concerns investigations of spin-up effects related with downscaling of ECMWF Ensemble Data Assimilation (EDA) forecasts with the HARMONIE system. There is also work on introduction and testing of ensemble data assimilation in the HARMONIE system itself. Longer term research is towards flow dependent background error statistics and close link between the data assimilation and the ensemble forecasting system.","category":"page"},{"location":"DataAssimilation/StructureFunctions/#References","page":"Structure functions","title":"References","text":"","category":"section"},{"location":"DataAssimilation/StructureFunctions/","page":"Structure functions","title":"Structure functions","text":"Festat guide, Ryad El Katib, Meteo France, 2014","category":"page"},{"location":"PostProcessing/gl/#Post-processing-with-gl","page":"GL","title":"Post processing with gl","text":"","category":"section"},{"location":"PostProcessing/gl/#Introduction","page":"GL","title":"Introduction","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"gl ( as in griblist ) is a multi purpose tool for file manipulation and conversion. It uses ECMWF's   ecCodes library, and can be compiled with and without support for HARMONIE FA/LFI or NETCDF files. The gl package also includes software for extraction for verification, fldextr, and field comparison, xtool.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":" USAGE: gl file [-n namelist_file] [-o output_file] -[lfgmicpsdtq] [-lbc CONF]\n\n gl [-f] file, list the content of a file, -f for FA/lfi files  \n -c    : Convert a FA/lfi file to grib ( -f implicit )          \n -p    : Convert a FA file to grib output without extension zone\n         (-c and -f implicit )                                  \n -musc : Convert a MUSC FA file ASCII ( -c implicit )           \n -lbc ARG : Convert a CONF file to HARMONIE input               \n            where CONF is ifs or hir as in ECMWF/HIRLAM data    \n         climate_aladin assumed available                       \n -d    : Together with -lbc it gives a (bogus) NH boundary file   \n         climate_aladin assumed available                       \n -s    : Work as silent as possible                             \n -g    : Prints ksec/cadre/lfi info                             \n -m    : Prints min,mean,max of the fields                      \n -i    : Prints the namelist options (useless)                  \n -tp   : Prints the GRIB parameter usage                        \n -t    : Prints the FA/lfi/GRIB table (useful)                  \n -wa   : Prints the atmosphere FA/NETCDF/GRIB table in wiki fmt \n -ws   : Prints the surfex FA/NETCDF/GRIB table in wiki fmt     \n -q    : Cross check the FA/lfi/GRIB table (try)                \n -pl X : Give polster_projlat in degrees                        \n\n gl file -n namelist_file : interpolates file according to      \n                            namelist_file                       \n gl -n namelist_file : creates an empty domain according to     \n                       specifications in namelist_file          \n -igd  : Set lignore_duplicates=T                               \n -igs  : Set lignore_shortname=T. Use indicatorOfParameter      \n             instead of shortName for selection                 \n","category":"page"},{"location":"PostProcessing/gl/#ecCodes-definition-tables","page":"GL","title":"ecCodes definition tables","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Since ecCodes has replaced gribapi as the ECMWF primary software package to handle GRIB, we will hereafter only refer to ecCodes but same similar settings applies for gribapi as well. With the change to ecCodes we heavily rely on the shortName key for identification. To get the correct connection between the shortnames and the GRIB1/GRIB2 identifiers we have defined specific tables for harmonie. These tables can be found in /util/gl/definitions. To use these tables you have to define the ECCODES_DEFINITION_PATH environment variable as ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"export ECCODES_DEFINITION_PATH=SOME_PATH/gl/definitions:PATH_TO_YOUR_ECCODES_INSTALLATION","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"If this is not set correctly the interpretation of the fields may be wrong.","category":"page"},{"location":"PostProcessing/gl/#GRIB/FA/LFI-file-listing","page":"GL","title":"GRIB/FA/LFI file listing","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Listing of GRIB/ASIMOF/FA/LFI files.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":" gl [-l] [-f] [-m] [-g] FILE","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"where FILE is in GRIB/ASIMOF/FA/LFI format","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Option Description\n-l input format is LFI\n-f input format is FA\n -l and -f are equivalent\n-g print GRIB/FA/LFI header\n-m print min/mean/max values","category":"page"},{"location":"PostProcessing/gl/#GRIB/FA/LFI-file-conversion","page":"GL","title":"GRIB/FA/LFI file conversion","text":"","category":"section"},{"location":"PostProcessing/gl/#Output-to-GRIB1","page":"GL","title":"Output to GRIB1","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"gl [-c] [-p] FILE [ -o OUTPUT_FILE] [ -n NAMELIST_FILE]","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"where ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":" \n-c converts the full field (including extension zone) from FA to GRIB1\n-p converts field excluding the extension zone (\"p\" as in physical domain) from FA to GRIB1","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"The FA/LFI to GRIB mapping is done in a table defined by a util/gl/inc/trans_tab.h","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"To view the table:","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"gl -t\ngl -tp","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"To check for duplicates in the table:","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"gl -q","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"The translation from FA/LFI to GRIB1 can be changed through a namelist like this one:","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"  &naminterp\n    user_trans%full_name ='CLSTEMPERATURE',\n    user_trans%t2v       = 253,\n    user_trans%pid       = 123,\n    user_trans%levtype   = 'heigthAboveGround',\n    user_trans%level     = 002,\n    user_trans%tri       = 000,\n  /","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"or for the case where the level number is included in the FA name","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"  &naminterp\n    user_trans%full_name='SNNNEZDIAG01',\n    user_trans%cpar='S'\n    user_trans%ctyp='EZDIAG01',\n    ...\n  /","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Conversion can be refined to convert a selection of fields. Below is and example that will write out ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"T (shortname='t',pid=011), u (shortname='u',pid=033) andv (shortname='v',pid=034) on all (level=-1) model levels (levtype='hybrid')\nT (shortname='t',pid=011) at 2m (lll=2) above the ground (levtype='heightAboveGround') [T2m]\nTotal precipitation (shortname='tp',pid=061,levtype='heightAboveGround',level=000)","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"  &naminterp\n   readkey%shortname=   't',     'u',     'v',                't',               'tp',               'fg',\n   readkey%levtype='hybrid','hybrid','hybrid','heightAboveGround','heightAboveGround','heightAboveGround',\n   readkey%level=        -1,      -1,      -1,                  2,                  0,                 10,\n   readkey%tri =          0,       0,       0,                  0,                  4,                  2,\n  /","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"where ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"shortname is the ecCodes shortname of the parameter \nlevtype is the ecCodes level type\nlevel is the GRIB level\ntri means timeRangeIndicator and is set to distinguish between instantaneous, accumulated and min/max values.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"The first three ones are well known to most users. The time range indicator is used in HARMONIE to distinguish between instantaneous and accumulated fields. Read more about the options here Note that for levtype hybrid setting level=-1 means all. ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"We can also pick variables using their FA/lfi name:","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"  &naminterp\n    readkey%faname = 'SPECSURFGEOP','SNNNTEMPERATURE',\n  /","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Where SNNNTEMPERATURE means that we picks all levels.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Fields can be excluded from the conversion by name","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"  &naminterp\n    exclkey%faname = 'SNNNTEMPERATURE'\n  /","category":"page"},{"location":"PostProcessing/gl/#Output-to-GRIB2","page":"GL","title":"Output to GRIB2","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"To get GRIB2 files the format has to be set in the namelist as ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"  &naminterp\n    output_format = 'GRIB2'\n  /","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"The conversion from FA to GRIB2 is done in gl via the ecCodes tables. All translations are defined in util/gl/scr/harmonie_grib1_2_grib2.pm where we find all settings required to specify a parameter in GRIB1 and GRIB2.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"\n  tmax => {\n   editionNumber=> '2',\n   comment=> 'Maximum temperature',\n   discipline=> '0',\n   indicatorOfParameter=> '15',\n   paramId=> '253015',\n   parameterCategory=> '0',\n   parameterNumber=> '0',\n   shortName=> 'tmax',\n   table2Version=> '253',\n   typeOfStatisticalProcessing=> '2',\n   units=> 'K',\n  },\n","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"To create ecCodes tables from this file run","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"   cd gl/scr\n   ./gen_tables.pl harmonie_grib1_2_grib2","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"and copy the grib1/grib2 directories to gl/definitions.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Note that there are no GRIB2 transations yet defined for the SURFEX fields!","category":"page"},{"location":"PostProcessing/gl/#postprocessing","page":"GL","title":"postprocessing","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"gl can be used to produce postprocessed parameters possibly not available directly from the model. ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Postprocessed parameters are defined in util/gl/grb/postprocess.f90 and util/gl/grb/postp_pressure_level.f90. Some more popular parameters are listed:\nPseudo satellite pictures\nTotal precipitation and snow\nWind (gust) speed and direction\nCloud base, cloud top, cloud mask and significant cloud top","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"For a comprehensive list please check the output information for each cycle. NOTE that all parameters may not be implemented in gl","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"To produce \"postprocessed\" MSLP and accumulated total precipitation and visibility use the following namelist, nam_FApp:\n&naminterp\n pppkey(1:3)%shortname='pres','tp','vis',\n pppkey(1:3)%levtype='heightAboveSea','heightAboveGround','heightAboveGround'\n pppkey(1:3)%level=  0, 0, 0,\n pppkey(1:3)%tri=  0, 4, 0,\n lwrite_pponly= .TRUE.,\n/\ngl -p ICMSHHARM+0003 -o output_pp.grib -n nam_FApp\nNote:\nSet lwrite_pponly as true to only write the postprocessed fields to file\nSet lwrite_pponly as false write all fields will be written to the file, input fields as well as the postprocessed fields.","category":"page"},{"location":"PostProcessing/gl/#Vertical-interpolation","page":"GL","title":"Vertical interpolation","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"gl can be used to carry out vertical interpolation of parameters. Four types are available","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"HeightAboveSea, give height above sea in meters\nHeightAboveGround, give height above ground in meters\nHeightAboveGroundHighPrecision, give height above ground in centimeters\nisobaricInHpa, give height above sea in hPa\nTo interpolation temperature to 1.40m (level 140 in cm) use the following namelist, nam_hl:\n&naminterp\n pppkey(1:1)%shortname='t',\n pppkey(1:1)%levtype='heightAboveGroundHighPrecision',\n pppkey(1:1)%level=  140,\n pppkey(1:1)%tri=  0,\n vint_z_order=1,\n lwrite_pponly= .TRUE.,\n/\ngl -p ICMSHHARM+0003 -o output_hl.grib -n nam_hl\nNote:\nVertical interpolation to z levels is controlled by VINTZORDER: 0 is nearest level, 1 is linear interpolation\nTo height interpolation (Levls 500, 850 and 925 in hPa, type=100) use the following namelist, nam_pl:\n&naminterp\n pppkey(1:3)%shortname='t','t','t',\n pppkey(1:3)%levtype='isobaricInhPa','isobaricInhPa','isobaricInhPa',\n pppkey(1:3)%level=  500, 850, 925,\n pppkey(1:3)%tri=  0, 0, 0,\n vint_z_order=1,\n lwrite_pponly= .TRUE.,\n/\ngl -p ICMSHHARM+0003 -o output_pl.grib -n nam_pl","category":"page"},{"location":"PostProcessing/gl/#Horizontal-interpolation","page":"GL","title":"Horizontal interpolation","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Interpolation/resampling between different geometries such as regular lat lon, Lambert conformal, Polar steregraphic, rotated lat lon and rotated Mercator is possible with gl\nThe interpolation methods available are:\nnearest grid-point (order=-2)\nmost representative grid-point (order=-1)\nnearest grid-point (order=0)\nbi-linear (order=1)\nbi-quadratic (order=2, mask not respected)\nbi-cubic (order=3, mask not respected)\nExample of (an Irish) rotated lat lon domain, nam_FArotll:\n&naminterp\n outgeo%nlon=50,\n outgeo%nlat=50,\n outgeo%nlev=-1,\n outgeo%gridtype='rotated_ll',\n outgeo%west=-2.5,\n outgeo%south=-2.5,\n outgeo%dlon=0.1,\n outgeo%dlat=0.1,\n outgeo%polon=-6.7,\n outgeo%polat=-36.2,\n order= 1,\n/","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"where DLON/DLAT are in degrees.The HIRLAM Domain Tool may be of use for viewing rotated lat lon domains.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"gl -p ICMSHHARM+0003 -n nam_FArotll  -o output.grib","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Example of a lambert domain\n&naminterp\n  outgeo%nlon       =  50 ,\n  outgeo%nlat       =  50,\n  outgeo%nlev       =  -1,\n  outgeo%gridtype   =  'lambert',\n  outgeo%west       =  15.0\n  outgeo%south      =  50.0\n  outgeo%dlon       = 10000.\n  outgeo%dlat       = 10000.\n  outgeo%projlat    =  60.\n  outgeo%projlat2   =  60.\n  outgeo%projlon    =  15.\n/\nwhere DLON/DLAT are in meters.The HIRLAM Domain Tool may be of use for viewing rotated lat lon domains.\nExample polar stereographic projection\n&naminterp\n  outgeo%nlon       =  50 ,\n  outgeo%nlat       =  50,\n  outgeo%nlev       =  -1,\n  outgeo%gridtype   =  'polar_stereographic',\n  outgeo%west       =  15.0\n  outgeo%south      =  50.0\n  outgeo%dlon       = 10000.\n  outgeo%dlat       = 10000.\n  outgeo%projlat    =  60.\n  outgeo%projlon    =  15.\n/\nwhere DLON/DLAT are in meters.Note: the GRIB1 standard assumes that the projection plane is at 60 degrees north whereas HARMONIE assumes it is at 90 degrees north.\nExample rotated Mercator\n&naminterp\n  outgeo%nlon       =  50 ,\n  outgeo%nlat       =  50,\n  outgeo%nlev       =  -1,\n  outgeo%projection =  11,\n  outgeo%west       =  15.0\n  outgeo%south      =  50.0\n  outgeo%dlon       = 10000.\n  outgeo%dlat       = 10000.\n  outgeo%projlat    =  60.\n  outgeo%projlon    =  15.\n/\nwhere DLON/DLAT are in metersNote: rotated Mercator is not supported in GRIB1.\nGeographical points is a special case of projection 0 use namelist file, nam_FAgp:\n&naminterp\n  outgeo%nlon=3 ,\n  outgeo%nlat=1,\n  outgeo%nlev=-1,\n  outgeo%gridtype='regular_ll',\n  outgeo%arakawa=  'a',\n  order             =   0,\n  readkey(1:3)%shortname='t','u','v',\n  readkey(1:3)%levtype='heightAboveGround','heightAboveGround','heightAboveGround',\n  readkey(1:3)%level=   2,  10, 10,\n  readkey(1:3)%tri=  0, 0, 0,\n  linterp_field     = f,\n  gplat          = 57.375,57.35,57.60\n  gplon          = 13.55,13.55,14.63\n/\nThe result will be written to a ASCII file with the name gpYYYYMMDDHHLLL.\ngl -p ICMSHHARM+0003 -n nam_FAgp \ncat gp20140702_1200+003","category":"page"},{"location":"PostProcessing/gl/#Extract-(crop)-a-sub-domain","page":"GL","title":"Extract (crop) a sub-domain","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"gl can be used to \"cut out\" a sub-domain from an input file using the namelist namCUT:","category":"page"},{"location":"PostProcessing/gl/#Crop-using-lower-left-and-upper-right-coordinates","page":"GL","title":"Crop using lower left and upper right coordinates","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"&naminterp\nistart = 150\njstart = 150\nistop = 350\njstop = 350\n/","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Use this command:","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"gl input.grib -n namCut -o cutout.grib","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Another way of specifying your sub domain is to define how many points to exclude in the end","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"&naminterp\nistart = 150\njstart = 150\nistop = -10\njstop = -10\n/","category":"page"},{"location":"PostProcessing/gl/#Crop-using-SW,NE-corner-and/or-number-of-points","page":"GL","title":"Crop using SW,NE corner and/or number of points","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Here, you specify any of the SouthWest, NorthEast corners and/or the number of gridpoints","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"&naminterp\noutgeo%gridtype = 'crop',\noutgeo%nlat  = 200,\noutgeo%nlon  = 300,\noutgeo%south =  50.155,\noutgeo%west  = -12.88,\n/","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Or ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"&naminterp\noutgeo%gridtype = 'crop',\noutgeo%north =  58.277,\noutgeo%east  =  12.3,\noutgeo%south =  50.155,\noutgeo%west  = -12.88,\n/","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"If you specify outgeo%gridtype as 'crop', the SouthWest corner will be translated to lower left grid coordinates and Nlat,Nlon will translate to upper right coordinates.  You may specify any of SW, NE, nlat/nlon.  Priority is given to SW, NE.  The behaviour is as follows:","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"SW and NE have priority, these will anchor either corner.  If a corner is not specified, Nlat/Nlon will extend from the other corner.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"If only one coordinate is specified, the other corner becomes the corner of the input domain.  So: ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Specify only SW and you get a crop from there to the NE corner of the  input domain.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Specify only NE and you get a crop from the SW corner of the input  domain.\nSpecify SW and NE and you get a crop between these corners","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Specify SW and Nlat/Nlon and you get Nlat x Nlon from SW corner.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Specify NE and Nlat/Nlon and you get Nlat x Nlon south and west of NE corner.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Specify SW, NE and Nlat/Nlon and you get a crop between SW/NE corners.  Nlat/Nlon are ignored.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Specify only Nlat/Nlon and you get Nlat x Nlon from SW corner of the   input domain.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"The crop must be within the original domain unless you set","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"ldemand_inside = .FALSE.","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"in the namelist.  In this case, the crop will be adjusted to lie within the original domain and the output will be smaller than Nlat x Nlon.  In the case where the requested crop lies entirely outside the original domain, the program will abort.","category":"page"},{"location":"PostProcessing/gl/#Output-to-several-files","page":"GL","title":"Output to several files","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"It is possible to let gl read data once and do processing loops with these data. Let us look at an example namelist","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"&naminterp\n OUTPUT_FORMAT='MEMORY'\n/\n&naminterp\n INPUT_FORMAT='MEMORY'\n OUTPUT_FORMAT='GRIB'\n OUTFILE='test1.grib'\n/\n&naminterp\n INPUT_FORMAT='MEMORY'\n OUTPUT_FORMAT='GRIB'\n OUTFILE='test2.grib'\n READKEY%FANAME='SNNNTEMPERATURE'\n/\n&naminterp\n INPUT_FORMAT='MEMORY'\n OUTPUT_FORMAT='GRIB'\n READKEY%FANAME='CLSTEMPERATURE'\n outgeo%nlon       =  50 ,\n outgeo%nlat       =  50,\n outgeo%nlev       =  -1,\n outgeo%gridtype   =  'polar_stereographic',\n outgeo%west       =  15.0\n outgeo%south      =  50.0\n outgeo%dlon       = 10000.\n outgeo%dlat       = 10000.\n outgeo%projlat    =  60.\n outgeo%projlon    =  15.\n OUTFILE='test3.grib'\n/","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"In the first loop we read data and store it in memory. In the second look we read the data from memory and output to the file test1.grib. Then we make two more loops where we in the first one only output a subset and in the last one also do an interpolation to a new grid. The data in memory is however still untouched.","category":"page"},{"location":"PostProcessing/gl/#Input-from-several-files","page":"GL","title":"Input from several files","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"It's also possible to read several files and write them into one. This is used to gather the various FA fields written from the IO-server. A typical namelist would look like","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"&naminterp\n maxfl=28,\n output_format='MEMORY',\n output_type = 'APPEND',\n input_format='FA',\n infile='forecast/io_serv.000001.d/ICMSHHARM+0003.gridall',\n/\n&naminterp\n output_format='MEMORY',\n output_type = 'APPEND',\n input_format='FA',\n infile='forecast/io_serv.000002.d/ICMSHHARM+0003.gridall',\n/\n...\n&naminterp\n input_format = 'MEMORY',\n output_format= 'GRIB'\n output_type  = 'NEW',\n outfile      = 'test.grib'\n/","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Where maxfl tells how many files that will be read.","category":"page"},{"location":"PostProcessing/gl/#domain_prop","page":"GL","title":"domain_prop","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"domain_prop is used do extract various properties from a file. ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Climate:  $MPPGL $BINDIR/domain_prop -DOMAIN_CHECK $LCLIMDIR/m$M1 -f || \\","category":"page"},{"location":"PostProcessing/gl/#Check-an-existing-domain-with-a-namelist-specification","page":"GL","title":"Check an existing domain with a namelist specification","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"domain_prop -DOMAIN_CHECK -f CLIMATE_FILE","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"The geometry is read from fort.10 and the program aborts if the new and old geometries differs. See  scr/Climate for an example.","category":"page"},{"location":"PostProcessing/gl/#Check-if-Q-is-in-gridpoint-or-spectral-representation","page":"GL","title":"Check if Q is in gridpoint or spectral representation","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"domain_prop -f -QCHECK FAFILE","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"returns 1 if Q is spectral and 0 if Q is in gridpoint space.","category":"page"},{"location":"PostProcessing/gl/#Check-if-a-specific-field-is-present","page":"GL","title":"Check if a specific field is present","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"domain_prop -f -CHECK_FIELD S001CLOUD_FRACTI","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"returns 1 if S001CLOUD_FRACTI is found, 0 otherwise","category":"page"},{"location":"PostProcessing/gl/#Check-the-number-of-levels-in-a-file","page":"GL","title":"Check the number of levels in a file","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"domain_prop -f -NLEV FAFILE  ","category":"page"},{"location":"PostProcessing/gl/#Check-the-geographical-extension-of-the-domain","page":"GL","title":"Check the geographical extension of the domain","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"domain_prop -f -MAX_EXT FAFILE  ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"This is used in several places to determine the domain to be extracted from MARS or limit the observations sample. Another way is to provide the projection parameters of your domain as input","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"domain_prop -MAX_EXTR \\\n-NLON $NLON -NLAT $NLAT \\\n-LATC $LATC -LONC $LONC \\\n-LAT0 $LAT0 -LON0 $LON0 \\\n-GSIZE $GSIZE","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"To get the geographical position of the lower left corner use","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"domain_prop -f -LOW_LEFT FAFILE  ","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"To print out the important projection parameters in a file use:","category":"page"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"domain_prop -f -4JB FAFILE","category":"page"},{"location":"PostProcessing/gl/#Get-time-information-from-a-file","page":"GL","title":"Get time information from a file","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"domain_prop -f -DATE FAFILE","category":"page"},{"location":"PostProcessing/gl/#fldextr-and-obsextr","page":"GL","title":"fldextr and obsextr","text":"","category":"section"},{"location":"PostProcessing/gl/","page":"GL","title":"GL","text":"Read about the verification extraction programs here","category":"page"},{"location":"System/MFaccess/#Using-Météo-France-Servers","page":"MF Access","title":"Using Météo-France Servers","text":"","category":"section"},{"location":"System/MFaccess/#Introduction","page":"MF Access","title":"Introduction","text":"","category":"section"},{"location":"System/MFaccess/","page":"MF Access","title":"MF Access","text":"The procedure to get access to MF servers and their read-only git repository is outlined here","category":"page"},{"location":"System/MFaccess/#First-steps","page":"MF Access","title":"First steps","text":"","category":"section"},{"location":"System/MFaccess/","page":"MF Access","title":"MF Access","text":"Discuss your requirements for access to MF servers with the HIRLAM System project leader, Daniel Santos (dsantosm@aemet.es).\nDownload two forms \"Undertaking for the use of Météo-France computer resources\" and \"Demande d'authorisation de conexion au résau de Météo Franc\" from http://www.cnrm.meteo.fr/aladin/spip.php?article157. \nThe \"Undertaking for the use of Météo-France computer resources\" form  is to be signed by you only\nThe \"Demande d'authorisation de conexion au résau de Météo France\" must be signed by you and your department head. It must also include an institute stamp. You should enter details in Contacts, Compte d'accesés aux machines du Centre de Cacul and at the bottom with authorization from you institute manager with institute stamp.   - A scan of both forms with a brief introductory note should be sent to Eric Escaliere (eric.escaliere@meteo.fr) and cc'ed to Daniel Santos (dsantosm@aemet.es) and Claude Fischer (claude.fischer@meteo.fr).\nBe careful with the \"Machine du client\". I had to specify the name and IP address of my institute's Firewall server as this is what the outside world sees when I access external servers from my PC.\nMétéo-France will send (by post) your username (Identificateur) and password (Mot de passe) for log in.\nThe authentication process itself remains in two steps (first “parme”, then target), as before. \nA few specific examples follow (see MF's instructions for full details):\nbeaufix:","category":"page"},{"location":"System/MFaccess/","page":"MF Access","title":"MF Access","text":"ewhelan@realin23:gcc-8.3.1:.../~> which beaufix\nalias beaufix='telnet beaufix.meteo.fr'\n\t/usr/bin/telnet\newhelan@realin23:gcc-8.3.1:.../~> beaufix \nTrying 137.129.240.110...\nConnected to beaufix.meteo.fr.\nEscape character is '^]'.\nCheck Point FireWall-1 authenticated Telnet server running on mascarpone\nUser: whelane\npassword: your_parme_password\nUser whelane authenticated by FireWall-1 authentication\n\nConnected to 137.129.240.110\nRed Hat Enterprise Linux Server release 6.9 (Santiago)\nKernel 2.6.32-696.6.3.el6.x86_64 on an x86_64\nbeaufixlogin0 login: whelane\nPassword: your_ldap_password\nLast login: Tue Oct 13 10:15:53 from gw2.met.ie\n _                           __  _       \n| |                         / _|(_)      \n| |__    ___   __ _  _   _ | |_  _ __  __\n| '_ \\  / _ \\ / _` || | | ||  _|| |\\ \\/ /\n| |_) ||  __/| (_| || |_| || |  | | >  < \n|_.__/  \\___| \\__,_| \\__,_||_|  |_|/_/\\_\\ \n\n[whelane@beaufixlogin0 ~]$ ","category":"page"},{"location":"System/MFaccess/#What-next?-**TO-BE-CONFIRMED**","page":"MF Access","title":"What next? TO BE CONFIRMED","text":"","category":"section"},{"location":"System/MFaccess/#Access-to-MF-servers-via-parme","page":"MF Access","title":"Access to MF servers via parme","text":"","category":"section"},{"location":"System/MFaccess/","page":"MF Access","title":"MF Access","text":"Once you are happy that you can access PARME from your PC you should once again contact Eric Escaliere (eric.escaliere@meteo.fr) and request login details for merou (Eric will send you a temporary password) and LDAP login details to front-id to enable access to COUGAR, YUKI, BEAUFIX and ID-FRONT\nAn automatic e-mail will be sent from expl-identites@meteo.fr with you LDAP repository password.\nfront-id requires certain criteria for your password. These are detailed in French below. When you have received LDAP login details for front-id:\newhelan@eddy:~> telnet parme.meteo.fr\nTrying 137.129.20.1...\nConnected to parme.meteo.fr.\nEscape character is '^]'.\nCheck Point FireWall-1 authenticated Telnet server running on parmesan\nUser: whelane\npassword: ********\nUser whelane authenticated by FireWall-1 authentication\nHost: front-id\n\nConnected to id-front\nRed Hat Enterprise Linux AS release 4 (Nahant Update 5)\nKernel 2.6.9-55.ELsmp on an x86_64\nlogin: whelane\nPassword: \nLast login: Mon Nov  4 05:14:22 from gw2.met.ie\nBienvenue EOIN WHELAN\nVous pouvez changer votre mot de passe\n-------------------------------------------------------------------------\n- Controle de validite sur les mots de passe avant de poster la demande -\n- Le OLD doit etre fourni. -\n- Au moins 8 car, au plus 20 car. -\n- Au moins 2 car. alpha et 2 car. non-alpha. -\n- Ne pas ressembler a UID NAME et OLD sur une syllabe de + de 2 car. -\n-------------------------------------------------------------------------\n-------------------------------------------------------------------------\nHello EOIN WHELAN\nYou may change your password\n-------------------------------------------------------------------------\n- Validity control before demand acceptation -\n- You must enter the old password first -\n- The new password must contain: -\n- At least 8 characters, 20 characters maximum -\n- At least 2 alphanumeric characters and 2 non-alphanumeric characters -\n- The passwd must contain a part of UID NAME -\n-------------------------------------------------------------------------\nChanging password for user 'whelane(56064)'.\nEnter login(LDAP) password: \nNew password: \nRe-enter new password: \nVotre mot de passe a ete change\nWhen you have received login details for merou from Eric:\newhelan@eddy:~> telnet parme.meteo.fr\nTrying 137.129.20.1...\nConnected to parme.meteo.fr.\nEscape character is '^]'.\nCheck Point FireWall-1 authenticated Telnet server running on parmesan\nUser: whelane\npassword: ********\nUser whelane authenticated by FireWall-1 authentication\nHost: merou\n\nConnected to merou\nRed Hat Enterprise Linux Server release 5.6 (Tikanga)\nKernel 2.6.18-238.el5 on an x86_64\nlogin: whelane\nPassword: \nLast login: Tue Nov  5 10:06:35 from gw2.met.ie\n[whelane@merou ~]$ passwd\nChanging password for user whelane.\nChanging password for whelane\n(current) UNIX password: \nNew UNIX password: \nRetype new UNIX password: \npasswd: all authentication tokens updated successfully.\n[whelane@merou ~]$ ","category":"page"},{"location":"System/MFaccess/#Access-to-(read-only)-MF-git-arpifs-git-repository","page":"MF Access","title":"Access to (read-only) MF git arpifs git repository","text":"","category":"section"},{"location":"System/MFaccess/","page":"MF Access","title":"MF Access","text":"MF use ssh keys to allow access to their read-only git repository. If approved by the HIRLAM System PL you should request access to the repository by sending a request e-mail to Eric Escaliere and cc'ed to Daniel Santos and Claude Fischer your ssh public key attached.","category":"page"},{"location":"System/MFaccess/","page":"MF Access","title":"MF Access","text":"Once you have been given access you can create a local clone by issuing the following commands:","category":"page"},{"location":"System/MFaccess/","page":"MF Access","title":"MF Access","text":"cd $HOME\nmkdir arpifs_releases\ncd arpifs_releases\ngit clone ssh://reader054@git.cnrm-game-meteo.fr/git/arpifs.git","category":"page"},{"location":"System/MFaccess/","page":"MF Access","title":"MF Access","text":"Happy gitting!","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#MUSC","page":"MUSC","title":"MUSC","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/#MUSC-using-the-develop-branch-(CY46)-in-the-git-repository","page":"MUSC","title":"MUSC using the develop branch (CY46) in the git repository","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"If you find any issues with any of the instructions or scripts, feel free to notify Emily Gleeson (emily.gleesonATmet.ie) and Eoin Whelan (eoin.whelanATmet.ie)","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Currently a \"reference\" test case, called musc_ref, works on ATOS, as well as  the ARMCU cases (with and without SURFEX for both AROME and HARMONIE namelists) and the two microphysics-related cases (supercooled liquid) developed by Bjorg Jenny Engdahl in cycle 40. ","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Some instructions on how to use MUSC are included below. See https://asr.copernicus.org/articles/17/255/2020/ for some information on HARMONIE-AROME experiments using MUSC but note that the scripts have changed somewhat since that paper was written.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Set-up-MUSC","page":"MUSC","title":"Set up MUSC","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Get the code:\nmkdir -p $SCRATCH/harmonie_releases/git/HCY46\ncd $SCRATCH/harmonie_releases/git/HCY46\ngit clone git@github.com:Hirlam/Harmonie.git\ncd Harmonie\ngit checkout dev-CY46h1 \n# If you already have a clone of the code but want to update it to the latest version, \nuse \"git pull\" rather than \"git branch\".\nSet up a MUSC experiment using HARMONIE scripting \nIn this example the ECMWF.atos config file is used. \nmkdir -p $HOME/hm_home/test_0001\ncd $HOME/hm_home/test_0001\n$SCRATCH/harmonie_releases/git/HCY46/Harmonie/config-sh/Harmonie setup -r $SCRATCH/harmonie_releases/git/HCY46/Harmonie/ -h ECMWF.atos \nCompile your experiment (still in $HOME/hm_home/test_0001)\n$SCRATCH/harmonie_releases/git/HCY46/Harmonie/config-sh/Harmonie install BUILD_WITH=cmake\n# Note that for the ARMCU cases cmake needs FFT modifications (not yet committed by Yurii)\nSome MUSC specific settings including copying over scripts and a check that Harmonie setup has been run\n$SCRATCH/harmonie_releases/git/HCY46/Harmonie/util/musc/scr/musc_setup.sh -r $SCRATCH/harmonie_releases/git/HCY46/Harmonie/\nGenerate your namelist, unless you're using an idealised case with pre-defined namelists (so for ARMCU* you do not generate the namelists for example). If you wish to change the radiation scheme (RADSCHEME- RAYFM (IFS) or RAY (ACRANEB2)) or how you use aerosols (BDAER - cams or none), you need to edit ecf/configexp.h in your expt before running muscnamelist.sh. For using NRT aerosols, they need to be included in your input files already e.g. the MUSCIN* files should come from a 3D NRT aerosol expt.\ncd $HOME/hm_home/test_0001\n./musc_namelist.sh -h\n./musc_namelist.sh -l <length of run> -i <the ID name on the generated namelist files e.g. DEF> \n                     -[N nudging - optional]\nGet a copy of the input files\ncd $SCRATCH\nretrieve the input files from https://github.com/Hirlam/HarmonieMuscData","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Note that if you need to do experiments with 2 patches etc, ensure you derive some MUSC input files yourself using 3D HARMONIE-AROME files run with 2 patches. MUSC*REFL65* input files have only 1 patch. Changing MUSC namelists won't enable 2 patch output from a MUSC run.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Run-MUSC","page":"MUSC","title":"Run MUSC","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Musc_ref","page":"MUSC","title":"Musc_ref","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"The reference test is a X-hr experiment (change CSTOP in musc_namelist.sh if you wish to change the run length) and produces Out *lfa files for each model time-step of the time period. ICM* files are produced at each hour.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Run your experiment\ncd $HOME/hm_home/test_0001\n./musc_run.sh -h\n./musc_run.sh -d $SCRATCH/muscCY46InputData/musc_ref -n REFL65 -i DEF [ -e ECOCLIMAP_PATH]\n# optional path for ECOCLIMAP data may be given. For musc_ref -i must be given as no \n# namelists are provided with this experiment and must be generated before musc_run.sh \n# is executed. For the idealised cases, if -i is not specified -i becomes the name of \n# the idealised case once the namelist files are copied to $HOME/hm_home/test_0001 e.g. \n# for armcu the namelist files become namelist_atm_armcu etc.\n","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#ARMCU","page":"MUSC","title":"ARMCU","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"** Note that these will not work until we can compile with FFTW.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"This is an idealized SCM test case, the \"Sixth GCSS WG-1 case (ARM—Atmospheric Radiation Measurement)\", focussing on the diurnal cycle of cumulus clouds over land (Brown et al, 2002, Lenderink et al, 2004. The input files and namelist settings have been taken from /src/validation/mitraille/namelist/L1ARO but the atmospheric namelist needed editing for use in our environment. Atmospheric and surface forcings are included in the MUSC input files and the namelists in the util/musc/test/armcu directory are set up specifically for this case and are hence not edited by musc_run.sh.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Run your experiment\ncd $HOME/hm_home/test_0001\n./musc_run.sh -h\n./musc_run.sh -d $HOME/muscCY46InputData/ARMCU_HAR -n ARMCU (There are now 4 ARMCU experiments to chose from e.g. ARMCU_EB and ARMCUs_EB are ones that use AROME namelists, ARMCU_Har and ARMCUs_Har use HARMONIE-AROME namelists. Currently, the results are a bit different.)","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#MUSC-Output","page":"MUSC","title":"MUSC Output","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/#DDH-Toolbox","page":"MUSC","title":"DDH Toolbox","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"The outputs from a MUSC run are small files in lfa format. DDH tools can be used to handle these files.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"To download the DDH toolbox, go to https://www.umr-cnrm.fr/gmapdoc/spip.php?article19 and download the tarball. Untar it and within the tools folder run ./install. Now the various \"tools\" are compiled. For example lfaminm $file shows you the max, min and mean of all the output variables in a file. lfac $file $var shows the value(s) of $var in $file e.g. lfac Out.000.0000.lfa PTS shows you surface temperature. In order to be able to use the plotting scripts below, you'll need the lfac tool in your path. ","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"For example on ATOS, I set the following paths (may differ a bit for you depending on where you downloaded the ddhtools to). Perhaps add to your .bashrc file:","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"export PATH=$HOME/ddhtoolbox/tools/lfa/:$PATH\nexport DDHI_BPS=$HOME/ddhtoolbox/ddh_budget_lists/\nexport DDHI_LIST=$HOME/ddhtoolbox/ddh_budget_lists/conversion_list","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Plot-output-time-series-from-the-MUSC-output-lfa-files","page":"MUSC","title":"Plot output time-series from the MUSC output lfa files","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"cd $HOME/hm_home/test_0001\n./musc_plot1Dts.sh -d <musc-data-dir>\n\n## python based plotting scripts and \"default\" png plots \n## will be produced in $HOME/hm_home/test_0001/plots1Dts\n","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Extract-output-from-the-MUSC-output-ICM*-fa-files-and-plot-time-series-using-these","page":"MUSC","title":"Extract output from the MUSC output ICM* fa files and plot time-series using these","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"By default you get ICM* files on the hour - you can change the namelist should you require a higher frequency.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"cd $HOME/hm_home/test_0001\n./musc_convertICM2ascii.sh -l <number_hours_in_expt_run> -f <path_to_your_musc_output>\n\n## Generates an OUT ascii file for each atm and sfx ICM* input file\n## ICM files have additional input not in lfa files e.g. TKE which is useful - also similar to 3D outputs\n\n./musc_plot_profiles_ICMfiles.sh -d <data-dir> -p <parameter-model-level> -l <run_length_hours>\n","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Creating-your-own-input-files","page":"MUSC","title":"Creating your own input files","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"A converter script, musc_convert.sh, is available to extract a MUSC column from a model state file (ICMSHHARM+HHHH). musc_convert.sh is a Bash script that calls gl_grib_api to carry the data conversions.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Extract-a-MUSC-input-file","page":"MUSC","title":"Extract a MUSC input file","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"cd $HOME/hm_home/test_0001\n./musc_convert.sh -d $HOME/muscCY46InputData/harm_arome/ -c extr3d -n REFIRL -l 53.5,-7.5 -t 6\nmkdir $HOME/muscCY46InputData/musc_refirl\ncp MUSCIN_REFIRL_atm.fa MUSCIN_REFIRL_sfx.fa MUSCIN_REFIRL_pgd.fa $HOME/muscCY46InputData/musc_refirl/","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Convert-MUSC-FA-to-MUSC-ASCII","page":"MUSC","title":"Convert MUSC FA to MUSC ASCII","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"cd $HOME/hm_home/test_0001\n./musc_convert.sh -c fa2ascii -d $HOME/muscCY46InputData/musc_refirl -n REFIRL\nls -ltr\ncp MUSCIN_REFIRL_atm.ascii MUSCIN_REFIRL_sfx.ascii $HOME/muscCY46InputData/musc_refirl/","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Convert-MUSC-ASCII-to-MUSC-FA","page":"MUSC","title":"Convert MUSC ASCII to MUSC FA","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"cd $HOME/hm_home/test_0001\n./musc_convert.sh -c ascii2fa -d $HOME/muscCY46InputData/musc_refirl -n REFIRL\nls -ltr","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Forcing-in-MUSC","page":"MUSC","title":"Forcing in MUSC","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"musc_convert.sh includes forcing for temperature (11), humidity (51) and wind speed (32) . ","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"You may edit the following lines to include other forcing:","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"  PPPKEY(1:4)%shortname  = 'ws','#','#','#',\n  PPPKEY(1:4)%faname  = '#','SNNNFORC001','SNNNFORC002','SNNNFORC003'\n  PPPKEY(1:4)%levtype = 'hybrid','hybrid','hybrid','hybrid',\n  PPPKEY(1:4)%level   = -1,-1,-1,-1,\n  PPPKEY(1:4)%pid     = 32,-1,-1,-1,\n  PPPKEY(1:4)%nnn     = 0,0,0,0,\n  PPPKEY(1:4)%lwrite  = F,T,T,T,\n  IFORCE              = 11,51,32,","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Further information on forcing is available here: MUSC/Forcing","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#MUSC-local-adaptation","page":"MUSC","title":"MUSC local adaptation","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/#KNMI-workstations","page":"MUSC","title":"KNMI workstations","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"The following files were added to make it possible to run MUSC on KNMI workstations:","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"config-sh/config.LinuxPC-MPI-KNMI\nconfig-sh/submit.LinuxPC-MPI-KNMI\nutil/makeup/config.linux.gfortran.mpi-knmi ","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"for use with the setup script:","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"./musc_setup.sh [...] -c LinuxPC-MPI-KNMI","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"In addition, the following workaround has to be applied to be able to run the REFL65 test case:","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"$ git diff src/ifsaux/utilities/echien.F90\ndiff --git a/src/ifsaux/utilities/echien.F90 b/src/ifsaux/utilities/echien.F90\nindex 55d5ce94e..694c87d83 100644\n--- a/src/ifsaux/utilities/echien.F90\n+++ b/src/ifsaux/utilities/echien.F90\n@@ -532,7 +532,7 @@ IF((KINF == 0).OR.(KINF == -1).OR.(KINF == -2).OR.(KINF == -3)) THEN\n            & 'LEVEL ',JFLEV,' : ',&\n            & 'FILE = ',ZVALH(JFLEV), ' ; ARGUMENT = ',PVALH(JFLEV)\n           IERRA=1\n-          IERR=1\n+!         IERR=1\n         ENDIF\n         IF(ABS(ZVBH(JFLEV)-PVBH(JFLEV)) > PEPS) THEN\n           WRITE(KULOUT,*) ' VERTICAL FUNCTION *B* MISMATCH ON ',&","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Then you are ready to compile:","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"remove the file experimentislocked from the experiment directory.\nremove the directory with your previous build (if any).\nstart the compile with the musc_compile.sh script","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"When starting the MUSC run, add the PATH to mpirun and the libraries:","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"export PATH=$PATH:/usr/lib64/openmpi/bin\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib64/openmpi/lib\n./musc_run.sh [...]","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#MUSC-FAQ","page":"MUSC","title":"MUSC FAQ","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"If there is an error, what files do I look in? NODE.001_01 and lola in your output directory.\nHow to I handle the output files? The output files are of the form Out.XXX.XXXX and appear in your output directory. There are in lfa format and can be handled using ddh tools. See the bash script musc_plot1Dts.sh for ideas. There are also ICM*lfa output files that are also handy for plotting profiles - use musc_convertICM2ascii.sh to convert these files to ASCII and musc_plot_profiles_ICMfiles.sh to plot some profiles e.g. TKE, cloud liquid etc.\nI ran a different idealised case but did not get different results? The likely reason for this is that you did not delete the namelists from your experiment directory. If the namelists are there, the musc_run.sh script neither creates them nor copies them from the repository.\nHow do I create a new idealised case? This is not straightforward but the following was used to create the ASTEX cases in cy43 using info from cy38: https://www.overleaf.com/7513443985ckqvfdcphnng\nHow can I access a list of MUSC output parameters? Ensure you have the ddhtoolbox compiled. Then use lfaminm $file on any of your output files and it will show what is there. To look at a particular variable try lfac $file parametere.g.lfac file PTS(for surface temperature). You can use cat to copy the values to an ASCII file for ease of use (e.g.lfac file PTS > ASCIIfile`).  \nIs MUSC similar to the full 3D model version - is the physics the same? Yes, if you checkout develop then you have MUSC up-to-date with that.\nDo I need to recompile the model if I modify code? Yes, if you modify code in a single file you must recompile the code but do not delete the original compiled model first. This will recompile relatively quickly. If you modify code in multiple files and you change what variables are passed between files, then you must delete your original compiled model and recompile the code. This will take longer to recompile. ","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#MUSC-variable-names","page":"MUSC","title":"MUSC variable names","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"A list of variable names found in the MUSC lfa output files can be found here. Please note that this is not a complete list of MUSC output parameters (yet). The variables in regular ICMSH... fa output are documented here","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Outstanding-Issues","page":"MUSC","title":"Outstanding Issues","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"ARMCU and Jenny's cases run without surface physics, radiation etc and hence return NANs in apl_arome. To circumvent this on ecgb, we needed to compile less strictly. This needs to be investigated further.\nThe ASTEX cases currently do not run on ecgb but work perfectly at Met Eireann - debugging needed.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#MUSC-using-EMS","page":"MUSC","title":"MUSC using EMS","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"In this section a description of how to install and run MUSC using EMS is provided. This is based on compilation and execution in a Ubuntu 20.04 container (tested using Apptainer on the Atos) and use of the EMS system to execute MUSC and convert the out to NetCDF. EMS is primarily developed by Romain Roehrig (Meteo France) https://github.com/romainroehrig/EMS.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Start-your-container","page":"MUSC","title":"Start your container","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Log in to hpc-login on the Atos\nLoad the Apptainer module and start the Ubuntu 20.04 container:","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"user@hpc-login:~> module load apptainer\nuser@hpc-login:~> /home/dui/musc_ubuntu.sif\nApptainer>","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Further details concerning Apptainer on the Atos are available here: https://confluence.ecmwf.int/display/UDOC/HPC2020%3A+Container+support","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Compile-the-code","page":"MUSC","title":"Compile the code","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"I (Eoin) have not had time to sort out compilation using CMake but the following instructions provide a minimalist approach to compile the code using makeup:","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Apptainer> mkdir -p $HOME/hm_home/musc_exp_1\nApptainer> cd $HOME/hm_home/musc_exp_1\nApptainer> /path/to/harmonie_git/dev-CY46h1/config-sh/Harmonie setup -r /path/to/harmonie_git/dev-CY46h1 -h ubuntu20_nompi\nApptainer> EXP=$(basename $(pwd))\nApptainer> . Env_system\nApptainer> export MUSCWORK=$PERM/musc_build/${EXP}\nApptainer> mkdir -p $MUSCWORK\nApptainer> mkdir -p ${MUSCWORK}/lib/src\nApptainer> mkdir -p ${MUSCWORK}/lib/util\nApptainer> rsync -tvaz /path/to/harmonie_git/dev-CY46h1/src/ ${MUSCWORK}/lib/src/\nApptainer> rsync -tvaz /path/to/harmonie_git/dev-CY46h1/util/ ${MUSCWORK}/lib/util/\nApptainer> cd $MUSCWORK/\nApptainer> ${MUSCWORK}/lib/util/makeup/build -n 2 config.${HARMONIE_CONFIG}","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Install-EMS","page":"MUSC","title":"Install EMS","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"The foolowing instructions provide details on how to download a HIRLAM version of EMS and install locally in your own account:","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Apptainer> cd $HOME\nApptainer> mkdir EMS_test\nApptainer> cd EMS_test\nApptainer> git clone git@github.com:ewhelan/EMS.git -b hirlam EMS_git/EMS\nApptainer> cd EMS_git/EMS/\nApptainer> mkdir build\nApptainer> cd build/\nApptainer> export EMS_DIR=$HOME/EMS_test_new/ems_install\nApptainer> cmake .. -DCMAKE_INSTALL_PREFIX=$EMS_DIR && make && ctest && make install\nApptainer> export PATH=${EMS_DIR}/bin:$PATH","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#EMS-Cases","page":"MUSC","title":"EMS Cases","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"The table below lists the cases available in EMS and results of early tests.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Case Status Reference(s)\nGABLS1 REF ❌[1] \nGABLS1 MESONH ❌[1] \nGABLS4 STAGE3 ❌[1] \nGABLS4 STAGE3-SHORT ❌[1] \nAYOTTE 00SC ✔️ \nAYOTTE 00WC ✔️ \nAYOTTE 03SC ✔️ \nAYOTTE 05SC ✔️ \nAYOTTE 05WC ✔️ \nAYOTTE 24SC ✔️ \nIHOP REF ✔️ \nSCMS REF ✔️ \nRICO SHORT ✔️ \nRICO MESONH ✔️ \nARMCU REF ✔️ \nARMCU MESONH ✔️ \nARMCU E3SM ✔️ \nBOMEX REF ❌[2] \nMPACE REF ✔️ \nFIRE REF ✔️ \nSANDU REF ✔️ \nSANDU FAST ✔️ \nSANDU SLOW ✔️ \nAMMA REF ✔️ \nDYNAMO NSA3A ✔️ Takes a long time!\nDYNAMO NSA3A_D1 ✔️ \nDYNAMO NSA3A_D30 ❌[3] \nDYNAMO NSA3A_MJO1 ✔️ ","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"[1]: Issue with SURFEX namelist","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"[2]: Python issue L241 $EMS_DIR/ems/prep_init_forc_atm_GMAP.py","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"[3]: Missing data_input.nc ","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Running-EMS","page":"MUSC","title":"Running EMS","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Here are some instructions on how to use EMS to execute idealised cases","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Apptainer> export EMS_DIR=$HOME/EMS_test_new/ems_install\nApptainer> export PATH=${EMS_DIR}/bin:$PATH\nApptainer> mkdir $HOME/EMS_test_new/ems_exec\nApptainer> cd $HOME/EMS_test_new/ems_exec\nApptainer> cp ${EMS_DIR}/share/config/config_46h1_HARMAROME_DEV.py .\n##### edit his file to point to your binaries\nApptainer> export PYTHONPATH=${EMS_DIR}:$(pwd)\nApptainer> ems_list_cases.py\nApptainer> # MUSC.py -config config_46h1_HARMAROME_DEV.py -case $CASE  -subcase $SUBCASE\nApptainer> MUSC.py -config config_46h1_HARMAROME_DEV.py -case ARMCU  -subcase REF","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"Output and log file can be found in $HOME/EMS_test_new/ems_exec/simulations/46t1/46h1_HARMAROME_DEV/${CASE}/${SUBCASE}","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/#Analysing-results-using-Atlas","page":"MUSC","title":"Analysing results using Atlas","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"These instructions work on Atos and should work well on your PC!","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC/","page":"MUSC","title":"MUSC","text":"user@hpc-login~> module load python3\nuser@hpc-login~> cd $HOME\nuser@hpc-login~> git clone git@github.com:ewhelan/SCM-atlas.git -b hirlam SCM-atlas_git/ewhelan/hirlam\nuser@hpc-login~> export PATH=$HOME/SCM-atlas_git/ewhelan/hirlam/apptools:$PATH\nuser@hpc-login~> export PYTHONPATH=$HOME/SCM-atlas_git/ewhelan/hirlam:$PYTHONPATH\nuser@hpc-login~> export ATLAS_CONFIG=\"\"\nuser@hpc-login~> mkdir -p $HOME/Atlas1D/hirlam\nuser@hpc-login~> cd $HOME/Atlas1D/hirlam\nuser@hpc-login~> mkdir config\nuser@hpc-login~> cp $HOME/SCM-atlas_git/ewhelan/hirlam/examples/config/config_HARM.py config/\n### edit config/config_HARM.py\nuser@hpc-login~> run_atlas1d.py -config config/config_HARM.py","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#Running-Harmonie-on-Atos","page":"Running on Atos","title":"Running Harmonie on Atos","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#Before-you-start","page":"Running on Atos","title":"Before you start","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"New Harmonie users will require membership of the hirald user group at ECMWF. Please contact the HIRLAM System Manager, Daniel Santos, to make this request on your behalf. Futhermore ECMWF will have to setup a virtual machine for you to run the ecFlow server on (see here).","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"tip: Tip\nTo share your experiments with the members of the hirald group do: chmod 755 $HOME $SCRATCH $PERM $HPCPERM\nchgrp -R hirald $HOME/hm_home $SCRATCH/hm_home $PERM/HARMONIE $HPCPERM/hm_home\nchmod g+s $HOME/hm_home $SCRATCH/hm_home $PERM/HARMONIE $HPCPERM/hm_homeThe chmod g+s sets the SGID bit which will ensure that new experiments created in hm_home will automatically be in the hirald group","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#Configure-your-experiment-(option-1)","page":"Running on Atos","title":"Configure your experiment (option 1)","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Create an experiment directory under $HOME/hm_home and use the master script Harmonie to set up a minimum environment for your experiment. E.g for the tagged harmonie-43h2.2.1\nmkdir -p $HOME/hm_home/my_exp\ncd $HOME/hm_home/my_exp\nln -sf ~hlam/harmonie_release/git/tags/harmonie-43h2.2.1/config-sh/Harmonie\n./Harmonie setup -r ~hlam/harmonie_release/git/tags/harmonie-43h2.2.1 -h ECMWF.atos\nOr to use the dev-C46h1 branch\nln -sf ~hlam/harmonie_release/git/branches/dev-CY46h1/config-sh/Harmonie\n./Harmonie setup -r ~hlam/harmonie_release/git/branches/dev-CY46h1 -h ECMWF.atos\nwhere\n-r tells which version to use.  Check the directories under ~hlam/harmonie_release/git to see the available versions. \n-h tells which configuration files to use. At ECMWF config.ECMWF.atos is the default one. For harmonie-43h2.2 use -hconfig.aa`","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"This would give you the default setup which currently is AROME physics with CANARI+OI_MAIN surface assimilation and 3DVAR upper air assimilations with 3h cycling on a domain covering Denmark using 2.5km horizontal resolution and 65 levels in the vertical.\nNow you can edit the basic configuration file ecf/config_exp.h to configure your experiment scenarios. Modify specifications for domain, data locations, settings for dynamics, physics, coupling host model etc. Read more about the options here. You can also use some of the predefined configurations by calling Harmonie with the -c option:\n./Harmonie setup -r PATH_TO_HARMONIE -h YOURHOST -c CONFIG -d DOMAIN\nwhere CONFIG is one of the setups defined in scr/Harmonie_configurations.pm. If you give -c without an argument or a non existing configuration a list of configurations will be printed.\nIn some cases you might have to edit the general system configuration file config-sh/config.ECMWF.atos. See here for further information. \nThe rules for how to submit jobs on Atos are defined in config-sh/submit.ECMWF.atos. See here for further information\nIf you experiment in data assimilation you might also want to change settings in scr/include.ass.","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#Configure-your-experiment-using-github-repo-(option-2)","page":"Running on Atos","title":"Configure your experiment using github repo (option 2)","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Instead of using the repositories in ~hlam/harmonie_release/git/ you may choose to work with another remote, such as your fork of the Harmonie repository on GitHub. The advantage of this approach is that it is easier to work on feature branches and continuously update that. When done, you will be ready for a pull-request instantly.","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"You will need to add your SSH key to your GitHub account to be able to push to your remote, if you haven't done so already","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Make a fork of the Harmonie repository. From now we assume your fork will be located at https://github.com/<user>/Harmonie.\nLog in to ATOS as usual and perform the following commands:","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"mkdir -p $PERM/hm_home && cd $PERM/hm_home/\ngit clone -b <remote_branch> git@github.com:<user>/Harmonie.git  <exp_name>\ncd <exp_name>\ngit checkout -b <feature/branch_name>\nconfig-sh/Harmonie setup -r $(pwd) -h ECMWF.atos","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Where the git clone command clones a spefic branch into a directory called <exp_name>. git checkout with the -b flag, then creates a new branch for you to work on. Call it something meaningful. Then the experiment is set up as usual, but using your local repository as reference to itself.","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Then you do some work and when ready to commit something you do","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"git add <path to modified file>\ngit commit --author \"Name <name@host>\" -m \"Commit message\"\ngit push --set-upstream origin <feature/branch_name>","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Specifying --set-upstream origin <feature/branch_name> to git push is only necessary the first time you push your branch to the remote. When ready you can now go to GitHub and make a pull-request to the Harmonie repository from your fork.","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#Start-your-experiment","page":"Running on Atos","title":"Start your experiment","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Launch the experiment by giving start time, DTG, end time, DTGEND","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"./Harmonie start DTG=YYYYMMDDHH DTGEND=YYYYMMDDHH\n# e.g., ./Harmonie start DTG=2012122400 DTGEND=2012122406","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"If successful, Harmonie will identify your experiment name and start building your binaries and run your forecast. If not, you need to examine the ECFLOW log file $HM_DATA/ECF.log. $HM_DATA is defined in your Env_system file. At ECMWF $HM_DATA=$SCRATCH/hm_home/$EXP where $EXP is your experiment name. Read more about where things happen further down.","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#Continue-your-experiment","page":"Running on Atos","title":"Continue your experiment","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"If your experiment have successfully completed and you would like to continue for another period you should write","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"./Harmonie prod DTGEND=YYYYMMDDHH","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"By using prod you tell the system that you are continuing the experiment and using the first guess from the previous cycle. The start date is take from a file progress.log created in your $HOME/hm_home/my_exp directory. If you would have used start the initial data would have been interpolated from the boundaries, a cold start in other words.","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#Start/Restart-of-ecflow_ui","page":"Running on Atos","title":"Start/Restart of ecflow_ui","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"To start the graphical window for ECFLOW","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"./Harmonie mon","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"The graphical window runs independently of the experiment and can be closed and restarted again with the same command. With the graphical interface you can control and view logfiles of each task. ","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#Making-local-changes","page":"Running on Atos","title":"Making local changes","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Very soon you will find that you need to do changes in a script or in the source code. Once you have identified which file to edit you put it into the current $HOME/hm_home/my_exp directory, with exactly the same subdirectory structure as in the reference. e.g, if you want to modify a namelist setting ","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"./Harmonie co nam/harmonie_namelists.pm   # retrieve default namelist harmonie_namelists.pm\nvi nam/harmonie_namelists.pm              # modify the namelist","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Next time you run your experiment the changed file will be used. You can also make changes in a running experiment. Make the change you wish and rerun the InitRun task from the viewer. The InitRun task copies all files from your local experiment directory to your working directory $HM_DATA. Once your InitRun task is complete your can rerun the task you are interested in. If you wish to recompile something you will also have to rerun the Build tasks.","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#Directory-structure","page":"Running on Atos","title":"Directory structure","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#SCRATCH","page":"Running on Atos","title":"$SCRATCH","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"In $SCRATCH/hm_home/$EXP you will find ","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Directory Content\nbin Binaries\nlib Source code synced from HM_LIB and compiled code\nlib/src Object files and source code (if you build with makeup, set by MAKEUPBUILDDIR)\nlib/util Utilities such as makeup, gl_grib_api or oulan\nclimate Climate files\nYYYYMMDD_HH Working directory for the current cycle. If an experiment fails it is useful to check the IFS log file, NODE.001_01, in the working directory of the current cycle. The failed job will be in a directory called something like Failed_this_job.\narchive Archived files. A YYYY/MM/DD/HH structure for per cycle data. ICMSHHARM+NNNN and ICMSHHARM+NNNN.sfx are atmospheric and surfex forecast output files\nextract Verification input data. This is also stored on the permanent disk $HPCPERM/HARMONIE/archive/$EXP/parchive/archive/extract\nECF.log Log of job submission","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#ECFS","page":"Running on Atos","title":"ECFS","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Since $SCRATCH is cleaned regularly we need to store data permanently on ECFS, the EC file system, as well. There are two options for ECFS, ectmp and ec. The latter is a permanent storage and first one is cleaned after 90 days. Which one you use is defined by the`ECFSLOC variable. To view your data type e.g.\nels ectmp:/$USER/harmonie/my_exp\nThe level of archiving depends on ARSTRATEGY in ecf/config_exp.h. The default setting will give you one YYYY/MM/DD/HH structure per cycle data containing:\nSurface analysis, ICMSHANAL+0000[.sfx]\nAtmospheric analysis result MXMIN1999+0000\nBlending between surface/atmospheric analysis and cloud variable from the first guess LSMIXBCout\nICMSHHARM+NNNN and ICMSHHARM+NNNN.sfx are atmospheric and surfex forecast model state files\nPFHARM* files produced by the inline postprocessing\nICMSHSELE+NNNN.sfx are surfex files with selected output\nGRIB files for fullpos and surfex select files\nLogfiles in a tar file logfiles.tar\nObservation database and feedback information in odb_stuff.tar.\nExtracted files for obsmon in sqlite.tar\nClimate files are stored in the climate directory\nOne directory each for vfld and vobs data respectively for verification data","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#PERM","page":"Running on Atos","title":"$PERM","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Directory Content\nHARMONIE/$EXP ecflow log and job files\nhm_lib/$EXP/lib Scipts, config files, ecf and suite, source code (not compiled, set by HM_LIB). Reference with experiment's changes on top","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#HPCPERM","page":"Running on Atos","title":"$HPCPERM","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"In $HPCPERM/hm_home/$EXP","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Directory Content\nparchive/archive/extract/ Verification input data.","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#HOME-on-ecflow-gen-:({user})-001","page":"Running on Atos","title":"$HOME on ecflow-gen-{user}-001","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Directory Content\necflow_server/ ecFlow checkpoint and log files","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#Cleanup-of-old-experiments","page":"Running on Atos","title":"Cleanup of old experiments","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"danger: Danger\nThese commands may not work properly in all versions. Do not run the removal before you're sure it's OK","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Once you have complete your experiment you may wish to remove code, scripts and data from the disks. Harmonie provides some simple tools to do this. First check the content of the different disks by","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Harmonie CleanUp -ALL","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Once you have convinced yourself that this is OK you can proceed with the removal.","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Harmonie CleanUp -ALL -go ","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"If you would like to exclude the data stored on e.g ECFS ( at ECMWF ) or in more general terms stored under HM_EXP ( as defined in Env_system ) you run ","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Harmonie CleanUp -d","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"to list the directories intended for cleaning. Again, convince yourself that this is OK and proceed with the cleaning by","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Harmonie CleanUp -d -go","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"You can always remove the data from ECFS directly by running e.g.","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"erm -R ec:/YOUR_USER/harmonie/EXPERIMENT_NAME ","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"or","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"erm -R ectmp:/YOUR_USER/harmonie/EXPERIMENT_NAME ","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"For more information about cleaning with Harmonie read here\nFor more information about the ECFS commands read here","category":"page"},{"location":"System/ECMWF/RunningHarmonieOnAtos/#Debugging-Harmonie-with-ARM-DDT","page":"Running on Atos","title":"Debugging Harmonie with ARM DDT","text":"","category":"section"},{"location":"System/ECMWF/RunningHarmonieOnAtos/","page":"Running on Atos","title":"Running on Atos","text":"Follow instructions here. Use Run DDT client on your Personal Computer or End User Device ","category":"page"},{"location":"Build/Build_with_makeup/#Building-with-MAKEUP","page":"Makeup","title":"Building with MAKEUP","text":"","category":"section"},{"location":"Build/Build_with_makeup/#Background","page":"Makeup","title":"Background","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Makeup is an alternative mechanism to build the HARMONIE system Instead of using GMKPACK to build the libraries and binaries, standard GNU make (gmake) procedures are used, making build of executables an easier task. Also parallel make comes for free, thus enhanced turn-around time for build process. Furthermore, rebuilds and change of compiler flags – either per project and/or per source files basis – are now trivial to do.","category":"page"},{"location":"Build/Build_with_makeup/#MAKEUP-very-quickly","page":"Makeup","title":"MAKEUP very quickly","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"The process of using the MAKEUP system in stand-alone fashion is described next.","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Lets define two helper variables for the presentation purposes:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"The variable $HARMONIE_SRC refers to the directory, where the AROME source code is situated. Another variable $HARMONIE_MAKEUP refers to the directory, where build configuration files and MAKEUP's scripts are located. ","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"#!sh\n# In ksh/bash\nexport HARMONIE_SRC=/some/path/harmonie/src\nexport HARMONIE_MAKEUP=/some/path/harmonie/util/makeup\n# In csh/tcsh\nsetenv HARMONIE_SRC /some/path/harmonie/src\nsetenv HARMONIE_MAKEUP /some/path/harmonie/util/makeup","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Usually $HARMONIE_MAKEUP is $HARMONIE_SRC/../util/makeup , but it doesn't have to be (e.g. in FMI's production system the $HARMONIE_MAKEUP is situated on a separate disk than the source code $HARMONIE_SRC) – and MAKEUP can handle this now.","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"The process of building HARMONIE executable contains just a few steps:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Goto directory $HARMONIE_MAKEUP and create/edit your configuration file (config.*). Beware of preferred naming convention:\nconfig.<MET-INSTITUTE>.<MACHINE-PLATFORM>.<COMPILER-NAME>.<FUNDAMENTAL-OPTIONS>.<OPTIONAL-OPTIONS>\nRun MAKEUP's configure script under $HARMONIE_SRC (for example):\ncd $HARMONIE_SRC\n$HARMONIE_MAKEUP/configure $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\nIf applicable, adjust environment settings before launching of make. e.g., on some platforms, one needs to remember loading adequate modules, such as for DMI Cray XT5,\nmodule swap PrgEnv-pgi PrgEnv-pathscale  # if pathscale is to be used\nmodule swap xt-mpt xt-mpt/3.5.0\nmodule swap xt-asyncpe/3.8 xt-asyncpe/3.4\nGoto $HARMONIE_SRC directory and type make (or gmake, if make is non-GNU make). Redirect output to a file & terminal:\ncd $HARMONIE_SRC\ngmake 2>&1 |  tee logfile  # ksh/bash\ngmake      |& tee logfile  # csh/tcsh","category":"page"},{"location":"Build/Build_with_makeup/#Using-MAKEUP-to-build-auxlibs-(bufr,-gribex,-rgb)","page":"Makeup","title":"Using MAKEUP to build auxlibs (bufr, gribex, rgb)","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"You can now build EMOS- and related libraries by using the MAKEUP. All you need to know is what is your sources.<arch> that you would use to build this stuff anyway. Pass that generic name to the MAKEUP's configure through -E option and you're in business. An example for FMI's Cray:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"cd $HARMONIE_SRC\n$HARMONIE_MAKEUP/configure -E sources.crayxt $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\ngmake","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"This will create extra libs (so called MY_SYSLIBS) libbufr.a, libgribex.a and librgb.a and they will end up being linked into your executables, like MASTERODB.","category":"page"},{"location":"Build/Build_with_makeup/#Using-MAKEUP-to-build-also-util/gl-tools","page":"Makeup","title":"Using MAKEUP to build also util/gl -tools","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"HARMONIE utility package GL as located in util/gl directory can also be built as part of MAKEUP process, if option -G is also given to the configure:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"cd $HARMONIE_SRC\n$HARMONIE_MAKEUP/configure -G $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\ngmake","category":"page"},{"location":"Build/Build_with_makeup/#Using-MAKEUP-to-build-also-Oulan-and/or-Monitor-tools","page":"Makeup","title":"Using MAKEUP to build also Oulan and/or Monitor -tools","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"HARMONIE utility package MONITOR and obs-preprocessor OULAN can also be build with MAKEUP. If you add option -B , then you will get Oulan and Monitor executables built, too. Or you can be more selective and oopt only for oulan with -b oulan, or just monitor -b monitor :","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"cd $HARMONIE_SRC\n# Request for building both oulan & monitor, too\n$HARMONIE_MAKEUP/configure -B $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\n# .. or add oulan only :\n$HARMONIE_MAKEUP/configure -b oulan $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\n# .. or add monitor only :\n$HARMONIE_MAKEUP/configure -b monitor $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\ngmake","category":"page"},{"location":"Build/Build_with_makeup/#Building-objects-away-from-HARMONIE_SRC-directory","page":"Makeup","title":"Building objects away from $HARMONIE_SRC-directory","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"If you do not want to pollute your source directories with objects and thus making it hard to recognize which files are under version handling system SVN and which ain't (... although SVN command svn -q st would tell ...), then use -P option. This will redirect compilations away from source code, under $HARMONIE_SRC/../makeup.ZZZ, where ZZZ is the suffix of your config-file, e.g. FMI.cray_xt5m.pathscale.mpi+openmp.","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"The operation sequence is as follows:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"cd $HARMONIE_SRC\n$HARMONIE_MAKEUP/configure [options] -P $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\ncd $HARMONIE_SRC/../makeup.FMI.cray_xt5m.pathscale.mpi+openmp/src\ngmake","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"The drawback with this approach is that whenever there is an update in the master source directories, you need to run lengthy configure in order to rsync the working directory up to date. We may need to introduce a separate command for this to avoid full rerun of configure.","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"You can also use lowercase -p option with argument pointing to a directory-root, where to compile:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"cd $HARMONIE_SRC\n$HARMONIE_MAKEUP/configure [options] -p /working/path $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\ncd /working/path/src\ngmake","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Now, it is important to understand that this /working/path has no connection to version handling i.e. if you change something  in your master copy (say : issue a svn up-command), then your working directory remains unaltered. To synchronize it, do the following:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"cd /working/path/src\ngmake rsync","category":"page"},{"location":"Build/Build_with_makeup/#More-details","page":"Makeup","title":"More details","text":"","category":"section"},{"location":"Build/Build_with_makeup/#Re-running-configure","page":"Makeup","title":"Re-running configure","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Afterwards you can rerun configure as many times as you wish.  Please note that the very first time is always slowed (maybe 10 minutes) as interface blocks for arp/ and ald/ projects are generated.","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Usually running configure many times is not necessary – not even when you have changed your config-file (!) – except when interface blocks needs to be updated/re-created (-c or -g options). For example, when subroutine/function call argument list has changed. Then the whole config+build sequence can be run under $HARMONIE_SRC as follows:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"cd $HARMONIE_SRC\n# -c option: Check if *some* interface blocks need regeneration and regenerate\n$HARMONIE_MAKEUP/configure -c $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\n# -g option: Force to regenerate interface blocks \n# $HARMONIE_MAKEUP/configure -g $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\ngmake","category":"page"},{"location":"Build/Build_with_makeup/#Changing-the-number-of-tasks-for-compilation","page":"Makeup","title":"Changing the number of tasks for compilation","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"The number of tasks used for gmake-compilations is set by default to 8. See NPES parameter in HARMONIE_MAKEUP/defaults.mk To change the default, you can have two choices:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Add NPES to your config-file, for example set it to 2:\nNPES=2\nInvoke gmake with NPES parameter, e.g. set it to 10:\ngmake NPES=10","category":"page"},{"location":"Build/Build_with_makeup/#Inserting-DRHOOK-for-Meso-projects","page":"Makeup","title":"Inserting DRHOOK for Meso-projects","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"To insert DrHook profiling automatically for mpa/ and mse/ projects, reconfigure with -H option:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"cd $HARMONIE_SRC\n$HARMONIE_MAKEUP/configure -H $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"You can also pick and choose either mpa/ or mse/ projects with -h option (can be supplied several times):","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"cd $HARMONIE_SRC\n$HARMONIE_MAKEUP/configure -h mpa $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\n$HARMONIE_MAKEUP/configure -h mse $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\n# The following are the same as if the option -H was used\n$HARMONIE_MAKEUP/configure -h mpa -h mse -h surfex $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\n$HARMONIE_MAKEUP/configure -h mpa:mse:surfex $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"In the future it may not be necessary to insert DrHook automagically, if the insertion has been  done in the svn (version handling) level.","category":"page"},{"location":"Build/Build_with_makeup/#Speeding-up-compilations-by-use-of-RAM-disk","page":"Makeup","title":"Speeding up compilations by use of RAM-disk","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"To further speedup compilation and if you have several GBytes of Linux RAM-disk (/dev/shm) available, do the following:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Create your personal RAM-disk subdirectory and check available disk space\nmkdir /dev/shm/$USER\ndf -kh /dev/shm/$USER\nReconfigure with RAM-disk either by defining LIBDISK in your config-file or running\ncd $HARMONIE_SRC\n$HARMONIE_MAKEUP/configure -L /dev/shm/$USER $HARMONIE_MAKEUP/config.FMI.cray_xt5m.pathscale.mpi+openmp\nAlso define TMPDIR to point to /dev/shm/USER to allow compiler specific temporary files on RAM-disk\n# In ksh/bash-shells:\nexport TMPDIR=/dev/shm/$USER\ngmake 2>&1 |  tee logfile\n# In csh/tcsh-shells:\nsetenv TMPDIR /dev/shm/$USER\ngmake      |& tee logfile","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Please note that the step-2 creates all libraries AND executablus under the directory pointed by the -L argument. Object files and modules still, however, are placed under corresponding source directories.","category":"page"},{"location":"Build/Build_with_makeup/#What-if-you-run-out-of-RAM-disk-space-?","page":"Makeup","title":"What if you run out of RAM-disk space ?","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Sometimes you may find that the disk space becomes limited in /dev/shm/$USER. Then you have an option to supply LIBDISK parameter directly to gmake-command without need to reconfigure:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"gmake LIBDISK=`pwd`","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"This usually increases the throughput time as creation of the AROME executable to disk rather than RAM-disk may be 5-10 times slower. But at least you won't run out of disk space.","category":"page"},{"location":"Build/Build_with_makeup/#How-is-ODB-related-stuff-handled-?","page":"Makeup","title":"How is ODB related stuff handled ?","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"The Observational DataBase (ODB) is a complicated beast for good reasons. Unlike any other project, which produce just one library per project, correct use of ODB in variational data assimilation requires several libraries.","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"The trick to manage this with MAKEUP is to create a bunch of symbolic links pointing to $HARMONIE_SRC/odb/ -project directory. There will be one (additional) library for each link. And then we choose carefully the correct subdirectories and source codes therein to be compiled for each library.","category":"page"},{"location":"Build/Build_with_makeup/#Specific-ODB-libraries,-their-meaning-and-the-source-files-included","page":"Makeup","title":"Specific ODB-libraries, their meaning & the source files included","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Library Description Source files\nlibodb ODB core library lib/ & aux/ : [a-z]*.F90 [a-z]*.c\n  module/ & pandor/module : *.F90\nlibodbport Interface between IFS (ARPEGE/ALADIN/AROME) & ODB cma2odb/ & bufr2odb/ : *.F90\n – also contains BUFR2ODB routines pandor/extrtovs & pandor/fcq & pandor/mandalay : *.F90\nlibodbdummy ODB-related dummies lib/   : [A-Z]*.F90 [A-Z]*.c\nlibodbmain ODB tools, main programs (C & Fortran) tools/ : [A-Z]*.F90 *.c *.F\nlibPREODB ERA40 database (not needed, but good for debugging) ddl.PREODB/*.sql  , ddl.PREODB/*.ddl\nlibCCMA Compressed Central Memory Array database (minimization) ddl.CCMA/*.sql    , ddl.CCMA/*.ddl\nlibECMA Extended Central Memory Array database (obs. screening) ddl.ECMA/*.sql    , ddl.ECMA/*.ddl\nlibECMASCR Carbon copy of ECMA for obs. load balancing between PEs ddl.ECMASCR/*.sql , ddl.ECMASCR/*.ddl","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"From the file $HARMONIE_MAKEUP/configure you can also find how different files are nearly hand-picked for particular libraries. Search for block","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":" if [[ \"$d\" = @(odb|odbport|odbdummy|odbmain)]] ; then\n     case \"$d\" in\n              odb) case \"$i\" in\n                   lib|aux)              files=$(\\ls -C1 [a-z]*.F90 [a-z]*.c 2>/dev/null) ;;\n                   module|pandor/module) files=$(\\ls -C1 *.F90 2>/dev/null) ;;\n                   esac ;;\n          odbport) case \"$i\" in\n                   cma2odb|bufr2odb)                           files=$(\\ls -C1 *.F90 2>/dev/null) ;;\n                   pandor/extrtovs|pandor/fcq|pandor/mandalay) files=$(\\ls -C1 *.F90 2>/dev/null) ;;\n                   esac ;;\n         odbdummy) [[ \"$i\" != \"lib\"]] || files=$(\\ls -C1 [A-Z]*.F90 [A-Z]*.c 2>/dev/null) ;;\n          odbmain) [[ \"$i\" != \"tools\"]] || files=$(\\ls -C1 [A-Z]*.F90 *.c *.F 2>/dev/null) ;;\n     esac\n elif [[ \"$d\" = @($case_odbs)]] ; then\n   [[ \"$i\" != \"ddl.$d\"]] || {\n       files=$(\\ls -C1 *.ddl *.sql 2>/dev/null)\n       mkdepend=$CMDROOT/sfmakedepend_ODB\n   }\n else\n  ... ","category":"page"},{"location":"Build/Build_with_makeup/#Handling-SQL-query-and-data-layout-files","page":"Makeup","title":"Handling SQL-query and data layout files","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"For SQL-query compilations (ODB/SQL queries are translated into C-code for greater performance), odb98.x SQL-compiler executable is also built as a first thing in the MAKEUP process.","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Queries and data definition layouts (DDL-files) are always under <database>/ddl.<database>/ directory.","category":"page"},{"location":"Build/Build_with_makeup/#Miscellaneous-stuff","page":"Makeup","title":"Miscellaneous stuff","text":"","category":"section"},{"location":"Build/Build_with_makeup/#Selective-compilation","page":"Makeup","title":"Selective compilation","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"It is very easy to deviate from the generic compilation options for certain source files or even projects. If you want to change compiler option (say) from -O3 to -O2 for routine src/arp/pp_obs/pppmer.F90, you can add the following lines at the end of your config-file:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"pppmer.o: FCFLAGS := $(subst -O3,-O2,$(FCFLAGS))","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"If you want to apply this to all pppmer*.F90-routines, then you need to enter the following \"wildcard\"-sequence:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"pppme%.o: FCFLAGS := $(subst -O3,-O2,$(FCFLAGS))","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Note by the way that for some reason we need to use pppme%.o as the more natural (from Unix) pppmer%.o would choose only routines pppmertl.F90 and pppmerad.F90, not the routine pppmer.F90 at all!","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Applying different compiler flags for project (say) arp only, then one can put the following at the end of config-file:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"ifeq ($(PROJ),arp)\n%.o:  FCFLAGS := $(subst -O3,-O2,$(FCFLAGS))\nendif","category":"page"},{"location":"Build/Build_with_makeup/#(Re-)building-just-one-project","page":"Makeup","title":"(Re-)building just one project","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Sometime you could opt for rebuilding only (say) the xrd-project i.e. libxrd.a. This can be done as follows:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"gmake PROJ=xrd","category":"page"},{"location":"Build/Build_with_makeup/#Cleaning-up-files","page":"Makeup","title":"Cleaning up files","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"You can clean up by","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"gmake clean","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"... or selectively just the project arp:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"gmake PROJ=arp clean","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"This clean does not wipe out makefiles i.e. you don't have to rerun configure after this.","category":"page"},{"location":"Build/Build_with_makeup/#Restoring-and-cleaning-up-the-state-of-HARMONIE_SRC","page":"Makeup","title":"Restoring and cleaning up the state of $HARMONIE_SRC","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"The following command you can run only once before issuing another configure command. It will remove all related object and executable files as well as generated makefiles, logfiles etc. stuff which was generated by MAKEUP's configure :","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"cd $HARMONIE_SRC\ngmake veryclean\n\n# .. or alternatively :\n$HARMONIE_MAKEUP/unconfigure","category":"page"},{"location":"Build/Build_with_makeup/#Ignoring-errors","page":"Makeup","title":"Ignoring errors","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"Sometimes it is useful to enforce compilations even if one or more routines fail to compile. In such cases recommended syntax is:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"gmake -i\n\n# or not to mess up the output, use just one process for compilations\n\ngmake NPES=1 -i","category":"page"},{"location":"Build/Build_with_makeup/#Creating-precompiled-installation","page":"Makeup","title":"Creating precompiled installation","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"If you want to provide precompiled libraries, objects, source code to other users so that they do not have to start compilation from scratch, then make a distribution or precompiled installation as follows:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"gmake PRECOMPILED=/a/precompiled/rootdir precompiled","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"After this the stuff you just compiled ends up in directory /a/precompiled/rootdir with two subdirectories : src/ and util/. All executables are currently removed.","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"You can repeat this call, and it will just rsync the modified bits.","category":"page"},{"location":"Build/Build_with_makeup/#Update/check-your-interface-blocks-outside-configure","page":"Makeup","title":"Update/check your interface blocks outside configure","text":"","category":"section"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"The configure has options -c or -g to check up or enforce for (re-)creation of interface blocks of projects arp and ald. To avoid full and lengthy configure-run, you can just do the following:","category":"page"},{"location":"Build/Build_with_makeup/","page":"Makeup","title":"Makeup","text":"gmake intfb","category":"page"},{"location":"DataAssimilation/ObservationOperators/#Observation-operators","page":"HOP_DRIVER","title":"Observation operators","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"This documentation summarises the observation operator in HARMONIE and the use of the HOP_DRIVER tool. The test harness, HOP_DRIVER, calls the observation operator and generates FG departures without calling any model code or initialising any model modules. Firstly, the IFS is used to dump a single-observation gom_plus to file from the 1st trajectory of an experiment. Dumping multiple observations would require a more complex and full-featured dump (good file format, multi-process parallel). For code refactoring HOP_DRIVER can be used to test changes to the observation operator of a particular observation type.","category":"page"},{"location":"DataAssimilation/ObservationOperators/#HARMONIE-and-HOP_DRIVER","page":"HOP_DRIVER","title":"HARMONIE and HOP_DRIVER","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"The HOP_DRIVER program was first added to CY42R2 code. The tool was initially implemented to test refactoring of the IFS observation operator code src/arpifs/op_obs/hop.F90. At the moment the refactor branch (branches/refactor/harmonie) is the only HARMONIE code set that includes HOP_DRIVER. Instructions on how to prepare the code and run HOP_DRIVER using HARMONIE are outlined below.  Presentation made at [wiki:HirlamMeetings/ModelMeetings/ObOpWorkshop OOPS Observation Operator Workshop] may provide some useful background information.","category":"page"},{"location":"DataAssimilation/ObservationOperators/#Comments-on-the-branch","page":"HOP_DRIVER","title":"Comments on the branch","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"Code changes were required in order to compile cy42r2bf.04 + mods (provided by MF/ECMWF) in the HARMONIE system: [14312], [14325], [14326], [14330], [14331], [14332], [14333], [14334].\nChanges were made to makeup in order to compile HOP_DRIVER correctly: [14310], [14327], [14328], [14329], [14335], [14362], [14382], [14392].\nIncluded in [14362] is a change to ODBSQLFLAGS which is set to \"ODBSQLFLAGS=-O3 -C -UCANARI -DECMWF ODBEXTRAFLAGS\" in order to use ECMWF flavoured ODB used by HOP_DRIVER\nOn cca GNU compilers 4.9 are not fully supported, ie I had to build GRIB-API and NetCDF locally using gcc/gfortran 4.9 on cca\nAn environment variable, HOPDIR, is used to define the location of necessary input data for HOP_DRIVER\nAn environment variable, HOPCOMPILER, is used by the HOP_driver script to define the compiler used. This is used to compare results.","category":"page"},{"location":"DataAssimilation/ObservationOperators/#Running-on-ecgb/cca","page":"HOP_DRIVER","title":"Running on ecgb/cca","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"cd $SCRATCH\nmkdir -p harmonie_releases/branches/refactor\ncd harmonie_releases/branches/refactor\nsvn co https://svn.hirlam.org/branches/refactor/harmonie-42R2\ncd $HOME\nmkdir -p hm_home/rfexp\ncd hm_home/rfexp\n$SCRATCH/harmonie_releases/branches/refactor/harmonie-42R2/config-sh/Harmonie setup -r $SCRATCH/harmonie_releases/branches/refactor/harmonie-42R2\n$SCRATCH/harmonie_releases/branches/refactor/harmonie-42R2/config-sh/Harmonie hop_driver","category":"page"},{"location":"DataAssimilation/ObservationOperators/#Running-on-local-platforms","page":"HOP_DRIVER","title":"Running on local platforms","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"So far, only METIE.LinuxRH7gnu, which uses gfortran 4.9 and openmpi, has been tested. Input data for the amsua test case is available on ECFS at ECMWF: ec:/dui/hopdata.tar.gz","category":"page"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"cd $HOME\nmkdir -p harmonie_releases/branches/refactor\ncd harmonie_releases/branches/refactor\nsvn co https://svn.hirlam.org/branches/refactor/harmonie-42R2\ncd $HOME\nmkdir -p hm_home/rfexp\ncd hm_home/rfexp\n$HOME/harmonie_releases/branches/refactor/harmonie-42R2/config-sh/Harmonie setup -h METIE.LinuxRH7gnu -r $HOME/harmonie_releases/branches/refactor/harmonie-42R2\n$HOME/harmonie_releases/branches/refactor/harmonie-42R2/config-sh/Harmonie hop_driver","category":"page"},{"location":"DataAssimilation/ObservationOperators/#HOPOBS:-amsua","page":"HOP_DRIVER","title":"HOPOBS: amsua","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"Currently there is only one observation type, AMSU-A (HOPOBS=amsua), available for testing with HOP_DRIVER. Alan Geer (ECMWF) has already carried out the refactoring of the HOP code related to AMSU-A observations. A single observation is provided in the ECMA and is used to test the refactoring of the HOP code. To carry out the testing of the amsua refactoring HOPOBS should be set to amsua in ecf/config_exp.h.","category":"page"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"reportype@hdr obstype@hdr sensor@hdr statid@hdr stalt@hdr date@hdr time@hdr degrees(lat) degrees(lon) report_status@hdr datum_status@body obsvalue@body varno@body vertco_type@body\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 12 173.28 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 12 158.86 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 3 227.40 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 3 260.82 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 256.90 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 239.60 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 12 NULL 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 3 217.69 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 209.39 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 214.05 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 223.02 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 234.42 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 245.14 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 257.18 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 12 227.91 119 3","category":"page"},{"location":"DataAssimilation/ObservationOperators/#HOP_DRIVER","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/#Using-HOP_DRIVER","page":"HOP_DRIVER","title":"Using HOP_DRIVER","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"With LHOP_RESULTS=.TRUE. HOP_DRIVER will write results to a file called hop_results${MYPROC} for comparison between online and offline results. (The results file is opened by src/arpifs/var/taskob.F90. HOP_DRIVER results are written to hop_results${MYPROC} in src/arpifs/op_obs/hop.F90:","category":"page"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":" :\n :\nIF(LHOP_RESULTS) THEN\n!$OMP CRITICAL\n  ! Output for comparison between online and offline results:\n  WRITE(CFILENAME,'(\"hop_results\",I4.4)') MYPROC\n  OPEN(NEWUNIT=IU,FILE=CFILENAME,POSITION='APPEND',ACTION='WRITE',FORM='FORMATTED')\n  DO JOBS = 1,KDLEN\n    DO JBODY=1,IMXBDY\n      IF (JBODY>ICMBDY(JOBS)) CYCLE\n      IBODY = ROBODY%MLNKH2B(JOBS)+(JBODY-1)\n      WRITE(IU,'(6I8,2F30.14)') MYPROC, KSET, JOBS, NINT(ROBHDR%DATA(JOBS,ROBHDR%SEQNO_AT_HDR)),&\n        & NINT(ROBODY%DATA(IBODY,ROBODY%VERTCO_REFERENCE_1_AT_BODY)), &\n        & NINT(ROBODY%DATA(IBODY,ROBODY%VARNO_AT_BODY)), ZHOFX(JOBS,JBODY), ZXPPB(JOBS,JBODY)\n\n    ENDDO\n  ENDDO\n  CLOSE(IU)\n!$OMP END CRITICAL\nENDIF\n :\n :","category":"page"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"The HOPdriver script (based a script provided by MF) sorts the contents of the `hopresults0001` file for comparison with some results made available by ECMWF/MF:","category":"page"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":" :\n :\n#\n# Check HOP_DRIVER results (available for gfotran and intel)\n#\nln -s $HOPDIR/${HOPOBS}/results.$HOPCOMPILER .\ncat hop_results* | sort -k1,1n -k2,2n -k3,3n -k5,5n -k6,6n > results.driver\necho\ncmp -s results.$HOPCOMPILER results.driver\nif [ $? -eq 0] ; then\n  echo \"RESULTS ARE STRICTLY IDENTICAL TO THE REFERENCE FOR HOPCOMPILER=$HOPCOMPILER :-)\"\nelse\n  echo Compare exactly against the results dumped from hop:\n  echo \"xxdiff results.$HOPCOMPILER results.driver &\"\n  diff results.$HOPCOMPILER results.driver\n  exit 1\nfi\n :\n :","category":"page"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"On cca you will find useful output from HOPDRIVER in cca:TEMP/hmhome/rfexp/archive/HOPDRIVEROUT:","category":"page"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"fort.4\nNODE.001_01\nhop_results0001\nresults.gfortran\nresults.driver","category":"page"},{"location":"DataAssimilation/ObservationOperators/#The-code","page":"HOP_DRIVER","title":"The code","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"HOP_DRIVER is a short program written by Deborah Salmond (ECMWF) to test code changes made to the observation operator. The program src/arpifs/programs/hop_driver.F90 is summarised here.","category":"page"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"The program sets up the model geometry and observations:","category":"page"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":" :\n :\nCALL GEOMETRY_SET(YRGEOMETRY)\nCALL MODEL_SET(YRMODEL)\n\nCALL IFS_INIT('gc7a')\n\nCALL SUINTDYN\n\nCALL SUGEOMETRY(YRGEOMETRY)        !From GEOMETRY_SETUP\n\nCALL SURIP(YRGEOMETRY%YRDIM)             !From MODEL_CREATE\n\n! Set up Observations, Sets\nCALL SUDIMO(YRGEOMETRY,NULOUT)     !From SU0YOMB\nCALL SUOAF              !From SU0YOMB\nCALL SUALOBS            !From SU0YOMB\nCALL SURINC             !From SU0YOMB\nCALL SETUP_TESTVAR      !From SU0YOMB\nCALL SUOBS(YRGEOMETRY)              !From CNT1\nCALL ECSET(-1,NOBTOT,0) !From OBSV\nCALL SUPHEC(YRGEOMETRY,NULOUT)\n\n! Setup varbc (from cnt1.F90) and read VARBC.cycle\nCALL YVARBC%SETUP_TRAJ\n :\n :","category":"page"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"HOP_DRIVER then loops over the number of observation sets (NSETOT) and reads a GOM PLUS for each observation set. HRETR and HOP are then called:","category":"page"},{"location":"DataAssimilation/ObservationOperators/","page":"HOP_DRIVER","title":"HOP_DRIVER","text":" :\n :\nDO ISET=1,NSETOT\n  IDLEN   = MLNSET(ISET)\n  IMXBDY = MAX(MMXBDY(ISET),1)\n\n  ALLOCATE(ZHOFX(IDLEN,IMXBDY))\n  ZHOFX=RMDI\n\n  ! READ GOM_PLUS FROM DUMP\n  CALL GOM_PLUS_READ_DUMP(YGP5,ISET)\n\n  IF(IDLEN /= YGP5%NDLEN) THEN\n    CALL ABOR1('Sets are incompatible')\n  ENDIF\n\n  :\n  :\n  :\n\n  CALL HRETR(YRGEOMETRY%YRDIMV,IDLEN,IMXBDY,ISET,1,YGP5,YVARBC)\n\n  CALL HOP(YRGEOMETRY%YRDIMV,YGP5,YVARBC,IDLEN,IMXBDY,ISET,1,LDOOPS=.TRUE.,PHOFX=ZHOFX)\n\n  !write(0,*)'ZHOFX',ZHOFX\n  DEALLOCATE(ZHOFX)\n\n  CALL GOM_PLUS_DESTROY(YGP5)\n\nENDDO\n\n :\n :","category":"page"},{"location":"System/ECMWF/ECMWF_teleport/#Monitoring-Harmonie-suites-with-Teleport","page":"Teleport","title":"Monitoring Harmonie suites with Teleport","text":"","category":"section"},{"location":"System/ECMWF/ECMWF_teleport/","page":"Teleport","title":"Teleport","text":"In order to monitor the progress of your Harmonie suite(s) at ECMWF the ecFlow GUI ecflow_ui can be used directly from your local PC/server. This relies on teleport and ssh port forwarding which is described in more detail below. ","category":"page"},{"location":"System/ECMWF/ECMWF_teleport/#Open-Teleport-connection","page":"Teleport","title":"Open Teleport connection","text":"","category":"section"},{"location":"System/ECMWF/ECMWF_teleport/","page":"Teleport","title":"Teleport","text":"This relies on a Teleport connection to ECMWF. Further details on Teleport are available here:","category":"page"},{"location":"System/ECMWF/ECMWF_teleport/","page":"Teleport","title":"Teleport","text":"[itops@reaserve ~]$ tsh status\n> Profile URL:  https://jump.ecmwf.int:443\n  Logged in as: itops@met.ie\n  Cluster:      jump.ecmwf.int\n  Roles:        *\n  Logins:       duit\n  Valid until:  2021-03-23 22:00:35 +0000 UTC [valid for 11h21m0s]\n  Extensions:   permit-X11-forwarding, permit-agent-forwarding, permit-port-forwarding, permit-pty\n\n\n* RBAC is only available in Teleport Enterprise\n  https://gravitational.com/teleport/docs/enterprise\n[itops@reaserve ~]$ ","category":"page"},{"location":"System/ECMWF/ECMWF_teleport/","page":"Teleport","title":"Teleport","text":"In order to open a new Teleport connection execute the following and submit credential via browser:","category":"page"},{"location":"System/ECMWF/ECMWF_teleport/","page":"Teleport","title":"Teleport","text":"[ewhelan@reaserve ~]$ tsh login --proxy=jump.ecmwf.int:433","category":"page"},{"location":"System/ECMWF/ECMWF_teleport/#Log-in","page":"Teleport","title":"Log in","text":"","category":"section"},{"location":"System/ECMWF/ECMWF_teleport/","page":"Teleport","title":"Teleport","text":"To log in to ECMWF's Atos:","category":"page"},{"location":"System/ECMWF/ECMWF_teleport/","page":"Teleport","title":"Teleport","text":"[itops@reaserve ~]$ ssh -X hpc-login","category":"page"},{"location":"System/ECMWF/ECMWF_teleport/","page":"Teleport","title":"Teleport","text":"The Teleport connection to ECMWF is configured as follows:","category":"page"},{"location":"System/ECMWF/ECMWF_teleport/","page":"Teleport","title":"Teleport","text":"[ewhelan@reaserve ~]$ cat .ssh/config \nHost hpc-login\n  User dui\n  IdentityFile ~/.tsh/keys/jump.ecmwf.int/eoin.whelan@met.ie\n  ProxyCommand bash -c \"tsh login; ssh -W %h:%p %r@jump.ecmwf.int\"\n[ewhelan@reaserve ~]$ ","category":"page"},{"location":"System/ECMWF/ECMWF_teleport/#Open-ecFlow-ports","page":"Teleport","title":"Open ecFlow ports","text":"","category":"section"},{"location":"System/ECMWF/ECMWF_teleport/","page":"Teleport","title":"Teleport","text":"The following opens ports to ECMWF (dui, ECF_PORT=3141) ecFlow server. Based on instructions provided by [https://confluence.ecmwf.int/display/ECFLOW/Teleport+-+using+local+ecflow_ui]. In a new terminal:","category":"page"},{"location":"System/ECMWF/ECMWF_teleport/","page":"Teleport","title":"Teleport","text":"ssh hpc-login -C -N -L 3141:ecflow-gen-dui-001:3141","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Preparation-of-initial-and-boundary-files","page":"Preparation","title":"Preparation of initial and boundary files","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/#Introduction","page":"Preparation","title":"Introduction","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"HARMONIE can be coupled with external models as IFS, ARPEGE, HIRLAM. Internally it is possible to nest the different ALADIN/ALARO/AROME with some restrictions. In the following we describe the host initial and boundary files are generated depending on different configurations. Boundary file preparation basically includes two parts: forecast file fetching and boundary file generation.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"The ECFLOW tasks for initial and boundary preparation","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Boundary-strategies","page":"Preparation","title":"Boundary strategies","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"There are a number of ways to chose which forecast lengths you use as boundaries. The strategy is determined by BDSTRATEGY in ecf/config_exp.h  and there are a number of strategies implemented.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"available            : Search for available files in BDDIR adn try to keep forecast consistency. This is ment to be used operationally since it will at least keep your run going, but with old boundaries, if no new boundaries are available.\nsimulate_operational : Mimic the behaviour of the operational runs using ECMWF 6h old boundaries.\nsame_forecast        : Use all boundaries from the same forecast, start from analysis\nanalysis_only        : Use only analyses as boundaries. Note that BDINT cannot be shorter than the frequency of the analyses.\nlatest               : Use the latest possible boundary with the shortest forecast length\nRCR_operational      : Mimic the behaviour of the RCR runs, ie\n12h old boundaries at 00 and 12 and\n06h old boundaries at 06 and 18\njb_ensemble          : Same as same_forecast but used for JB-statistics generation. With this you should export JB_ENS_MEMBER=some_number\neps_ec               : ECMWF EPS members (on reduced Gaussian grid). It is only meaningful with ENSMSEL non-empty, i.e., ENSSIZE > 0","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"All the strategies are defined in scr/Boundary_strategy.pl. The script generates a file bdstrategy in your working directory that could look like:","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":" Boundary strategy\n\n       DTG: 2011090618\n        LL: 36\n     BDINT: 3\n   BDCYCLE: 6\n  STRATEGY: simulate_operational\n     BDDIR: /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/ECMWF/archive/@YYYY@/@MM@/@DD@/@HH@\nHOST_MODEL: ifs\nINT_BDFILE: /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/20110906_18/ELSCFHARMALBC@NNN@\n\n# The output bdstrategy file has the format of \n# NNN|YYYYMMDDHH INT_BDFILE BDFILE BDFILE_REQUEST_METHOD \n# where \n# NNN        is the input hour\n# YYYYMMDDHH is the valid hour for this boundary\n# INT_BDFILE is the final boundary file\n# BDFILE                is the input boundary file\n# BDFILE_REQUEST_METHOD is the method to the request BDFILE from e.g. MARS, ECFS or via scp\n\nSURFEX_INI| /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/20110906_18/SURFXINI.lfi \n000|2011090618 /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/20110906_18/ELSCFHARMALBC000 /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/ECMWF/archive/2011/09/06/12/fc20110906_12+006 MARS_umbrella -d 20110906 -h 12 -l 6 -t\n003|2011090621 /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/20110906_18/ELSCFHARMALBC001 /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/ECMWF/archive/2011/09/06/12/fc20110906_12+009 MARS_umbrella -d 20110906 -h 12 -l 9 -t\n...","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"Meaning that the if the boundary file is not found under BDDIR the command MARS_umbrella -d YYYYMMDD -h HH -l LLL -t BDDIR will be executed. A local interpretation could be to search for external data if your file is not on BDDIR. Like the example from SMHI:","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":" Boundary strategy\n\n       DTG: 2011090112\n        LL: 24\n     BDINT: 3\n   BDCYCLE: 06\n  STRATEGY: latest\n     BDDIR: /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/g05a/archive/@YYYY@/@MM@/@DD@/@HH@\nHOST_MODEL: hir\nINT_BDFILE: /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/20110901_12/ELSCFHARMALBC@NNN@\n EXT_BDDIR: smhi_file:/data/arkiv/field/f_archive/hirlam/G05_60lev/@YYYY@@MM@/G05_@YYYY@@MM@@DD@@HH@00+@LLL@H00M\nEXT_ACCESS: scp\n\n# The output bdstrategy file has the format of \n# NNN|YYYYMMDDHH INT_BDFILE BDFILE BDFILE_REQUEST_METHOD \n# where \n# NNN        is the input hour\n# YYYYMMDDHH is the valid hour for this boundary\n# INT_BDFILE is the final boundary file\n# BDFILE                is the input boundary file\n# BDFILE_REQUEST_METHOD is the method to the request BDFILE from e.g. MARS, ECFS or via scp\n\n# hh_offset is 0 ; DTG is  \nSURFEX_INI| /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/20110901_12/SURFXINI.lfi \n000|2011090112 /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/20110901_12/ELSCFHARMALBC000 /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/g05a/archive/2011/09/01/12/fc20110901_12+000 scp smhi:/data/arkiv/field/f_archive/hirlam/G05_60lev/201109/G05_201109011200+000H00M \n003|2011090115 /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/20110901_12/ELSCFHARMALBC001 /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/g05a/archive/2011/09/01/12/fc20110901_12+003 scp smhi:/data/arkiv/field/f_archive/hirlam/G05_60lev/201109/G05_201109011200+003H00M ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"In this example an scp from smhi will be executed if the expected file is not in BDDIR. There are a few environment variables that one can play with in sms/confi_exp.h that deals with the initial and boundary files","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"HOST_MODEL : Tells the origin of your boundary data      * ifs : ecmwf data      * hir : hirlam data      * ald : Output from aladin physics, this also covers arpege data after fullpos processing.      * ala : Output from alaro physics      * aro : Output from arome physics\nBDINT : Interval of boundaries in hours\nBDLIB : Name of the forcing experiment. Set\nECMWF to use MARS data\nRCRa  to use RCRa data from ECFS\nOther HARMONIE/HIRLAM experiment\nBDDIR : The path to the boundary file. In the default location BDDIR=$HM_DATA/${BDLIB}/archive/@YYYY@/@MM@/@DD@/@HH@ the file retrieved from e.g. MARS will be stored in a separate directory. On could also consider to configure this so that all the retrieved files are located in your working directory $WRK. Locally this points to the directory where you have all your common boundary HIRLAM or ECMWF files.\nINT_BDFILE : is the full path of the interpolated boundary files. The default setting is to let the boundary file be removed by directing it to $WRK.\nINT_SINI_FILE : The full path of the initial surfex file. ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"There are a few optional environment variables that could be used that are not visible in config_exp.h ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"EXT_BDDIR : External location of boundary data. If not set rules are depending on HOST_MODEL\nEXT_ACCESS : Method for accessing external data. If not set rules are depending on HOST_MODEL\nBDCYCLE : Assimilation cycle interval of forcing data, default is 6h.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"More about this can be bounds in the Boundary_strategy.pl script.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"The bdstrategy file is parsed by the script ExtractBD. ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"scr/ExtractBD Checks if data are on BDDIR otherwise copy from EXT_BDDIR.  The operation performed can be different depending on HOST and HOST_MODEL. IFS data at ECMWF are extracted from MARS, RCR data are copied from ECFS.\nInput parameters: Forecast hour\nExecutables: none.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"In case data should be retrieved from MARS there is also a stage step. When calling MARS with the stage command we ask MARS to make sure data are on disk. In HARMONIE we ask for all data for one day of r forecasts ( normally four cycles ) at the time. ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Near-real-time-aerosols","page":"Preparation","title":"Near real time aerosols","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"The use of near real time aerosols require the presence of aerosol fields in the boundary files.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"BDAERO : Origin of the aerosol fields\nnone : no aerosols (default configuration)\ncams : aerosol from CAMS.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"A bdstrategycams file is generated. After the data is retrieved, the files are merge with the files from the HOSTMODEL to get the final boundary conditions files.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Initial-and-Boundary-file-generation","page":"Preparation","title":"Initial and Boundary file generation","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"To be able to start the model we need the variables defining the model state.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"T,U,V,PS in spectral space\nQ in gridpoint or spectral space","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"Optional:","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"Q,,l,,, Q,,i,,, Q,,r,,, Q,,g,,,  Q,,s,,, Q,,h,,\nTKE","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"For the surface we need the different state variables for the different tiles. The scheme selected determines the variables.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"Boundary files (coupling files) for HARMONIE are prepared in two different ways depending on the  nesting procedure defined by HOST_MODEL.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Using-gl","page":"Preparation","title":"Using gl","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"If you use data from HIRLAM or ECMWF gl_grib_api will be called to generate boundaries. The generation can be summarized in the following steps:","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"Setup geometry and what kind of fields to read depending on HOST_MODEL\nRead the necessary climate data from a climate file\nTranslate and interpolate the surface variables horizontally if the file is to be used as an initial file. All interpolation respects land sea mask properties. The soil water is not interpolated directly but interpolated using the Soil Wetness Index to preserve the properties of the soil between different models. The treatment of the surface fields is only done for the initial file.\nHorizontal interpolation of upper air fields as well as restaggering of winds.\nVertical interpolation using the same method (etaeta) as in HIRLAM\nConserve boundary layer structure\nConserve integrated quantities\nOutput to an FA file ( partly in spectral space )","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"gl_grib_api is called by the script scr/gl_bd where we make different choices depending on PHYSICS and HOST_MODEL","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"When starting a forecast there are options to whether e.g. cloud properties and TKE should be read from the initial/boundary file through NREQIN and NCOUPLING. At the moment these fields are read from the initial file but not coupled to. gl reads them if they are available in the input files and sets them to zero otherwise. For a Non-Hydrostatic run the non-hydrostatic pressure departure and the vertical divergence are demanded as an initial field. The pressure departure is by definition zero if you start from a non-hydrostatic mode and since the error done when disregarding the vertical divergence is small it is also set to zero in gl. There are also a choice in the forecast model to run with Q in gridpoint or in spectral space.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"It's possible to use an input file without e.g. the uppermost levels. By setting LDEMAND_ALL_LEVELS=.FALSE. the missing levels will be ignored. This is used at some institutes to reduce the amount of data transferred for the operational runs. ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Using-fullpos","page":"Preparation","title":"Using fullpos","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"If you use data generated by HARMONIE you will use fullpos to generate boundaries and initial conditions. Here we will describe how it's implemented in HARMONIE but there are also good documentation on the gmapdoc site.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"fullpos","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"In HARMONIE it is done by the script scr/E927. It contains the following steps:","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"Fetcht climate files. Fullpos needs a climate file and the geometry definition for both the input and output domains. \nSet different moist variables in the namelists depending if your run AROME or ALADIN/ALARO.\nCheck if input data has Q in gridpoint or spectral space.\nDemand NH variables if we run NH.\nDetermine the number of levels in the input file and extract the correct levels from the definition in scr/Vertical_level.pl\nRun fullpos","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"E927 is also called from 4DVAR when the resolution is changed between the inner and outer loops.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Generation-of-initial-data-for-SURFEX","page":"Preparation","title":"Generation of initial data for SURFEX","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"For SURFEX we have to fill the different tiles with correct information from the input data. This is called the PREP step in the SURFEX context. scr/Prep_ini_surfex creates an initial SURFEX file from an FA file if you run with SURFACE=surfex. ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/","page":"Preparation","title":"Preparation","text":"Read more about SURFEX","category":"page"},{"location":"ClimateGeneration/DownloadInputData/#Download-input-data","page":"Input Data","title":"Download input data","text":"","category":"section"},{"location":"ClimateGeneration/DownloadInputData/","page":"Input Data","title":"Input Data","text":"Before you can start running HARMONIE experiments some input data (external from the code repository) needs to be available on your platform. The input data contains physiography data, topography information and climatological values determined from a one year ARPEGE assimilation experiment with a resolution of T79.","category":"page"},{"location":"ClimateGeneration/DownloadInputData/","page":"Input Data","title":"Input Data","text":" E923_DATA-harmonie-43h2.1.tar.gz: Climate and physiography data for atmospheric climate generation (E923)\n PGD-harmonie-43h2.1.tar.gz: Physiography data for SURFEX (PGD)\nGMTED2010-harmonie-43h2.1.tar.gz : Digital elevation model from UGS\n SOILGRID-harmonie-43h2.1.tar.gz: Soil type data from SOILGRIDS\n sat-harmonie-43h2.1.tar.gz: Constants for satellite information\nrttov7L54-harmonie-43h2.1.tar.gz  : RTTOV constants\nECOCLIMAP second generation is available from here. It's also available on hpc-login:/ec/res4/hpcperm/hlam/data/climate/ECOCLIMAP2G\ntestbed-harmonie-43h2.1.tar.gz: Test data set with boundaries and observations for a small 50x50 domain]","category":"page"},{"location":"EPS/SPP/#SPP-in-HarmonEPS","page":"SPP","title":"SPP in HarmonEPS","text":"","category":"section"},{"location":"EPS/SPP/#SPP-options-in-HARMONIE","page":"SPP","title":"SPP options in HARMONIE","text":"","category":"section"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"The Stochastically Perturbed Parameterizations scheme (SPP) introduces stochastic perturbations to values of chosen closure parameters representing efficiencies or rates of change in parameterized atmospheric (sub)processes. See here for more information. SPP is available since cy40h1.1.1.","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"SPP is activated by setting SPP=yes in ecf/config_exp.h","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"SPP uses the Stochastic Pattern Generator (SPG). The pattern characteristics are set by the following settings in config_exp.h:","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":" SDEV_SPP = 1.0           # Standard deviation of the pattern\n TAU_SPP = 43200          # Time scale (seconds)\n XLCOR_SPP = 200000       # Length scale (m)\n SPGQ_SPP = 0.5           # Controls small vs. large scales \n SPGADTMIN_SPP=0.15       # initialization to ensure stationary statistics from the start of the integration\n SPGADTMAX_SPP=3.0        # initialization to ensure stationary statistics from the start of the integration\n NPATFR_SPP=1             # Frequency to evolve pattern: >0 in timesteps, <0 in hours","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"The parameters that can be perturbed are: ","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"Perturbation Description Perturbes\nP1: LPERT_PSIGQSAT Perturb saturation limit sensitivity VSIGQSAT\nP3: LPERT_CLDDPTHDP Perturb threshold cloud thickness used in shallow/deep convection decision RFRMIN(20)\nP4: LPERT_ICE_CLD_WGT Perturb cloud ice content impact on cloud thickness RFRMIN(21)\nP5: LPERT_ICENU Perturb ice nuclei RFRMIN(9)\nP6: LPERT_KGN_ACON Perturb Kogan autoconversion speed RFRMIN(10)\nP7: LPERT_KGN_SBGR Perturb Kogan subgrid scale (cloud fraction) sensitivity RFRMIN(11)\nP8: LPERT_RADGR Perturb graupel impact on radiation RADGR\nP9: LPERT_RADSN Perturb snow impact on radiation RADSN\nP10:LPERT_RFAC_TWOC Perturb top entrainment RFAC_TWO_COEF\nP11:LPERT_RZC_H Perturb stable conditions length scale RZC_H\nP12:LPERT_RZL_INF Asymptotic free atmospheric length scale RZL_INF\nP13:LPERT_RLWINHF Long wave inhomogeneity factor RLWINHF\nP14:LPERT_RSWINHF Short wave inhomogeneity factor RSWINHF\nP15:LPERT_ALPHA Cloud droplet gamma distribution parameters alpha (over sea) ALPHA\nP16:LPERT_RZNUC Cloud droplet gamma distribution parameters nu (over land) RZNUC\nP17:LPERT_RZMFDRY Parameter for dry mass flux RZMFDRY\nP18:LPERT_RZMBCLOSURE Closure parameter for moist mass flux RZMBCLOSURE","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"The following describes the namelist flags for SPP. Namelist flags for SPP are found in the namelist NAMSPP in nam/harmonie_namelists.pm","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"Activate perturbation of a parameter by setting LPERT_[PARAMETER] to TRUE in harmonie_namelists.pm, e.g.:","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":" NAMSPP=>{\n 'LPERT_PSIGQSAT' => '.TRUE.,',\n ...\n },","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"The size of the perturbation (the standard deviation of the parameter distribution) is set by CMPERT_[PARAMETER], e.g.:","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":" NAMSPP=>{\n 'CMPERT_PSIGQSAT' => '0.3,',\n ...\n },","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"The min/max range of each perturbed parameter can be controlled by the CLIP_[PARAMETER] namelist variable where the limits are specified as e.g.:","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"NAMSPP=>{\n'CLIP_PSIGQSAT' => '0.0,0.1',\n...\n },","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"Two distributions for the parameter values are possible: lognormal and pseudo uniform. Note: when a pseudo uniform distribution is used, the distribution may extend to negative values, which should be avoided. This can be assured by setting a clipping range (see above). Set LUNIFORM_[PARAMETER] to FALSE to use lognormal and to TRUE to use pseudo uniform, e.g.:","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"NAMSPP =>{\n'LUNIFORM_PSIGQSAT' => '.FALSE.,',\n...\n },","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"When pseudo uniform is chosen, it is possible to shift the distribution by setting UNIFORM_OFFSET_[PARAMETER], where offset 0.5 is default, <0.5 moves the distribution to the right and  >0.5 moves the distribution to the left e.g.:","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"NAMSPP =>{\n'UNIFORM_OFFSET_PSIGQSAT' => '0.45,',\n...\n },","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"When lognormal distribution is chosen, the flag LLNN_MEAN1_[PARAMETER] decides if the mean or the median of the distribution corresponds to the unperturbed, deterministic value of the parameter. Set to FALSE to use the median and to TRUE to use the mean, e.g.:","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"NAMSPP =>{\n'LLNN_MEAN1_PSIGQSAT' => '.TRUE.,',\n...\n },","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"For two of the parameters there is a possibility to correlate them, meaning that they use the same pattern for the perturbations. The two parameters are RZC_H and RZL_INF, and the correlations is activated by setting","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"NAMSPP =>{\nLCORR_RZL_INF' => '.TRUE.,',\n...\n },","category":"page"},{"location":"EPS/SPP/#Recommended-SPP-settings-(cy43):","page":"SPP","title":"Recommended SPP settings (cy43):","text":"","category":"section"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"CMPERT needs to be tuned for each parameter. CMPERT1 in the table below is the value that gives the range of values for the parameters recommended by the physics experts (when a lognormal distribution is used, if not stated otherwise). CMPERT is the value recommended for use. Tuning is ongoing, hence not all recommendations are in place yet. The well tested settings are in bold, preliminary suggestions are in italic.","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"Perturbation Det. value Recom. range CMPERT (CMPERT1) Distribution Offset Mean Median Clipping Corr.\nPSIGQSAT 0.02 0-0.06 0.6 (0.3) Log-normal N.A. Mean No N.A.\nCLDDPTHDP 4000 0-10000 0.6 (0.3) Log-normal N.A. Mean No N.A.\nICE_CLD_WGT 1 0.2-2 1.2 (0.3) Uniform 0.5 N.A. 0.01 - 10 N.A.\nICENU 1 0.01-100 TBD (1.05) TBD TBD TBD TBD N.A.\nKGN_ACON 10 1-100 TBD (0.75) TBD TBD TBD TBD N.A.\nKGN_SBGR 0.5[1] 0.01-1 TBD (0.3) TBD TBD TBD 0., 1. N.A.\nRADGR 0.5 0-1 TBD (0.4) TBD TBD TBD 0., 2. N.A.\nRADSN 1 0-2 TBD (0.35) TBD TBD TBD 0., 2. N.A.\nRFAC_TWOC 2 0.5-3 TBD (0.3) TBD TBD TBD TBD N.A.\nRZC_H 0.11 0.1-0.2 1.05 (0.3) Uniform 0.475 N.A. 0.001, 100 Yes\nRZL_INF 40 20-200 0.45 (0.45) Log-normal N.A. Mean No Yes\nLPERT_RLWINHF 1? 0.95-1 Not tested     \nLPERT_RSWINHF 1? 0.95-1 Not tested     \nLPERT_ALPHA 3 0.2-5 1.4 (0.3) Uniform 0.5 N.A. TBD N.A.\nLPERT_RZNUC 3 0.2-10 0.6 (0.3) Log-normal N.A. Mean No N.A.\nLPERT_RZMFDRY 1 0.2-2 0.8 (0.3) Log-normal N.A. Mean No N.A.\nLPERT_RZMBCLOSURE 0.35 0.05-0.7 0.8 (0.3) Log-normal N.A. Mean No N.A.","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"[1]: Originally, but set to 0.5 for SPP","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"For more SPP details see src/arpifs/module/spp_mod.F90","category":"page"},{"location":"EPS/SPP/#Pattern-diagnostics","page":"SPP","title":"Pattern diagnostics","text":"","category":"section"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"The raw and scaled patterns are stored in the vertical column of SNNNSPP_PATTERN using the index given in the SPP initialization. Thus","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"...\nKGET_SEED_SPP: PSIGQSAT                10000   1841082593\n   pattern   1 for PSIGQSAT         using seed   1841082593\nKGET_SEED_SPP: CLDDPTH                 10002    570790063\n   pattern   2 for CLDDPTH          using seed    570790063\nKGET_SEED_SPP: CLDDPTHDP               10004    980493159\n   pattern   3 for CLDDPTHDP        using seed    980493159\nKGET_SEED_SPP: ICE_CLD_WGT             10008   1362729695\n   pattern   4 for ICE_CLD_WGT      using seed   1362729695\n...","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"would give us","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"Perturbation raw pattern scaled pattern\nPSIGQSAT S001SPP_PATTERN S002SPP_PATTERN\nCLDDPTH S003SPP_PATTERN S004SPP_PATTERN\nCLDDPTHDP S005SPP_PATTERN S006SPP_PATTERN\nICE_CLD_WGT S007SPP_PATTERN S008SPP_PATTERN","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"note: Note\nThis is activated by setting TEND_DIAG=yes in ecf/config_exp.h.","category":"page"},{"location":"EPS/SPP/#Tendency-diagnostics","page":"SPP","title":"Tendency diagnostics","text":"","category":"section"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"Tendency diagnostics is available through the TEND_DIAG switch in ecf/config_exp.h. Activation gives six new 3D-fields","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"FANAME Description\nSNNNPTENDU U-component tendencies\nSNNNPTENDV V-component tendencies\nSNNNPTENDT Temperature tendencies\nSNNNPTENDR Moisture tendencies\nSNNNMULNOISE SPPT pattern, same for all levels\nSNNNSPP_PATTERN SPP pattern, distribution as explained above","category":"page"},{"location":"EPS/SPP/#Cy40h111-settings-(NB-only-log-normal-distribution-possible)","page":"SPP","title":"Cy40h111 settings (NB only log-normal distribution possible)","text":"","category":"section"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"In config_exp.h:","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"SDEV_SPP = 3.0      # Standard deviation of the pattern","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"Perturbation Det. value Recommended range CMPERT (CMPERT1) Clipping range Mean or median\nLPERT_PSIGQSAT 0.02, but set to 0.03 0-0.06 0.4 (0.1) No Mean\nLPERT_CLDDPTHDP 4000 1000-8000 0.4 (0.1) No Mean\nLPERT_ICE_CLD_WGT 1 0-2 0.4 (0.1) No Mean\nLPERT_ICENU 1 0.1-10 0.7 (0.35) No Median\nLPERT_KGN_ACON 10 2-50 0.5 (0.25) No Mean\nLPERT_KGN_SBGR 1, but set to 0.5 0.01-1 0.2 (0.1) 0.0 - 1.0 Mean\nLPERT_RADGR 0, but set to 0.5 0-1 0.3 (0.15) 0.0 - 2.0 Mean\nLPERT_RADSN 0, but set to 0.5 0-1 0.3 (0.15) 0.0 - 2.0 Mean\nLPERT_RFAC_TWOC 2 0.5-3 0.4 (0.1) No Mean\nLPERT_RZC_H 0.15 0.1-0.25 0.4 (0.1) No Mean\nLPERT_RZL_INF 100 30-300 0.6 (0.15) No Mean","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"In cy 40 the output of patterns and tendencies was as follows:","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"The raw and scaled patterns are stored in the vertical column of SNNNEZDIAG01 using the index given in the SPP initialization. Thus","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"...\nKGET_SEED_SPP: PSIGQSAT                10000   1841082593\n   pattern   1 for PSIGQSAT         using seed   1841082593\nKGET_SEED_SPP: CLDDPTH                 10002    570790063\n   pattern   2 for CLDDPTH          using seed    570790063\nKGET_SEED_SPP: CLDDPTHDP               10004    980493159\n   pattern   3 for CLDDPTHDP        using seed    980493159\nKGET_SEED_SPP: ICE_CLD_WGT             10008   1362729695\n   pattern   4 for ICE_CLD_WGT      using seed   1362729695\n...","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"would give us","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"Perturbation raw pattern scaled pattern\nPSIGQSAT S001EZDIAG01 S002EZDIAG01\nCLDDPTH S003EZDIAG01 S004EZDIAG01\nCLDDPTHDP S005EZDIAG01 S006EZDIAG01\nICE_CLD_WGT S007EZDIAG01 S008EZDIAG01","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"and so on","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"SPPT pattern EZDIAG02 (same in all levels)","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"SPP tendencies PtendU EZDIAG03","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"SPP tendencies PtendV EZDIAG04","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"SPP tendencies PtendT EZDIAG05","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"SPP tendencies PtendQ EZDIAG06","category":"page"},{"location":"EPS/SPP/#Suggestions-for-parameters-to-include-in-SPP:","page":"SPP","title":"Suggestions for parameters to include in SPP:","text":"","category":"section"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"Parameter Description Deterministic value cy43 Suggested range of values suggestion for parameter to correlate with Person responsible for implementing\n Terminal fall velocities of rain, snow and graupel    Sibbo\nRFRMIN(39) Depo_rate_graupel   RFRMIN 39 and 40 should approximately respect log10C = -3.55 x + 3.89, see eq. 6.2 on p. 108 in the meso-NH documentation: [https://hirlam.org/trac/attachment/wiki/HarmonieSystemDocumentation/EPS/SPP/sciICE3doc_p3.pdf Doc] Pirkka\nRFRMIN(40) Depo_rate_snow)   RFRMIN 39 and 40 should approximately respect log10C = -3.55 x + 3.89, see eq. 6.2 on p. 108 in the meso-NH documentation: [https://hirlam.org/trac/attachment/wiki/HarmonieSystemDocumentation/EPS/SPP/sciICE3doc_p3.pdf Doc] Pirkka\nRFRMIN(16) Distr_snow_c   to be correlated with RFRMIN(17) \nRFRMIN(17) Distr_snow_x   to be correlated with RFRMIN(16) ","category":"page"},{"location":"EPS/SPP/#Experiments","page":"SPP","title":"Experiments","text":"","category":"section"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"List with cy43h22 experiments is here: [wiki:HarmonieSystemDocumentation/EPS/ExplistSPPcy43 List of experiments]","category":"page"},{"location":"EPS/SPP/","page":"SPP","title":"SPP","text":"A guide for running the tuning experiments is here: [wiki:HarmonieSystemDocumentation/EPS/HowtoSPPcy43 Guide]","category":"page"},{"location":"EPS/Howto/#How-to-run-an-ensemble-experiment","page":"Howto","title":"How to run an ensemble experiment","text":"","category":"section"},{"location":"EPS/Howto/#Simple-configuration","page":"Howto","title":"Simple configuration","text":"","category":"section"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"Running an ensemble experiment is not very different from running a deterministic one. The basic instructions about setup are the same and will not be repeated here. ","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"What is different is that in ecf/config_exp.h one needs to pay attention to this particular section:","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"# *** Ensemble mode general settings. ***\n# *** For member specific settings use msms/harmonie.pm ***\nENSMSEL=                                # Ensemble member selection, comma separated list, and/or range(s):\n                                        # m1,m2,m3-m4,m5-m6:step    mb-me == mb-me:1 == mb,mb+1,mb+2,...,me\n                                        # 0=control. ENSMFIRST, ENSMLAST, ENSSIZE derived automatically from ENSMSEL.\nENSINIPERT=                             # Ensemble perturbation method (bnd). Not yet implemented: etkf, hmsv, slaf.\nENSCTL=                                 # Which member is my control member? Needed for ENSINIPERT=bnd. See harmonie.pm.\nENSBDMBR=                               # Which host member is used for my boundaries? Use harmonie.pm to set.\nENSMFAIL=                               # Failure tolerance for all members. Not yet implemented.\nENSMDAFAIL=                             # Failure tolerance for members doing own DA. Not yet implemented.","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"In addition one should also look at BDSTRATEGY, choose eps_ec if you want to use EC EPS at the boundaries (this option gets the EC EPS data from the GLAMEPS ECFS archive). If you want to use SLAF see here.","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"What really triggers EPS mode is having a non-empty ENSMSEL (ensemble member selection). The reason the specification looks a bit complicated is that our ensemble members do not necessarily have to be numbered consecutively from 0 or 1 and up, but can also be specified with steps. The rationale behind this is that we may want to e.g. downscale a subset of the 51 ECMWF EPS members, but not necessarily starting from their lowest number or taking them consecutively. ENSMSEL is a heritage from the Hirlam EPS system and has been retained in Harmonie.","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"In the simplest case of consecutive numbering, say we want a control run (member 0) and 20 perturbed members. We can then put","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"ENSMSEL=0-20","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"Now assume that we still have a control and 20 members, but that we want to take only every second pair of the host EPS members, i.e., take 0,1,2, skip 3,4, take 5,6, skip 7,8 and so on. The following specifications are then equivalent:","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"ENSMSEL=0,1,2,5,6,9,10,13,14,17,18,21,22,25,26,29,30,33,34,37,38\nENSMSEL=0,1-37:4,2-38:4","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"In the second version we use the step option, so our list is 0, 1 to 37 in steps of 4 and 2 to 38 in steps of 4. The system will take care of transforming this into an ascending list for easier handling within the script system, but we don't have to worry about that.","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"The ENSMSEL selection is still not totally flexible. It would not be possible to have more than one of our members having boundaries from the same member of the host model. This might be relevant in the case of multiple physics, and multiple control members. For this reason the variable ENSBDMBR has also been added (in [10953]). The usage of this variable is explained in the next section (advanced configuration).","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"For the rest of the ENS... variables, not everything planned is implemented by the time of this writing. The only valid choice (except empty) for ENSINIPERT (initial state perturbation method) is \"bnd\". This option means to take the perturbations of the first (interpolated) boundary file, and add these perturbations to a reference (control) analysis. This will involve the script scr/PertAna, a section of its header is quoted below:","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"#| Different perturbation methods are distinguished by\n#| ENSINIPERT. This script implements ENSINIPERT=bnd\n#|\n#| bnd: boundary data mode\n#|      an($ENSMBR) = an(cntrl) + bnd1($ENSMBR) - bnd1(cntrl)\n#|           where bnd1 denotes the first boundary file","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"Which member is the control member is specified by the variable ENSCTL.","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"But how to specify that one (or more) member(s) run assimilation and others do not, or in other words, how to specify member specific values to the variables in config_exp.h? This is the topic of the next section.","category":"page"},{"location":"EPS/Howto/#Advanced-configuration,-member-specific-settings","page":"Howto","title":"Advanced configuration, member specific settings","text":"","category":"section"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"It would perhaps have been possible to also have member specific configuration in config_exp.h, but since perl is more flexible with lists than the shell, and since perl is already used extensively in the Harmonie system, it was decided to extend the handling of the template definition files in mini-SMS in such a way that every tdf can now also have an associated perl module to help in its interpretation. And, since after the changesets [10930] and [10932] there is no separate tdf for HarmonEPS anymore (harmonie.tdf is used also for EPS runs), the file that is used for member specific settings is thus msms/harmonie.pm.","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"The idea of harmonie.pm is to be able to override some of the environment variables of config_exp.h with new values for selected members of the ensemble. This is achieved by populating the perl hash %env with key => value pairs. The keys are names of environment variables, like ANAATMO, ANASURF, PHYSICS etc. Only names that are present and exported in config_exp.h should be used as keys. Values can take four different forms:","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"A hash, i.e., a new set of key => value pairs. The syntax in this case is { m1 => val1, m2 => val2, ... }. The numbers m1, m2, etc. must be member numbers given in ENSMSEL. Order is irrelevant, and only members with values different from the default need be listed of course.\nAn array, where indices implicitly run from 0 and up. The syntax in this case is [ val0, val1, val2, ...]. Here the array should have as many values as members given in ENSMSEL, but if not, missing values will be recycled from the start of the array (as many times as necessary). Thus, using arrays will give values to all members, and order is important.\nA scalar (string). This string is subject to variable substitution, i.e., any occurrence of the substring @EEE@ will be replaced by the relevant 3-digit ensemble member number.\nA subroutine (reference), syntax is typically sub { my $mbr = shift; return \"something dependent on $mbr\"; }. The arguments given to the subroutine are the \"args\" of the invoking &Env('SOMEVAR',args) call (see below).","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"In addition to the hash %env, harmonie.pm also contains a subroutine Env. In msms/harmonie.tdf many earlier occurences of $ENV{SOMEVAR} have now been replaced by subroutine calls &Env('SOMEVAR','@EEE@'). The @EEE@ argument will be replaced by the relevant member number before invocation, and Env will check the hash %env for a member specific setting to possibly return instead of the default value $ENV{SOMEVAR}. There should normally be no need to make changes to the subroutine Env, putting entries into the hash %env ought to be enough.","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"Note also that not every occurrence of $ENV{...} has been replaced by a corresponding &Env(...) in harmonie.tdf, only those variables that are most likely to have variations among members are changed. If you need variations in e.g. $HOST_MODEL, then harmonie.tdf needs to be updated so that those variations are respected within the ensemble (EEE) loops.","category":"page"},{"location":"EPS/Howto/#An-example","page":"Howto","title":"An example","text":"","category":"section"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"We will now look at one particular example, in order to (hopefully) make the descriptions above a bit more clear. Our intent is to have an ensemble with a mix of members with AROME and ALARO physics, with one control member and 10 perturbed members for each. The control members will both do their own 3DVAR assimilation, while perturbed members will have ANAATMO=blending. But with ENSINIPERT=bnd, the control analysis will be used also by the perturbed members. All members will do surface assimilation, but the forecast interval differs. The control members have a forecast interval of 6 hours (because of the 3D-Var), while the perturbed members have FCINT=12. To achieve this, we have the following settings in config_exp.h:","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"ANAATMO=blending\nANASURF=CANARI_OI_MAIN\nFCINT=12\nBDSTRATEGY=eps_ec\nENSMSEL=0-21\nENSINIPERT=bnd\nENSCTL=\nENSBDMBR=","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"In harmonie.pm our %env looks as follows:","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"%env = (\n   'ANAATMO' => { 0 => '3DVAR', 1 => '3DVAR' },\n   'FCINT'   => { 0 => 6,       1 => 6 },\n   'PHYSICS' => [ 'arome','alaro','alaro','arome'],\n   'ENSCTL'  => [ '000',  '001',  '001',  '000'],\n   'ENSBDMBR' => [ 0, 0, 1..20],\n\n### Normally NO NEED to change the variables below\n   'ARCHIVE' => '${ARCHIVE}mbr@EEE@/',\n   'CLIMDIR' => '$CLIMDIR/mbr@EEE@',\n   'OBDIR' => '$OBDIR/mbr@EEE@',\n   'VFLDEXP' => '${EXP}mbr@EEE@',\n   'BDDIR' => sub { my $mbr = shift;\n                    if ($ENV{COMPCENTRE} eq 'ECMWF') {\n                       return '$BDDIR/mbr'.sprintf('%03d',$mbr);\n                    } else {\n                       return '$BDDIR/mbr'.sprintf('%03d',&Env('ENSBDMBR',$mbr));\n                    }\n                  },\n   'FirstHour' => sub { my $mbr = shift;\n                        return $ENV{StartHour} % &Env('FCINT',$mbr);\n                      }\n    );","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"ANAATMO is straightforward, only the control members need an exception from blending, so using a hash is most appropriate. Similarly for FCINT. For PHYSICS we have used an array and the fact that the array will be recycled. Thus member 0 will be the AROME control, while member 1 will be the ALARO control. The reason why we did not simply put a 2-element array [ 'arome','alaro'] to be repeated is that since the ECMWF perturbations come in +/- pairs, we don't want all the '+' perturbations to be always with the same physics (and the '-' perturbations with the other type). Therefore, we added a second pair with the order reversed, to alternate +/- perturbations between AROME and ALARO members. ENSCTL follows the same pattern as PHYSICS. Note the need for 3-digit numbers in ENSCTL, at present this is necessary to avoid parsing errors in the preparation step of mini-SMS.","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"Note also how we have used ENSBDMBR. For both the AROME control (member 0) and ALARO control (member 1), we have used the EC EPS control member 0 to provide boundaries. The syntax 1..20 is a perl shorthand for the list 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20.","category":"page"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"Note added after changeset [12537]: The setting of ENSBDMBR created a race condition in the boundary extraction for runs at ECMWF. This is hopefully solved by the new definition for BDDIR, which makes use of the possibility of having a subroutine to compute the member specific settings. Another example where a subroutine came out handy was for the setting of FirstHour.","category":"page"},{"location":"EPS/Howto/#Further-reading","page":"Howto","title":"Further reading","text":"","category":"section"},{"location":"EPS/Howto/","page":"Howto","title":"Howto","text":"More specific instructions and information about known problems can be found here.","category":"page"},{"location":"Build/Build_with_cmake/#Build-with-CMake","page":"CMake","title":"Build with CMake","text":"","category":"section"},{"location":"Build/Build_with_cmake/#Background","page":"CMake","title":"Background","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"CMake is a build system generator supporting multiple build systems and programming languages, specifically Fortran is a first-class citizen there, allowing, for example, out-of-the-box handling of the inter-module dependencies. A build system generator there means that description of the build procedure written in the CMake-script language is used by the cmake tool to generate the actual build system, for example using Unix Makefiles or Ninja generator. Thus, all modifications should be performed on the CMake-script level and not within the generated build system as these changes will be overwritten when re-running cmake at some point.","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Why providing yet another alternative for building HARMONIE-AROME? Well, makeup does a very good job building the system, however it's an in-house solution which has a number of limitations:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"makeup is an in-house build system, so there are components that require more maintenance compared to a standardized build tool\nmakeup uses a considerable number of sequential steps, which increase the total build time\nthe configure step takes quite some time, although in some cases it can be skipped, but users have to remember when they must re-run configure and this dependency is not enforced by makeup\nnot all the dependencies are tracked by makeup, for example updating configure files does not trigger a re-build","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"In an attempt to fix these limitation of makeup, CMake was chosen as an alternative. CMake has a mature Fortran support and improves upon some shortcomings of makeup with little effort (well, it obviously has its own fair share of quirks, but that's a different story...). Additionally, using CMake allows us to enforce usage requirements and dependencies between different components of HARMONIE-AROME, for example, it's a good idea to ensure that SURFEX routines do not directly call cloud microphysics functions. Currently makeup does not enforce these boundaries and this task is left to the developers who implement the new code. Of course, something like this can also be implemented with makeup, but it would require considerable development efforts.","category":"page"},{"location":"Build/Build_with_cmake/#Getting-started-with-CMake","page":"CMake","title":"Getting started with CMake","text":"","category":"section"},{"location":"Build/Build_with_cmake/#Selecting-the-CMake-based-build-system-when-installing-HARMONIE-AROME","page":"CMake","title":"Selecting the CMake-based build system when installing HARMONIE-AROME","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"If all the config files are available, building HARMONIE-AROME with CMake should be as simple as setting the BUILD_WITH variable when invoking Harmonie:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"config-sh/Harmonie install BUILD_WITH=cmake","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"or alternatively, setting the desired option in ecf/config_exp.h.","category":"page"},{"location":"Build/Build_with_cmake/#Building-HARMONIE-AROME-with-CMake-from-the-command-line","page":"CMake","title":"Building HARMONIE-AROME with CMake from the command line","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Sometimes calling Harmonie install is not the best choice and one might want to compile the code from the command line. In this case compilation of HARMONIE-AROME with CMake consists of three individual steps:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"compiling the auxiliary libraries (gribex and such)\ncompiling the main code of HARMONIE-AROME\noptionally, compile some additional tools (for example, gl)","category":"page"},{"location":"Build/Build_with_cmake/#.-Compiling-the-auxiliary-libraries","page":"CMake","title":"1. Compiling the auxiliary libraries","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"This step is rather straightforward, assuming that HARMONIE-AROME code is located under the path stored in the HM_LIB environment variable one can adapt the following snippet to compile all the required libraries:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"CMAKE_FLAGS=\"-DCONFIG_FILE=<path to your JSON config>\"\nINSTALL_DIR=\"<directory where the auxiliary libraries should be installed>\"\n\nAUX_LIBS='bufr_405 gribex_370 rgb_001 dummies_006/mpidummy'\nfor project in $AUX_LIBS; do\n  echo \"Compiling $project\"\n  current_project_dir=$HM_LIB/util/auxlibs/$project\n  current_build_dir=\"build-`echo $project | sed 's|/|-|g'`\"\n\n  mkdir -p $current_build_dir && cd $current_build_dir\n\n  # CMake build type can be changed to Debug, if needed\n  cmake $current_project_dir -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR $CMAKE_FLAGS\n  # Here -j tells CMake how many parallel compilation processes to use\n  cmake --build . --target install -j16\n\n  cd ..\ndone","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"If a specific config file is not there, you can try your luck with using generic config files provided for different compiler types. To do so, just drop the -DCONFIG_FILE from the list of CMake command line arguments and CMake will try to load a suitable configuration file, if available.","category":"page"},{"location":"Build/Build_with_cmake/#.-Compiling-the-main-code-of-HARMONIE-AROME","page":"CMake","title":"2. Compiling the main code of HARMONIE-AROME","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Following the procedure described in the previous step, one can use a similar approach to compile the main code (here, one of the generic configuration files is used, of course it can be replaced with a different one or dropped but it should be the same config file which was used to compile auxiliary libraries):","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"mkdir build && cd build\n# Configure and generate the build system\ncmake $HM_LIB/src \\\n  -G Ninja # Use Ninja to build HARMONIE-AROME, drop to build with Makefiles\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DCONFIG_FILE=$HM_LIB/util/util/cmake/config.GNU.cmake \\\n  -Dbufr_DIR=$INSTALL_DIR/lib/cmake/bufr \\\n  -Dgribex_DIR=$INSTALL_DIR/lib/cmake/gribex \\\n  -Drgb_DIR=$INSTALL_DIR/lib/cmake/rgb \\\n  -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR\n\n# Build and install HARMONIE-AROME\ncmake --build . --target install -j16","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"note: Note\nObviously, when compiling from command line, additional command line arguments might be provided to CMake at the configure step as needed. However, a preferred solution is to use a configuration file to handle as much of the machine-specific details as possible.","category":"page"},{"location":"Build/Build_with_cmake/#.-Compiling-the-tools","page":"CMake","title":"3. Compiling the tools","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"The approach is the same as with the main code, however, you might want to add -Dharmonie_DIR=$INSTALL_DIR/lib/cmake/harmonie if the tool in question needs HARMONIE-AROME libraries for compilation.","category":"page"},{"location":"Build/Build_with_cmake/#Configuration-files","page":"CMake","title":"Configuration files","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Configuration files, similarly to makeup, are used to provide compilation flags, define external libraries to use when compiling the code et cetera. Thus, having a correct configuration file is one of the key elements of successful building HARMONIE-AROME. The CMake-based build system of HARMONIE-AROME uses configuration files written in JSON format. JSON was chosen to make these files more declarative and, hopefully, easier to maintain and modify than plain CMake-script-based files would be.","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"The main config file, which is used to build auxiliary libraries and the main HARMONIE-AROME code should be placed under util/cmake/config directory. This file has a following top-level structure:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"{\n  \"build_tools\":[],\n  \"dependencies\":[],\n  \"programs\":[],\n  \"configure\":{},\n  \"compile\":[],\n  \"compile_single\":[],\n  \"compile_double\":[],\n  \"custom_compile\":{},\n  \"link\":[]\n}","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"there all the sections except configure, custom_compile and link are mandatory. In the following a detailed description of all the config file section is provided.","category":"page"},{"location":"Build/Build_with_cmake/#The-build_tools-section","page":"CMake","title":"The build_tools section","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"This section lists the external tools required for compiling HARMONIE-AROME,  excluding compilers. Currently, this section should always contain the two following entries: FLEX and BISON, but in future this list might be extended. So, currently this section is always defined as:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"\"build_tools\":[\"BISON\", \"FLEX\"]","category":"page"},{"location":"Build/Build_with_cmake/#The-dependencies-section","page":"CMake","title":"The dependencies section","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"This section provides a list of external (external here means \"not found within the src directory of HARMONIE-AROME\", so, for example, gribex is also an external library for CMake build) libraries required to compile and link HARMONIE-AROME code. Since finding a correct library can be a tricky task, this section allows a number of options for specifying external dependencies:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"You can completely rely on CMake and delegate it all the work for finding a dependency. In this case, a dependency is added as a simple string to the dependencies section, for example:\n\"dependencies\":[\"OpenMP\", \"LAPACK\"]\nThis option is for packages like OpenMP which do not involve finding libraries located in unusual places as often happens when using environment modules.\nYou can still rely on CMake to find the package, but provide a bit of detail on how to find it. In this case a dependency is added as a JSON object of the following form (using the NetCDF library as an example):\n{\n  \"pkg\":\"NetCDF\",\n  \"use_cmake_config\":false,\n  \"components\":[\"C\",\"Fortran\"],\n  \"hints\":[\"$ENV{NETCDF_DIR}\",\"$ENV{NETCDF_F_DIR}\"],\n  \"cmake\":{\"NETCDF_USE_DEFAULT_PATHS\":true}\n}\nThere the use_cmake_config field tells CMake which mechanism it should use to find the library in question. When use_cmake_config is set to true CMake will look for CMake configuration files installed with the library, which is a recommended option in modern CMake. Even though it's a recommended by the CMake authors option, not all the libraries provide CMake configuration files so just setting use_cmake_config to true does not work all the time (at least it works for the auxiliary libraries compiled with CMake). You might want to provide a -D<package name>_DIR=<path to CMake config files> as an argument to the cmake command when configuring the build if CMake fails to find a package.\nAnother alternative is setting use_cmake_config to false, then CMake will try to find the required dependency using the hand-written scripts provided by the authors of CMake (or found under the util/cmake directory of HARMONIE-AROME). These scripts usually do quite some work trying to find a dependency and sometimes fail even if library is there, for example when it's located in a very unusual place or has an unexpected pkg-config name.\nWhen using \"use_cmake_config\":false one may add a components list, if only a language-specific version of the dependency is wanted. For example, having:\n{\"pkg\":\"NetCDF\", \"use_cmake_config\":false, \"components\":[\"C\"]}\nCMake would not try to find the Fortran version of NetCDF library, which can be useful sometimes. Use this option of defining external dependencies for such libraries as MPI, which can have multiple vendors and subtle differences between libraries provided (for example CMake should be able to figure out the correct MPI libraries for both MPICH and Open-MPI).\nThe hints list tells CMake which directories it should check when looking for a library.\nnote: Note\nElements of the hints list are simply added to the <PackageName>_ROOT CMake variable. If CMake's find_package(<PackageName>) does not use this variable providing hints would have no effect.\nFinally, the cmake section provides a key-value set of elements, which will be converted to corresponding CMake variables set before calling find_package(<PackageName>). Thus, it can be used to control the behaviour of find_package.\nnote: Note\nVariables set in the cmake section are local to the current package and do not modify the global scope.\nWhen nothing of the above works, you can provide all the flags manually. To do so, use the following form for a dependency entry:\n{\n  \"pkg\":\"HDF5\",\n  \"raw_lib\":{\n    \"include\":\"$ENV{HDF5_DIR}/include\",\n    \"lib_directory\":\"$ENV{HDF5_DIR}/lib\",\n    \"lib\":[\"-lhdf5hl_fortran\", \"-lhdf5_fortran\", \"-lhdf5_hl\", \"-lhdf5\"]\n  }\n}\nwhere the raw_lib component provides all the needed include and link directories as well as the link libraries. If some some fields of the raw_lib object are unneeded they can be set to null:\n{\"pkg\":\"rt\", \"raw_lib\":{\"include\":null, \"lib_directory\":null, \"lib\":\"-lrt\"}}\nNote that all the members of the raw_lib object can be defined as lists:\n{\n  \"pkg\":\"HDF5\",\n  \"raw_lib\":{\n    \"include\":[\"$ENV{HDF5_DIR}/include\",\"$ENV{HDF5_DIR}/include_fortran\"],\n    \"lib_directory\":[\"$ENV{HDF5_DIR}/lib\",\"$ENV{HDF5_DIR}/lib64\"],\n    \"lib\":[\"-lhdf5hl_fortran\", \"-lhdf5_fortran\", \"-lhdf5_hl\", \"-lhdf5\"]\n  }\n}\nWhen providing the required libraries in the lib section one can skip the -l prefix, thus having \"lib\":\"-lrt\" and \"lib\":\"rt\" would have the same effect.\nSometimes it can be useful to define a dummy library in CMake without actually looking for the library files, for example when compiling a tool which uses only a subset of HARMONIE-AROME libraries. When loading HARMONIE-AROME as a CMake package all the targets associated with external dependencies should be present, but some of these dependencies might be not needed for successful linking (or these are added implicitly by the programming environment and adding them for the second time in CMake won't make any difference). In this case you can use the following:\n{\"pkg\":\"gribex\",  \"dummy\":true}","category":"page"},{"location":"Build/Build_with_cmake/#The-programs-section","page":"CMake","title":"The programs section","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"This section provides a list of HARMONIE-AROME programs to build (excluding MASTERODB which is always built by the CMake build system), for example:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"\"programs\":[\"BATOR\", \"oulan\", \"ioassign\", \"LSMIX\"]","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"CMake will try to find the corresponding Fortran source files and will complain if unable to do so. Currently it is not possible to explicitly tell CMake via JSON config which program should be compiled from which source file. If CMake is unable to figure out how to compile a program the CMake-code should be altered to tell it how to do so.","category":"page"},{"location":"Build/Build_with_cmake/#The-configure-section","page":"CMake","title":"The configure section","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"This section provides various configure-time flags controlling the build system or selecting features. Currently in the main HARMONIE-AROME config file this section is defined as follows:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"\"configure\":{\n    \"use_flexfix\":true\n  , \"precision\":\"double\"\n},","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"There the use_flexfix option controls the usage of the flexfix wrapper, set it to true to use the flexfix wrapper when generating lexers for the Blacklist and ODB compilers. Having use_flexfix as false results in using the flex tool directly. The precision option controls the floating point precision of the build, with possible values of double and single. This is a mandatory option, removing it would result in a CMake fatal error at the configure time.","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"The configure file for gl has the following options in the configure section:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"\"configure\":{\n    \"use_aladin\":true\n  , \"use_netcdf\":true\n  , \"check_preferlocalconcepts_bug\":true\n}","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Set use_aladin to true to compile with FA support (requires HARMONIE-AROME libraries). Set use_netcdf to true to enable NetCDF support in gl. Set check_preferlocalconcepts_bug to true to perform a configure-time auto-detection test checking whether the supplied eccodes version is affected by the preferLocalConcepts bug. This test can be skipped, although in such a case corresponding CPP definitions should be manually added to the config file if a 'bad' eccodes version is used.","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"note: Note\nIf an option is removed from the configure section it will be treated by CMake as set to false in case of boolean flags or empty string for string options.","category":"page"},{"location":"Build/Build_with_cmake/#Adding-a-new-configure-option","page":"CMake","title":"Adding a new configure option","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"There's no predefined list of configure section members of CMake JSON config, any element found in this section will be available as CONFIG_<option name> from CMake code. For example having:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"\"configure\":{\n    \"use_flexfix\":true\n  , \"precision\":\"double\"\n  , \"new_flag\":true\n},","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"would define the following variables available in CMake scripts (after call to the hm_get_json_configure_options function): CONFIG_USE_FLEXFIX, CONFIG_PRECISION and CONFIG_NEW_FLAG. How these newly introduced options are used when configuring the build is up to the developer.","category":"page"},{"location":"Build/Build_with_cmake/#The-compile-section","page":"CMake","title":"The compile section","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"This section defines compiler flags and preprocessor definitions which should be used when compiling various components of HARMONIE-AROME. Generally this section should consists of a list of objects with the following structure where the first of these objects should always have the project name set to null:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"{\n    \"project\":null\n  , \"flags_fortran\":{\"any\":[], \"debug\":[], \"release\":[]}\n  , \"flags_c\":[]\n  , \"defs_fortran\":[]\n  , \"defs_c\":[]\n  , \"exclusive_defs\":true\n  , \"exclusive_flags\":true\n},","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Individual members of these objects are defined as follows:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"project is the name of the CMake target for which the provided setting should be applied. CMake matches this field against the names of its targets, which means that having, for example, \"project\":\"surf\" would apply the compilation flags for all CMake targets named surf or with names starting with surf-, as in surf-module. When CMake finds a suitable project-specific section it will append project-specific compilation flags to the global compilation flags for the project in question (if not explicitly asked to not use global flags). If there's no suitable section with project-specific options, only the global options will be used.\nnote: Note\nAll compilation flags and preprocessor definitions should be set via JSON config and not through the native CMake variables.\nflags_fortran define compiler flags to be used when compiling Fortran source files. Flags can be defined in three different ways: as a string (e.g., \"flags_fortran\":\"-std=f2003\"), as a list (e.g., \"flags_fortran\":[\"-std=f2003\", \"-Wall\"]) or as an object. Providing the compiler flags as a JSON object allows for a fine-grained control applying different flags for different build configurations (the first two forms will apply the same flags for any build configuration). This object contains three subsections: any, debug and release. Compiler flags put under the any section will be used for any build type, while flags under debug and release will be used only when CMAKE_BUILD_TYPE is set to Debug or Release, respectively. Thus if having, for example, \"flags_fortran\":{\"any\":[\"-std=f2003\"], \"debug\":[\"-O0\"], \"release\":[\"-O2\"]} Fortran sources will be compiled using -std=f2003 -O0 when CMAKE_BUILD_TYPE=Debug and -std=f2003 -O2 when CMAKE_BUILD_TYPE=Release.\nnote: Note\nTechnically, the compiler flags there are not limited to the debug and release configurations. CMake allows defining custom CMAKE_BUILD_TYPE and it will look for a correspondingly named element under the flags_fortran object. However, this feature is not used in the current version of CMake-based build system for HARMONIE-AROME\nnote: Note\nCompiler flags there can be provided as a list (e.g., [\"-Wall\", \"-Wextra\"]) or as a plain string when there is only a single flag (e.g., \"flags_fortran\":\"-O2\") but never as a space-separated string with multiple compiler flags. Trying to do so will result in an error.\nflags_c same as flags_fortran but for compiling C code.\ndefs_fortran defined in the same way as flags_fortran (allowing the same three forms: a string, a list or an object) but contains preprocessor definitions.\nnote: Note\nPreprocessor definitions should be added without the -D prefix, for example: \"defs_fortran\":{\"debug\":\"PRINT_MORE\"} and not as defs_fortran\":{\"debug\":\"-DPRINT_MORE\"}.\ndefs_c same as defs_fortran but for the C code.\nexclusive_defs and exclusive_flags flags. In some situations it can be useful to ignore the global compilation flags defined by \"project\":null and use only project-specific flags or preprocessor definitions. To do so, add exclusive_defs and/or exclusive_flags to the project-specific element of the compile section, for example having:\n  {\n      \"project\":null\n    , \"flags_fortran\":\"-Wall\"\n  },\n  {\n      \"project\":\"gribex\"\n    , \"exclusive_defs\":true\n    , \"exclusive_flags\":true\n    , \"flags_fortran\":\"-O0\"\n  },\n  {\n      \"project\":\"surfex\"\n    , \"flags_fortran\":\"-O0\"\n  }\nwill result in having gribex to be always compiled with just -O0 without using any other flags or preprocessor definitions, but surfex will be compiled with -Wall -O0.","category":"page"},{"location":"Build/Build_with_cmake/#The-compile_single-and-compile_double-sections","page":"CMake","title":"The compile_single and compile_double sections","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"These sections repeat the structure of the compile section. Compiler flags defined by these sections are appended to the flags defined in the compile section of the config based on the value of the value of the precision flag from the configure section.","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"note: Note\nDo not add auto-double flags (e.g., GNU Fortran's -fdefault-real-8) there, they are handled differently.","category":"page"},{"location":"Build/Build_with_cmake/#The-custom_compile-section","page":"CMake","title":"The custom_compile section","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Sometimes it might be desirable to add some specific compile flags for a single source file. This can be achieved by using the custom_compile section of JSON config as follows:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"\"custom_compile\":{\n  \"sufpf.F90\":[\"-O0\", \"-Wextra\"]\n},","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"note: Note\nUnlike makeup, compilation flags found in the custom_compile are always appended to the list of compiler flags for the specified source file and there's no option to replace them.","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"note: Note\nWhen using the Unix Makefiles generator, per-source compile flags are set on the target level. Thus, for example, adding (or updating) a custom compile flag for a  source file within arpifs will result in recompilation of the whole arpifs project. The Ninja generator does not have such limitation, and only the source file in question will be recompiled (possibly triggering a recompilation cascade if other Fortran sources depend on it).","category":"page"},{"location":"Build/Build_with_cmake/#The-link-section","page":"CMake","title":"The link section","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"This section provides a list of linker flags to be used when linking HARMONIE-AROME executables, for example:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"\"link\":[\"LINKER:-export-dynamic,--as-needed\", \"-rdynamic\"]","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"note: Note\nAll flags that should be passed directly to ld can be grouped in strings of the following form \"LINKER:-option1,-option2\" and CMake will figure out how to pass them to the linker.","category":"page"},{"location":"Build/Build_with_cmake/#Using-environment-variables-in-JSON-configuration-files","page":"CMake","title":"Using environment variables in JSON configuration files","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Current version of CMake JSON config implemented in HARMONIE-AROME allows using environment variables as $ENV{VARIABLE} in the following contexts:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"string-valued members of the configure section, e.g., \"configure\":{\"precision\":\"$ENV{VARIABLE}\"}\nany elements of JSON config which allow a list of strings as one of the possible values, for example:\n\"link\":[\"LINKER:-export-dynamic,--as-needed\", \"$ENV{VARIABLE}\"] works since the link section expects a list of strings\n\"compile_double\":[{\"project\":null, \"defs_fortran\":\"$ENV{VARIABLE}\"}] also works because defs_fortran allows a list of strings as an option\n{\"pkg\":\"$ENV{VARIABLE}\",\"use_cmake_config\":true} does not work because pkg is expected to provide a single string","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"note: Note\nCurrently CMake recognises only the first $ENV{VAR} when reading a JSON string. Thus, having \"include\":\"$ENV{HDF5_DIR}/include\" works as expected, but \"include\":\"$ENV{HDF5_DIR}/$ENV{PRG_ENV}/include\" would keep the second $ENV as it is. This is not CMake's limitation, but rather a technical detail of the current JSON config for HARMONIE-AROME, which can be changed in the future, if desired.","category":"page"},{"location":"Build/Build_with_cmake/#Note-on-the-structure-of-the-JSON-configuration-files","page":"CMake","title":"Note on the structure of the JSON configuration files","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"The structure of JSON-based configuration files is not automagically understood by CMake, there is some boilerplate code to be added to CMake scripts. Thus, the described structure of the CMake config file fully applies only to the main build. Other components of the system try to use the same structure of the config file, but some sections might be not handled correctly yet (for example, if a project does not use configure-time flags adding a configure section without modifying the CMake code would not have any effect and corresponding CMake variable won't be added).","category":"page"},{"location":"Build/Build_with_cmake/#Note-on-the-auto-double-flags","page":"CMake","title":"Note on the auto-double flags","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"The compiler flags defining the preferred precision of the floating point variables in CMake build for HARMONIE-AROME are provided on the per-compiler and not per-config level. Thus, for each compiler type recognized by CMake a file named as FortranCompilerFlags.<compiler type>.cmake should be added to the util/cmake directory. This file should define the following CMake variables Fortran_DEFAULT_FLOAT_32, Fortran_DEFAULT_FLOAT_64, Fortran_DEFAULT_INT_32, Fortran_DEFAULT_INT_64 (some of them may be empty if a compiler does not provide corresponding flags). For example for the GNU compilers FortranCompilerFlags.GNU.cmake is defined as:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"set(Fortran_DEFAULT_FLOAT_32 \"\")\nset(Fortran_DEFAULT_FLOAT_64 \"-fdefault-double-8 -fdefault-real-8\")\n\nset(Fortran_DEFAULT_INT_32   \"\")\nset(Fortran_DEFAULT_INT_64   \"-fdefault-integer-8\")","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"When running cmake configure, and depending on the build precision, a subset of these flags is added to the CMAKE_Fortran_FLAGS variable thus affecting all the Fortran targets. Currently, DEFAULT_INT variables are not used in CMake build, but are provided for consistency.","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"note: Note\nWhen creating FortranCompilerFlags.<compiler type>.cmake, <compiler type> should follow the naming provided by CMAKE_Fortran_COMPILER_ID, for example, GNU for gfortran and Intel for ifort. See the CMake documentation for a list of all supported compiler vendors.","category":"page"},{"location":"Build/Build_with_cmake/#Note-on-generating-different-build-systems-with-CMake","page":"CMake","title":"Note on generating different build systems with CMake","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"CMake is a build system generator and it can create different native build systems from the same CMakeLists.txt. The full list of supported generators is available in the CMake documentation, however in practice when building HARMONIE-AROME on a Linux machine (or on a UNIX-like one in general) there are two options: the Unix Makefiles generator and the Ninja generator:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Unix Makefiles generator produces a build system based on the standard makefiles and does not use \"exotic\" tools. This is a default generator for CMake running on Linux and it usually works pretty well. However, when building with Unix Makefiles, CMake relies on its own Fortran parsers to scan the source tree and determine the build dependencies. Thus, in some rare cases of heavy CPP usage in Fortran code CMake can get inter-module dependencies wrong. The Unix Makefiles build is not parallel by default but it can be controlled, as with any conventional makefile-based build, by passing the desired -j flags to make. Additionally, when invoking the build via cmake --build command, a -j (or --parallel) flag can be used for setting the number of parallel jobs in a build-system-agnostic way, see CMake documentation.\nNinja is a modern alternative to Make. Ninja is built with focus on speed and Ninja build is parallel by default, however, unlike Make, the build files for Ninja are very cumbersome to hand-write and they are usually machine-generated. When building Fortran code with CMake Ninja generator, an explicit preprocessing step is added, thus the inter-module dependencies should be always correct (or at least these corner cases where Unix Makefiles struggles to get correct dependencies are handled correctly by Ninja). In some cases using Ninja generator can reduce the build time due to better parallelization of the build, however since Ninja has a separate preprocessing step, it generates more output and, if the file system is a bottleneck, Ninja build can be slower than Unix Makefiles build. Using the Ninja generator in CMake requires the ninja tool to be available in the $PATH at the configure time.","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"note: Note\nSpecific CMake generator can be selected at the configure time by passing the correct -G <gen> flag to cmake. For example, cmake -G Ninja <...other CMake args...> or cmake -G \"Unix Makefiles\" <...other CMake args...>.","category":"page"},{"location":"Build/Build_with_cmake/#Practical-considerations","page":"CMake","title":"Practical considerations","text":"","category":"section"},{"location":"Build/Build_with_cmake/#When-to-re-run-CMake-configure-in-my-experiment?","page":"CMake","title":"When to re-run CMake configure in my experiment?","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"In principle, it should be enough to run CMake configure only once to generate the build system and after that any modification of the source code or configuration files should be detected by the build system triggering the required re-build steps. The only time, when CMake configure should be explicitly re-run is when you add a new source file to HARMONIE-AROME. The current implementation of the CMake build scans the file system looking for the source files to compile, so just putting a new file under, say, src/surfex/SURFEX/ and re-running the build isn't enough since this new file would be still unknown to the build system, thus the need of rerunning the configure step first.","category":"page"},{"location":"Build/Build_with_cmake/#I-added-some-code-and-CMake-build-stopped-working","page":"CMake","title":"I added some code and CMake build stopped working","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Unlike makeup, CMake build for HARMONIE-AROME enforces inter-project boundaries and each project has an explicit list of its dependencies. For example, it is not possible to use modules from arpifs in surfex, but it is possible to use mse modules. If after a code modification CMake starts complaining about missing module files, then it means that this modification violates the project dependencies in the build. To fix this problem, please update your changeset to use only the available modules. If you believe that your modification is sound with respect to inter-project dependencies of HARMONIE-AROME and it's the CMake build which misses a dependency, please open a new GitHub issue explaining the problem.","category":"page"},{"location":"Build/Build_with_cmake/#Can-I-move/copy-my-build-directory-to-another-directory-and-re-use-it?","page":"CMake","title":"Can I move/copy my build directory to another directory and re-use it?","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"No, it's generally a bad idea. CMake loves absolute paths and uses them in many parts of the generated build system, thus simply moving the build directory would break the build.","category":"page"},{"location":"Build/Build_with_cmake/#Something-went-wrong-and-CMake-doesn't-behave-anymore,-can-I-refresh-the-build-without-nuking-the-whole-build-directory?","page":"CMake","title":"Something went wrong and CMake doesn't behave anymore, can I refresh the build without nuking the whole build directory?","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"You can try deleting just the CMakeCache.txt file from the build directory.","category":"page"},{"location":"Build/Build_with_cmake/#CMake-picks-a-wrong-compiler","page":"CMake","title":"CMake picks a wrong compiler","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Sometimes CMake selects a system default compiler instead of the compiler provided, for example, by loading a module. There are a few options available to force CMake to use a specific compiler, a straightforward one is to set the compiler via commonly-used environment variables (for example, export FC=ifort for a Fortran compiler). Another way, is to set the correct compilers in command-line arguments when configuring the CMake build (for example adding -DCMAKE_Fortran_COMPILER=ifort to the list of CMake arguments). CMake recognizes CMAKE_<LANG>_COMPILER passed from the command line where <LANG> can be Fortran, C or CXX.","category":"page"},{"location":"Build/Build_with_cmake/#Can-I-get-more-verbose-output-when-compiling-with-CMake?","page":"CMake","title":"Can I get more verbose output when compiling with CMake?","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"To get detailed information about individual steps and commands issued when compiling HARMONIE-AROME with CMake add -v to your build command:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"cmake --build . --target install -v","category":"page"},{"location":"Build/Build_with_cmake/#Is-there-a-way-to-visualise-dependencies-between-individual-targets-of-HARMONIE-AROME-in-CMake-build?","page":"CMake","title":"Is there a way to visualise dependencies between individual targets of HARMONIE-AROME in CMake build?","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"Since all the inter-target dependencies are defined in CMake scripts it can be useful to have an option to produce a graphical overview of the dependency graph of HARMONIE-AROME without grepping all the CMakeLists.txt files. This can be achieved by adding the --graphviz=<output file name> to the list of CMake arguments, for example:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"cmake $HM_LIB/src --graphviz=harmonie.dot","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"then the produced dependency graph can be visualized using the dot tool:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"dot -Tx11 harmonie.dot","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"The full dependency graph may be very cluttered and take quite some time to render, so it might be a good idea to plot dependencies of a single target, for example:","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"dot -Tx11 harmonie.dot.surf-static","category":"page"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"See the CMake documentation on graphviz for additional information about fine-tuning of the generated graphs.","category":"page"},{"location":"Build/Build_with_cmake/#I-need-more-information-about-CMake,-where-do-I-find-documentation?","page":"CMake","title":"I need more information about CMake, where do I find documentation?","text":"","category":"section"},{"location":"Build/Build_with_cmake/","page":"CMake","title":"CMake","text":"CMake documentation portal is a great source of detailed information about the various aspects of the CMake build system.","category":"page"},{"location":"PostProcessing/FileConversions/#File-conversions-this-page-under-construction","page":"FileConversions","title":"File conversions - this page under construction","text":"","category":"section"},{"location":"PostProcessing/FileConversions/#FA-–-GRIB","page":"FileConversions","title":"FA –> GRIB","text":"","category":"section"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"The default HARMONIE output is in FA format. HIRLAM/HARMONIE users are more used to dealing with data encoded according to GRIB, a WMO code for the representation of gridded data. Users have the option to convert HARMONIE FA format files to GRIB1 (short for GRIB edition 1), GRIB2 (short for GRIB edition 2) or NETCDF. Note that the NETCDF conversion is still experimental. References about different WMO GRIB editions (1, 2 and 3) can be found here.","category":"page"},{"location":"PostProcessing/FileConversions/#ecf/config_exp.h","page":"FileConversions","title":"ecf/config_exp.h","text":"","category":"section"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"The option to convert model output can be selected in the ecf/config_exp.h experiment configuration file:","category":"page"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"\n# **** GRIB ****\nCONVERTFA=yes                    # Conversion of FA file to grib/nc (yes|no)\nARCHIVE_FORMAT=GRIB1|2           # Format of archive files (GRIB1|GRIB2|nc). Currently nc format is only available in climate mode","category":"page"},{"location":"PostProcessing/FileConversions/#Details","page":"FileConversions","title":"Details","text":"","category":"section"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"From the perspective of harmonie suite, the conversion FA to GRIB is carried out in the following tasks:","category":"page"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"ecf/Makegrib_an.ecf - for fields produced in the analysis. This task is part of the /Expe/Date/Hour/Cycle/PostAnalysis family.\necf/Listen2file.ecf - for fields produced in the forecast. This task is part of the /Expe/Date/Hour/Cycle/Forecast family, possibly through a set of intermediate families Process-i (depending on the values of variables MULTITASK and MAKEGRIB_LISTENERS as set in the ecf/config_exp.h experiment configuration file).","category":"page"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"If ARCHIVE_FORMAT is set to GRIB1 or GRIB2, the scr/Makegrib bash script will be run from the tasks mentioned above (possibly through intermediate scripts). Finally, from the scr/Makegrib script the gl tool will be called to convert HARMONIE output from FA to GRIB. Notice that if a more verbose job output is needed, e.g. for debugging, variable PRINTLEV can be set, at the beginning of Makegrib, to something else than 0.","category":"page"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"Conversion of FA/lfi files to GRIB by gl:","category":"page"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"    gl [-c] [-p] FILE [ -o OUTPUT_FILE] [ -n NAMELIST_FILE]\n\n    gl -c FA/LFI-FILE -- converts the full field (including extension zone)\n    gl -p FAFILE      -- excludes the extension zone ( \"p\" as in physical domain only) ","category":"page"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"By default, Makegrib removes the biperiodic zone from FA files and creates GRIB files. HARMONIE data is produced on a Lambert projection. GRIB data can be interpolated onto different projections using gl. Further information is available in the gl documentation.","category":"page"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"Forecast output is converted from FA to GRIB in scr/Makegrib using the following command:","category":"page"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"  $MPPGL $BINDIR/gl -p $1 -o $2 -n namelist_makegrib${MG} || exit","category":"page"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"where ","category":"page"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"$1 is the input HARMONIE FA-file (ICSMH${HARM}+${ffff}, $HARM is the 4-char experiment identifier, $ffff is the forecast step)\n$2 is the output HARMONIE GRIB file (fc${DATE}${HH}+${FFF}grib)\nnamelist_makegrib${MG} is \n&naminterp\n outkey%yy=$YY,\n outkey%mm=$MM,\n outkey%dd=$DD,\n outkey%hh=$HH,\n outkey%mn=00,\n outkey%ff=$FF,\n time_unit=$time_unit\n pppkey(1:3)%ppp =   1, 61,184\n pppkey(1:3)%ttt = 103,105,105,\n pppkey(1:3)%lll =   0,  0,  0,\n pppkey(1:3)%tri =   0,  4,  4,\n skipsurfex = .TRUE.,\n fstart(15) = $fstart,\n fstart(16) = $fstart,\n fstart(162) = $fstart,\n fstart(163) = $fstart,\n/\nIn the namelist:\n$YY/$MM/$DD/$HH  is the forecast initial time\n$time_unit is the units of time to be used min/h\npppkey: selection of requested post-processed products (See: Postprocessing with gl for more details)\n$fstart is the start hour for time-range products such as maximum temperature.","category":"page"},{"location":"PostProcessing/FileConversions/#WMO-GRIB-editions-and-references","page":"FileConversions","title":"WMO GRIB editions and references","text":"","category":"section"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"Currently (Aug 2019) there are several editions of GRIB in use or in experimental phase.","category":"page"},{"location":"PostProcessing/FileConversions/","page":"FileConversions","title":"FileConversions","text":"GRIB edition 2 is currently the main GRIB edition. See WMO FM 92–XIV GRIB for details of the format.\nGRIB edition 1 is nowadays considered a legacy code. However it is still used, not only for legacy gridded data, but also to encode currently generated data. See WMO FM 92-XI GRIB for details of the format.\nThere is an experimental WMO GRIB edition 3. See WMO FM 92-16 GRIB for details of the format.","category":"page"},{"location":"PostProcessing/Diagnostics/#Diagnostics","page":"Diagnostics","title":"Diagnostics","text":"","category":"section"},{"location":"PostProcessing/Diagnostics/#Xtool","page":"Diagnostics","title":"Xtool","text":"","category":"section"},{"location":"PostProcessing/Diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"xtool","category":"page"},{"location":"PostProcessing/Diagnostics/#SAL","page":"Diagnostics","title":"SAL","text":"","category":"section"},{"location":"PostProcessing/Diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"SAL","category":"page"},{"location":"PostProcessing/Diagnostics/#DDH","page":"Diagnostics","title":"DDH","text":"","category":"section"},{"location":"PostProcessing/Diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"Diagnostics par Domaines Horizontaux (Diagnostics by Horizontal Domains) is a tool to create budgets of different processes in the model. Please read on in the gmap documentation","category":"page"},{"location":"PostProcessing/Diagnostics/#EZDIAG","page":"Diagnostics","title":"EZDIAG","text":"","category":"section"},{"location":"PostProcessing/Diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"From Lisa: Note, this is for printing out full 3D fields from the model physics to the FA-file. ","category":"page"},{"location":"PostProcessing/Diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"In the routine that you would like to print out your fields add args:\n& PDIAG, KNDIAG,&\nand declare them\nINTEGER(KIND=JPIM),INTENT(IN) :: KNDIAG\nREAL(KIND=JPRB)   ,INTENT(OUT)   :: PDIAG(KLON,KLEV,KNDIAG)\nPut values in the array if its dimension allows it, e.g.\nIF (KNDIAG.GE.1) THEN\n   PDIAG(KIDIA:KFDIA,KTDIA:KLEV,1)= YOURVAL(KIDIA:KFDIA,KTDIA:KLEV)\nENDIF\nor anything you wish. Note that the variable YOURVAL is now stored in NGFL_EZDIAG=1.","category":"page"},{"location":"PostProcessing/Diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"You can store this way up to 25 diagnostic 3D fields in the historic files.","category":"page"},{"location":"PostProcessing/Diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"If you want to store 2D fields, you can put them at different levels in the same 3D array.","category":"page"},{"location":"PostProcessing/Diagnostics/","page":"Diagnostics","title":"Diagnostics","text":"Remake the interfaces if running AROME (not needed if running ALARO), before recompiling.\nIn the NAMGFL namelist:\n! ADDITIONAL FIELDS FOR DIAGNOSTIC\n   NGFL_EZDIAG=1,          ! <=25\n   YEZDIAG_NL(1)%CNAME='YOURVAL',\n  YEZDIAG_NL(1)%LADV=.F.,\nIf you add more fields (e.g. you set NGFL_EZDIAG=4), I think you will also need to set the grib parameter, e.g. (the default is 999, that you can leave for the first one).\nYEZDIAG_NL(2)%IGRBCODE=998,\nYEZDIAG_NL(3)%IGRBCODE=997,\nYEZDIAG_NL(4)%IGRBCODE=996,\nNote that the two first places are already defined in harmonie_namelist.pm.\nIn order to have your variable converted from FA to grib, add the new variable in util/gl/inc/trans_tab.h","category":"page"},{"location":"EPS/BDSTRATEGY/#Boundary-strategies-for-HarmonEPS:-SLAF-and-EC-ENS","page":"BDSTRATEGY","title":"Boundary strategies for HarmonEPS: SLAF and EC ENS","text":"","category":"section"},{"location":"EPS/BDSTRATEGY/","page":"BDSTRATEGY","title":"BDSTRATEGY","text":"Presently there are two available options for choosing boundaries when running HarmonEPS: EC ENS or SLAF In the branch harmonEPS-40h1.1 SLAF is set as default ","category":"page"},{"location":"EPS/BDSTRATEGY/","page":"BDSTRATEGY","title":"BDSTRATEGY","text":" Settings for SLAF (default in branch harmonEPS-40h1.1 ) Settings for EC ENS\necf/config_exp.h BDSTRATEGY=simulate_operational BDSTRATEGY=eps_ec\n BDINT=1 (can be set to larger value) BDINT=3 (or larger, hourly input is not possible)\nmsms/harmonie.pm  Comment out SLAF settings: #SLAFLAG, #SLAFDIFF, #SLAFK\n 'ENSBDMBR' => [ 0] 'ENSBDMBR' => [ 0, 1..10] (or any other members from EC ENS you would like to use)","category":"page"},{"location":"EPS/BDSTRATEGY/","page":"BDSTRATEGY","title":"BDSTRATEGY","text":"More information about how to treat the settings in harmonie.pm, see: here Note that BDSTRATEGY=eps_ec uses EC ENS data as stored in the GLAMEPS archive (as ECMWF does not store model levels in MARS). Only EC ENS at 00UTC and 12UTC are in this archive, and with 3h output, hence you need to use BDINT=3 for this option.","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#Jk-as-a-pre-mixing-method","page":"LSMIX and Jk","title":"Jk as a pre-mixing method","text":"","category":"section"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"The 3D-Var cost function including the Jk term can be written:","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"J(x) = J_b + J_o + J_k = frac12 (x - x_b)^rm T B^-1(x - x_b) + frac12 (y - Hx)^rm TR^-1(y - Hx) + frac12 (x - x_LS)^rm T V^-1(x - x_LS)","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"Setting the gradient to zero, we have at the optimal x:","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"nabla J = B^-1(x - x_b) - H^rm TR^-1(y - Hx) + V^-1(x - x_LS) = 0 ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"or","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"leftB^-1 + V^-1 + H^rm TR^-1Hright left(x - x_b right) = H^rm TR^-1(y - Hx_b) + V^-1(x_LS - x_b) ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#Equivalent-pre-mixed-first-guess","page":"LSMIX and Jk","title":"Equivalent pre-mixed first guess","text":"","category":"section"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"Assume now that widetildex_b is some yet unknown, pre-mixed field depending on x_b and x_LS that we want to determine. By adding and subtracting identical terms to the gradient equation, we have","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"B^-1(x - x_b + widetildex_b - widetildex_b) - H^rm TR^-1(y - Hx + Hwidetildex_b - Hwidetildex_b) + V^-1(x - x_LS + widetildex_b - widetildex_b) = 0","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"which, when reorganized gives","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"leftB^-1 + V^-1 + H^rm TR^-1H right left(x - widetildex_bright) = H^rm TR^-1(y - Hwidetildex_b) + B^-1(x_b - widetildex_b) + V^-1(x_LS - widetildex_b) ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"If the last two terms on the right hand side add up to zero, i.e.,","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"B^-1(x_b - widetildex_b) + V^-1(x_LS - widetildex_b) = 0 ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"which means that","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"widetildex_b = B^-1 + V^-1^-1 ( B^-1 x_b + V^-1 x_LS ) ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"then we see that by using this mixed first guess the Jk term can be omitted, provided we use a modified B-matrix with the property that","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"widetildeB^-1 = B^-1 + V^-1  ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"By writing","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"B^-1 + V^-1 = B^-1(B + V)V^-1 = V^-1(B + V)B^-1 ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"we easily see by simply inverting that","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"widetildeB = B^-1 + V^-1^-1 = B(B + V)^-1V = V(B + V)^-1B  ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"To conclude, a 3D-Var minimization with Jk is equivalent to a minimization without the Jk term, provided that one pre-mixes the two first guess fields according to","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"widetildex_b = B^-1 + V^-1^-1 ( B^-1 x_b + V^-1 x_LS ) = widetildeB( B^-1 x_b + V^-1 x_LS ) = V(B + V)^-1x_b + B(B + V)^-1x_LS ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"and use the following covariance matrix for this mixed first guess:","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"widetildeB = B^-1 + V^-1^-1 = B(B + V)^-1V = V(B + V)^-1B  ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"Whether this is implementable in practice is a different story, it just shows the theoretical equivalence, and how LSMIXBC should ideally be done if Jk is the right answer.","category":"page"},{"location":"ForecastModel/Forecast/#Forecast","page":"Forecast","title":"Forecast","text":"","category":"section"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"scr/Forecast is the script, which initiates actual  forecast run (ALADIN/AROME/ALARO depending on FLAG and PHFLAG).","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"Input parameters: none.\nData: Boundary files (ELSCF*-files). Initial file (fc_start). If data assimilation is used, fc_start is the analysis file. In case of dynamical adaptation, fc_start is the first boundary file. In case of AROME, Surfex initial file (SURFXINI.lfi) is also needed scr/Prep_ini_surfex. \nNamelists: namelist templates nam/namelist_fcst${FLAG}_default are fetched based on FLAG and PHFLAG. The templates are completed in scr/Forecast based on the choices of NPROCX, NPROCY (see config-sh/submit.ecgb), TFLAG, OUTINT, BDINT and REDUCELFI. In case of AROME also the namelists to control SURFEX-scheme  nam/TEST.des and nam/EXSEG1.nam are needed.\nExecutables: as defined by MODEL.\nOutput: Forecast files (spectral files ICMSHALAD+*). In case of AROME, Surfex files containing the surface data (AROMOUT_*.lfi). ","category":"page"},{"location":"ForecastModel/Forecast/#Forecast-namelists","page":"Forecast","title":"Forecast namelists","text":"","category":"section"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"The current switches in the HARMONIE system (in ecf/config_exp.h) provide only very limited possibility to control the different aspects of the model. If the user wants to have more detailed control on the specific schemes etc., one has to modify the variety of the namelists options.","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"In general, the different namelist options are documented in the source code modules (e.g. src/arp/module/*.F90). Below is listed information on some of the choices.   ","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"NH-dynamics/advection/time stepping:","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"A detailed overview of the such options has been given by Vivoda (2008). ","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"Upper air physics switches","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"Switches related to different schemes of ALADIN/ALARO physics, src/arp/module/yomphy.F90.\nSwitches related to physics schemes in AROME src/arp/module/yomarphy.F90.\nSwitches to tune different aspects of physics, src/arp/module/yomphy0.F90, src/arp/module/yomphy1.F90, src/arp/module/yomphy2.F90 and src/arp/module/yomphy3.F90\nSwitches related to HIRLAM physics, src/arp/module/yhloption.F90 and src/arp/setup/suhloption.F90.","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"Initialization switch","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"Initialization is controlled by namelist NAMINI/NEINI, src/arp/module/yomini.F90.","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"Horizontal diffusion switches","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"Horizontal diffusion is controlled by namelist NAMDYN/RDAMP*, src/arp/module/yomdyn.F90#L55. Larger the coefficient, less diffusion.","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"MPP switches","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"The number of processors in HARMONIE are given in config-sh/submit.ecgb. These values are transfered in to src/arp/module/yomct0.F90#L276 and src/arp/module/yommp.F90.","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"Surface SURFEX switches","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"The SURFEX scheme is controlled through namelist settings in nam/surfex_namelists.pm. The different options are described here.","category":"page"},{"location":"ForecastModel/Forecast/#Archiving","page":"Forecast","title":"Archiving","text":"","category":"section"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"Archiving has a two layer structure. Firstly, all the needed analysis forecast and field extract files   are stored in ARCHIVE directory by scr/Archive_fc. This is the  place where the postprocessing step expects to find the files. ","category":"page"},{"location":"ForecastModel/Forecast/","page":"Forecast","title":"Forecast","text":"At ECMWF all the requested files are stored to ECFS into directory ECFSLOC by the script scr/Archive_ECMWF","category":"page"},{"location":"Observations/SYNOP/#SYNOP-observations","page":"SYNOP","title":"SYNOP observations","text":"","category":"section"},{"location":"Observations/SYNOP/","page":"SYNOP","title":"SYNOP","text":"This page documents how SYNOP observations (z, ps, u10m, t2m, rh2m) are assimilated. An overview of GTS messages, BUFR, conversion to ODB and screening options is provided.","category":"page"},{"location":"Observations/SYNOP/#SYNOP-on-the-GTS","page":"SYNOP","title":"SYNOP on the GTS","text":"","category":"section"},{"location":"Observations/SYNOP/#BUFR","page":"SYNOP","title":"BUFR","text":"","category":"section"},{"location":"Observations/SYNOP/#Conversion-to-ODB","page":"SYNOP","title":"Conversion to ODB","text":"","category":"section"},{"location":"Observations/SYNOP/","page":"SYNOP","title":"SYNOP","text":"At this stage we decide whether we want to assimilate geopotential, calculated from mean sea level pressure (MSLP) or surface pressure (Ps),  or Ps available in the SYNOP BUFR report.","category":"page"},{"location":"Observations/SYNOP/#Surface-pressure","page":"SYNOP","title":"Surface pressure","text":"","category":"section"},{"location":"Observations/SYNOP/","page":"SYNOP","title":"SYNOP","text":"In the context of Bator:","category":"page"},{"location":"Observations/SYNOP/","page":"SYNOP","title":"SYNOP","text":"The BUFR namelist the SYNOPPSMETHOD entry provides an option to assimilate MSLP observations only (MSL), Ps observations only (SFC) or take Ps then MSLP in that order (ANY). \n&BUFR\nSYNOPPSMETHOD='MSL',\nThe Bator param file can also used to read MSLP/Ps or not. If the values entry, NN, is set to -1 then the value will not be read by Bator.\nvalues    NN  010004 Pressure\nvalues    NN  010051 Pressure reduced to mean sea level\nZ or Ps values are blacklisted in the LISTE_LOC read by Bator. By default, Ps (110) is blacklisted for SYNOP (obstype=1) and BUOY (obstype=4). These \"110\" values should be changed to \"1\" in order to blacklst Z instead of Ps.\nN  1                  108\nN  1                  110\nN  4                  110","category":"page"},{"location":"Observations/SYNOP/#Screening","page":"SYNOP","title":"Screening","text":"","category":"section"},{"location":"Observations/SYNOP/#Blacklisting","page":"SYNOP","title":"Blacklisting","text":"","category":"section"},{"location":"Observations/SYNOP/","page":"SYNOP","title":"SYNOP","text":"t2m, rh2m not assimilated when soe, solar elevation, is less than 10.\nu10m/v10m not assimilated if codetype is 11/14/16/170 (SYNOP LAND)\nu10m/v10m not assimilated if land-sea mask is > 0\nu10m/v10m not assimilated if station and model orography differ by more than 200 m\nrh2m not assimilated if codetype is 21/22/23/24/182 (SYNOP SHIP)\nps/z not assimilated if model orography is greater than 500 m","category":"page"},{"location":"Observations/SYNOP/","page":"SYNOP","title":"SYNOP","text":"if (OBSTYP = synop) then\n\n    if VARIAB in (t2m,rh2m) then\n        if (soe < 10.0 ) then fail(CONSTANT); endif;\n    endif;\n\n    if VARIAB in (u10m, v10m) then\n        if (SPECIFIC > 0.0 ) then\n           if (CODTYP in (11, 14, 16, 170)) then fail(CONSTANT); endif;\n           if (RLSMASK > 0.) then fail(CONSTANT); endif;\n         end if;\n        if (STALT >= 0.) and (abs(STALT - MODORO) > 200.0) then fail(CONSTANT); endif;\n    endif;\n\n    if VARIAB in (rh, q) then\n        if (PRESS <= 300.) then fail(CONSTANT); endif;\n    endif;\n\n    if (VARIAB = rh2m) then\n        if (CODTYP in (21, 22, 23, 24, 182)) then fail(CONSTANT); endif;\n        if (RLSMASK < 0.5) then fail(CONSTANT); endif;\n        if (STALT >= 0.) and (abs(STALT - MODORO) > 200.0) then fail(CONSTANT); endif;\n    endif;\n\n    if VARIAB in (ps, z) then\n        if (STALT >= 500.) then fail(CONSTANT); endif;\n    endif;\n\nendif;","category":"page"},{"location":"Observations/SYNOP/#Screening-settings","page":"SYNOP","title":"Screening settings","text":"","category":"section"},{"location":"Observations/SYNOP/","page":"SYNOP","title":"SYNOP","text":"Namelist settings ...","category":"page"},{"location":"DataAssimilation/DFS/#DFS","page":"DFS","title":"DFS","text":"","category":"section"},{"location":"DataAssimilation/DFS/","page":"DFS","title":"DFS","text":"The DFS calculation software is still in under development. It is however possible to run the software already now even though it is not fully automated. Start by downloading the tar-file dfscalc.tar","category":"page"},{"location":"DataAssimilation/DFS/","page":"DFS","title":"DFS","text":"The following instructions is also available in the README file.","category":"page"},{"location":"DataAssimilation/DFS/","page":"DFS","title":"DFS","text":"To run the DFS calculations and plot the results do the following","category":"page"},{"location":"DataAssimilation/DFS/","page":"DFS","title":"DFS","text":"Edit the include.paths script to set dates and paths to your experiment and dfs directory\nRun prep_dfs in batch mode. This script generates two sets of ccma files, one with perturbed observations and one original unperturbed. \nBefore running the minimisation for the disturbed observations the namelist of your experiment must be copied to a file called \"namelminim\" in the dfs script dir. In one of your HMDate-files search for \"Start of log: fort.4\" and copy everything between the hash lines \"#############################\" into namel_minim.\nRun RunMinim in batch mode. This will generate new an-fg statistics for the perturbed observations.\nBefore running extract you need to create a compiled MANDALAY. This is easiest done by doing the following:","category":"page"},{"location":"DataAssimilation/DFS/","page":"DFS","title":"DFS","text":"Set up a new experiment Check out src/obd/ddl/mandalay.sql Replace this file by the mandalay.sql in dfs_scr (included in this tar-file) Start your experiment and let it run until the compilation is complete Copy the created MANDALAY to dfs_scr/bin (or point to it in extract_dfs)","category":"page"},{"location":"DataAssimilation/DFS/","page":"DFS","title":"DFS","text":"It is also necessary to compile dfscomp.F90 and dfstot.F90 e.g:","category":"page"},{"location":"DataAssimilation/DFS/","page":"DFS","title":"DFS","text":"gfortran -o bin/dfscomp.x dfscomp.F90\ngfortran -o bin/dfstot.x dfstot.F90","category":"page"},{"location":"DataAssimilation/DFS/","page":"DFS","title":"DFS","text":"Now run extract_dfs in batch mode. This generates the file $STARTDIR/OUTPUT/dsf.tot (STARTDIR in specified in include.paths) that contains the DFS statistics for the period. This can be plotted e.g. with the included R-script.\nPlot the resuts with dfs_plot.R. Place dfs.tot and dfs_plot.R in the same directory and type: Rscript dfs_plot.R This generates the file dfs_tot.eps in the same directory.","category":"page"},{"location":"EPS/SPPT/#SPPT","page":"SPPT","title":"SPPT","text":"","category":"section"},{"location":"EPS/SPPT/","page":"SPPT","title":"SPPT","text":"The SPPT configuration within HarmonEPS is being tested over the period 2016053000 to 2016060500 using the !MetCoOp domain. It has been found that there are some problems with the default pattern generator and thus it has been decided to use the Stochastic Pattern Generator (SPG).","category":"page"},{"location":"EPS/SPPT/","page":"SPPT","title":"SPPT","text":"Below is a table of experiments which will be completed in order to find a suitable configuration of the SPG control parameters TAU (time correlation scale) and XLCOR (length correlation scale). The value of the standard deviation of the perturbation amplitudes (SDEV_SDT) is kept fixed at 0.20 as is the clipping ratio of the perturbations (XCLIP_RATIO_SDT=5.0).  These values along with the default value for XLCOR come from suggested settings used by Mihaly Szucs.","category":"page"},{"location":"EPS/SPPT/","page":"SPPT","title":"SPPT","text":"First of all, keeping the XLCOR parameter constant (set at the default value of 2000000), TAU will be varied between 1h and 24h as shown in the table. The value of TAU is in seconds in the table below. The value of XLCOR is in metres.","category":"page"},{"location":"EPS/SPPT/","page":"SPPT","title":"SPPT","text":"The experiments are started by typing ~hlam/Harmonie start DTG=2016053000 DTGEND=2016060500 BUILD=yes","category":"page"},{"location":"EPS/SPPT/","page":"SPPT","title":"SPPT","text":"= Experiment Name   = = Who = = DTG  = = DTGEND = =  Version   = = Domain  = = TAU = = XLCOR = = Description and Comments = = Status = = Verification =\nSPPT_only_40h111_2000km_1h Alan 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 3600 2000000 XLCOR constant, TAU varying Suspended No\nSPPT_only_40h111_2000km_3h Karoliina 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 10800 2000000 XLCOR constant, TAU varying Crash No\nSPPT_only_40h111_2000km_6h Karoliina 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 21600 2000000 XLCOR constant, TAU varying Complete Yes\nSPPT_only_40h111_2000km_9h Alan 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 32400 2000000 XLCOR constant, TAU varying Complete Yes\nSPPT_only_40h111_2000km_12h Janne 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 43200 2000000 XLCOR constant, TAU varying Complete Yes\nSPPT_only_40h111_2000km_15h Karoliina 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 54000 2000000 XLCOR constant, TAU varying Complete Yes\nSPPT_only_40h111_2000km_18h Alan 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 64800 2000000 XLCOR constant, TAU varying Complete Yes\nSPPT_only_40h111_2000km_21h Janne 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 75600 2000000 XLCOR constant, TAU varying Complete Yes\nSPPT_only_40h111_2000km_24h Janne 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 86400 2000000 XLCOR constant, TAU varying Complete Yes","category":"page"},{"location":"EPS/SPPT/","page":"SPPT","title":"SPPT","text":"Once these experiments have been completed testing will commence on keeping the time correlation scale constant and the spatial scale will be varied. Below is a table of experiments to this effect. ","category":"page"},{"location":"EPS/SPPT/","page":"SPPT","title":"SPPT","text":"A default value of 8h will be used for TAU as per the suggested value from Mihaly Szucs.","category":"page"},{"location":"EPS/SPPT/","page":"SPPT","title":"SPPT","text":"= Experiment Name   = = Who = = DTG  = = DTGEND = =  Version   = = Domain  = = TAU = = XLCOR = = Description and Comments = = Status = = Verification =\nSPPTonly40h111100km8h Alan 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 28800 100000 XLCOR varying, TAU constant Complete Yes\nSPPTonly40h111200km8h Janne 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 28800 200000 XLCOR varying, TAU constant Complete Yes\nSPPTonly40h111400km8h Janne 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 28800 400000 XLCOR varying, TAU constant Complete Yes\nSPPTonly40h111600km8h Alan 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 28800 600000 XLCOR varying, TAU constant Complete Yes\nSPPTonly40h111800km8h Janne 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 28800 800000 XLCOR varying, TAU constant Complete Yes\nSPPTonly40h1111000km8h Karoliina 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 28800 1000000 XLCOR varying, TAU constant Complete Yes\nSPPTonly40h1111200km8h Alan 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 28800 1200000 XLCOR varying, TAU constant Complete Yes\nSPPTonly40h1111500km8h Karoliina 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 28800 1500000 XLCOR varying, TAU constant Complete Yes\nSPPTonly40h1111800km8h Karoliina 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 28800 1800000 XLCOR varying, TAU constant Complete Yes","category":"page"},{"location":"EPS/SPPT/","page":"SPPT","title":"SPPT","text":"The next step in the SPPT sensitivity analysis will be a set of experiments designed to test the impact of the SDEV parameter. Default values of 8h and 2000000 for TAU and XLCOR are used respectively.","category":"page"},{"location":"EPS/SPPT/","page":"SPPT","title":"SPPT","text":"The XCLIPRATIOSDT parameter will also be adjusted as a function of the SDEVSDT value. Initially keeping the clipping at 1.0 (clipping value is XCLIPRATIOSDT * SDEVSDT), but also exploring other options.","category":"page"},{"location":"EPS/SPPT/","page":"SPPT","title":"SPPT","text":"= Experiment Name   = = Who = = DTG  = = DTGEND = =  Version   = = Domain  = = SDEV_SDT = = XCLIPRATIOSDT = = Description and Comments = = Status = = Verification =\nSPPTonly40h111sdev01 Alan 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 0.1 10.0 SDEV and XCLIP varying Complete Yes\nSPPTonly40h111sdev02 Janne 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 0.2 5.0 SDEV and XCLIP varying Complete Yes\nSPPTonly40h111sdev03 Karoliina 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 0.3 3.3 SDEV and XCLIP varying Complete Yes\nSPPTonly40h111sdev04 Alan 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 0.4 2.5 SDEV and XCLIP varying Complete Yes\nSPPTonly40h111sdev05 Janne 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 0.5 2.0 SDEV and XCLIP varying Complete Yes\nSPPTonly40h111sdev06 Karoliina 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 0.6 1.65 SDEV and XCLIP varying Complete Yes\nSPPTonly40h111sdev07 Alan 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 0.7 1.4 SDEV and XCLIP varying Complete Yes\nSPPTonly40h111sdev08 Janne 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 0.8 1.25 SDEV and XCLIP varying Complete Yes\nSPPTonly40h111sdev09 Karoliina 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 0.9 1.1 SDEV and XCLIP varying Complete Yes\nSPPTonly40h111sdev10 Alan 2016053000 2016060500 harmonEPS40h1.1.1(17985) METCOOP25B 1.0 1.0 SDEV and XCLIP varying Complete Yes","category":"page"},{"location":"System/HarmonieBenchMark/#Harmonie-RAPS-benchmark","page":"Benchmark","title":"Harmonie RAPS benchmark","text":"","category":"section"},{"location":"System/HarmonieBenchMark/#Background","page":"Benchmark","title":"Background","text":"","category":"section"},{"location":"System/HarmonieBenchMark/","page":"Benchmark","title":"Benchmark","text":"This page describes the current situation for harmonie as a benchmarking tool. During 2013 several HIRLAM countries will start the procurement for new computers. We hope that by coordinating the efforts we can make a win-win situation for all concerned parties, both vendors and institutes. The benchmarkers on the vendors' side will be able to concentrate their efforts on a single source code base for better productivity and enhanced feedback to the community. Moreover, we believe that the efforts with a common benchmark package will improve the model and might be found useful also in other Harmonie institutes. For the current effort cy38h1 have been chosen as the baseline version. The cycle is currently under preparation and evaluation within the ALADIN/HIRLAM community.","category":"page"},{"location":"System/HarmonieBenchMark/#Getting-the-package","page":"Benchmark","title":"Getting the package","text":"","category":"section"},{"location":"System/HarmonieBenchMark/","page":"Benchmark","title":"Benchmark","text":"The package is under preparation and is based on harmonie-38h1.alpha.2 plus [11688] and [11708].","category":"page"},{"location":"System/HarmonieBenchMark/","page":"Benchmark","title":"Benchmark","text":"The second, version is available here.","category":"page"},{"location":"System/HarmonieBenchMark/","page":"Benchmark","title":"Benchmark","text":"Input data and some simple scripts can also be found on hirlam.org [https://hirlam.org/portal/download/benchmark/data/cy38].","category":"page"},{"location":"System/HarmonieBenchMark/#Organization-of-the-package","page":"Benchmark","title":"Organization of the package","text":"","category":"section"},{"location":"System/HarmonieBenchMark/#Input-files","page":"Benchmark","title":"Input files","text":"","category":"section"},{"location":"System/HarmonieBenchMark/","page":"Benchmark","title":"Benchmark","text":"Please note that it is crucial to have right namelist settings to enable reproducibility. A set of input files including feasible namelists for different problem sizes is provided to ease up the startup for testing. The problem sizes are sorted according to the international clothing sizes as follows:     ","category":"page"},{"location":"System/HarmonieBenchMark/","page":"Benchmark","title":"Benchmark","text":" XS)\n   # 50x50x65, 2.5km resolution\n   BDDIR=XS\n   MBX_SIZE=20000000\n   TSTEP=60\n\n  M)\n   #  384x400x65, 2.5km resolution\n   BDDIR=M\n   MBX_SIZE=200000000\n   TSTEP=60\n\n  L)\n   #  Nlon,Nlat,Nlev :         750         960          65, 2.5km resolution\n   BDDIR=L\n   BDINTERVAL=10800.\n   TSTEP=60\n\n  XL)\n   #  Nlon,Nlat,Nlev :1200        1200          65, 2.5km resolution\n   BDDIR=XL\n   MBX_SIZE=200000000\n   TSTEP=60\n\n  XXL)\n   # 1600x1600x65, 2.5km resolution\n   BDDIR=XXL\n   MBX_SIZE=2000000000\n   TSTEP=60","category":"page"},{"location":"System/HarmonieBenchMark/","page":"Benchmark","title":"Benchmark","text":"You will also need the files covers.tar and rrtm_const.tar.","category":"page"},{"location":"System/HarmonieBenchMark/#Status","page":"Benchmark","title":"Status","text":"","category":"section"},{"location":"System/HarmonieBenchMark/","page":"Benchmark","title":"Benchmark","text":"The model is not reproducible for different NPROCX/NRPOCY with the default HARMONIE edmfm scheme. Until solved reproducibility can be achieved by removing from &NAMPARAR:","category":"page"},{"location":"System/HarmonieBenchMark/","page":"Benchmark","title":"Benchmark","text":"   CMF_CLOUD='STAT',\n   CMF_UPDRAFT='DUAL',\n   LMIXUV=.TRUE.,","category":"page"},{"location":"System/HarmonieBenchMark/","page":"Benchmark","title":"Benchmark","text":"* This makes the the code reproducible on c2a(IBMP7) with the default compilation flags. UPDATE: also with multiple OpenMP threads. \n* Not reproducible on Intel Sandy Bridge (ifort 12.0.5.220 ) with -O2 -fp-model precise but with -O0. OpenMP not reproducible even with -O0.","category":"page"},{"location":"System/HarmonieBenchMark/","page":"Benchmark","title":"Benchmark","text":"OpenMP works technically (on Intel Sandy Bridge, ifort 12.0.5.220) but we don't get reproducible results when changing the number of openmp threads.\nOpenMP now also tested with ifort 13.0.1 on Sandy Bridge (not using -xAVX flag). Still reproducibility cannot be achieved with any combination of (the many!) compiler flags and environment variable settings tested.\nIf linked without MKL libraries reproducibility is retained.\nGfortran 4.6.3 on an Ubuntu 12.04 workstation, domain XS, gives full reproducibility against variations in NPROCX, NPROCY and OMP_NUM_THREADS.","category":"page"},{"location":"Observations/GNSS/#GNSS-ZTD-observations","page":"GNSS","title":"GNSS ZTD observations","text":"","category":"section"},{"location":"Observations/GNSS/#Introduction","page":"GNSS","title":"Introduction","text":"","category":"section"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"The NRT GNSS delay data contain information about the amount of water vapour above the GNSS sites.  E-GVAP European program’s aim is to provide its EUMETNET members with European GNSS delay and water vapour estimates for operational meteorology in near real-time. Currently, the E-GVAP network consists of more than 1500 GNSS sites.","category":"page"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"E-GVAP Programme here","category":"page"},{"location":"Observations/GNSS/#GNSS-ZTD-data","page":"GNSS","title":"GNSS ZTD data","text":"","category":"section"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"Raw data from GNSS sites are collected by a number of GNSS analysis centers, which process the data to estimate the Zenith Total Delays (ZTD) and other parameters. The ZTDs are then forwarded to a data server, for distribution to meteorological institutes. The observations are currently distributed from Met Office, in two different formats: BUFR that are distributed via GTS to the meteorological centers or in ASCII format, that may be download via ftp.","category":"page"},{"location":"Observations/GNSS/#Preprocessing-the-GNSS-ZTD-data","page":"GNSS","title":"Preprocessing the GNSS ZTD data","text":"","category":"section"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"The preprocessing of these data should be local, depending if you want to have them in BUFR or ASCII format.  ASCII option needs a local script to get the files from Metoffice server and transform them from COST format (EGVAP) into OBSOUL format. (In this case there is an optional script inside scr directory in Harmonie called GNSStoOBSOUL that could transforms ascii into OBSOUL format).","category":"page"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"Apart of the preprocessing, a White List of sites to be assimilated in your domain is needed. It will contain the values of:","category":"page"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"   statid lat lon alt dts bias sd obserr","category":"page"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"where statid is the name of the site (NNNNPPPP: NNNN=site PPPP=Procesing centre) , dts is the frequency in minutes between obs, and sd the standard deviation of that station  and obserr the observation error. You are supposed to have calculated these values before launching the experiment.","category":"page"},{"location":"Observations/GNSS/#Harmonie-changes-to-assimilate-GNSS-ZTD-data","page":"GNSS","title":"Harmonie changes to assimilate GNSS ZTD data","text":"","category":"section"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"scr/","category":"page"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"Bator and Fetch_assim_data have the white list path.\nOulan : has the white list and gnss observation files paths and cat this one to the rest of conventional observation file.   \ninclude.ass:  This script has two options about gnss bias correction: static bias correction (LSTATIC_BIAS) or variational bias correction (LVARBC_GNSS).  For the first case, a fix bias value from each site is read from the White List and then substracted from the corresponding observation value. For the second case, VarBC, it is also  needed to set in this script the  cold start option.\nexport GNSS_OBS=1            #GNSS\nexport LSTATIC_BIAS=F        #Swich for bias correction or not,(T|F)\nexport LVARBC_GNSS=T         #Swich for GNSS varbc\nexport VARBC_COLD_START=yes  #yes/no","category":"page"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"nam/  Here it should be the White list, called list.gpssol.201512 for example  /src/arpifs/obs_preproc/","category":"page"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"redgps.F90 : This routine is where the horizontal thinning is done (Cy40) , so the thinning distance  could be selected here.","category":"page"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"/src/blacklist/","category":"page"},{"location":"Observations/GNSS/","page":"GNSS","title":"GNSS","text":"mf_blacklist.b: here is posible to blacklist the gnss observations so to calculate the varbc coefficients. It can be done tuning to experimental the apdss variable.","category":"page"},{"location":"Observations/ObservationPreprocessing/#HARMONIE-Observation-Preprocessing","page":"Preprocessing","title":"HARMONIE Observation Preprocessing","text":"","category":"section"},{"location":"Observations/ObservationPreprocessing/#Introduction","page":"Preprocessing","title":"Introduction","text":"","category":"section"},{"location":"Observations/ObservationPreprocessing/","page":"Preprocessing","title":"Preprocessing","text":"The following figure shows different schematic steps in the HARMONIE data assimilation system.   It is worth mentioning some differences between the observation pre-processing systems used by ECMWF, Météo France, and HIRLAM. Some of these differences are listed below:","category":"page"},{"location":"Observations/ObservationPreprocessing/","page":"Preprocessing","title":"Preprocessing","text":" AROME/HARMONIE-AROME IFS\ndata format/content BUFR, but sometimes with own table BUFR with WMO code\ncreation of ODB database Bator converts BUFR to ODB b2o/bufr2odb converts BUFR to ODB\nblacklisting technique Bator (LISTE_LOC, LISTE_NOIRE_DIAP), Screening (hirlam_blacklist.B) & Minim (NOTVAR namelist) Screening only","category":"page"},{"location":"Observations/ObservationPreprocessing/#Observation-file-preparation","page":"Preprocessing","title":"Observation file preparation","text":"","category":"section"},{"location":"Observations/ObservationPreprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Observation Data: Observation data – where to get your BUFR!","category":"page"},{"location":"Observations/ObservationPreprocessing/#Preprocessing-Software","page":"Preprocessing","title":"Preprocessing Software","text":"","category":"section"},{"location":"Observations/ObservationPreprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Bator: Bator - reads BUFR/HDF5/OBSOUL observation data and writes ODBs used by data assimilation","category":"page"},{"location":"Observations/ObservationPreprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Other possibilities include:","category":"page"},{"location":"Observations/ObservationPreprocessing/","page":"Preprocessing","title":"Preprocessing","text":"Oulan: Oulan - Converts conventional BUFR data to OBSOUL file that is read by BATOR\nCope: Cope - preparation of ODBs used by data assimilation (in development)","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/#Surface-Data-Assimilation-in-HARMONIE","page":"Surface Analysis","title":"Surface Data Assimilation in HARMONIE","text":"","category":"section"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/#Surface-related-variables-in-ecf/config_exp.h-:","page":"Surface Analysis","title":"Surface related variables in ecf/config_exp.h :","text":"","category":"section"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"Surface model: SURFACE = surfex / old_surface","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"surfex: SURFEX is used as surface model (default and used in all Harmonie configurations) The surface fields are in a separat AROMOUT_.LLLL.lfi file in LFI format. \nold_surface: surface physics modelled by routines integrated in code The surface fields are a part of the atmospheric file (ICMSHXXXX+LLLL) in FA format.","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"Surface analysis method: ANASURF = \"CANARI_OI_MAIN\" / \"CANARI_EKF_SURFEX\"","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"the horizontal interpolation of screen level parameters is performed by CANARI in both cases\nCANARIOIMAIN updates soil temperature, water and ice based on 2m analysis increments using coefficients that are derived empirically for ISBA2/3-layers scheme\nCANARIEKFSURFEX (experimental) updates soil parameters using the Extended Kalman Filter method.","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"ANASURF_MODE = \"before\" / \"after\"/ \"both\" - surface analysis performed before/after/both before and after 3DVAR","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"ANASURF_INLINE = \"yes\" /\"no\"","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"yes: call SODA for updating soil parameters inside CANARI (default and experimental)\nno: soil parameters are updated after CANARI","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/#Some-details","page":"Surface Analysis","title":"Some details","text":"","category":"section"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"The default surface model is SURFEX and the default surface assimilation scheme is CANARI_OI_MAIN. CANARI_EKF_SURFEX was first implemented in cy37 and will be undergoing tests in experimental and research mode before it can be used in operational setups.","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"CANARI is used for Optimum Interpolation horizontally to find analysis increments in each grid point based on observations minus first guess. The SURFEX assimilation schemes use two different techniques to propagate this information into the ground. The two ways CANARI is used is separated by two namelist settings needed when running with SURFEX:","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"LAEICS=.FALSE.","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"No initialization of ground variables are done as they are in the SURFEX file","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"LDIRCLSMOD=.TRUE.","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"2 metre variables taken directly from input file because they without surfex are diagnosed from 0 metre and lowest model height with the model specific routine achmt.","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"CANARI was designed before SURFEX was introduced and some of the climate variables that normally exist in the input file for CANARI, do not exist when using SURFEX. This means the task Addsurf is run before CANARI, adding the needed fields from the FA climate file (mMM).","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"The screen level analyisis (eg. T2m) used in blending/3DVAR/4DVAR is the same as for CANARI in the old_surface case.","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/#Variables-updated-in-CANARI-for-old_surface-and-SURFEX","page":"Surface Analysis","title":"Variables updated in CANARI for old_surface and SURFEX","text":"","category":"section"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"src/arpifs/module/qactex.F90","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"HARMONIE namelist settings:","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"!  * LAET2M  : .T. 2 meter temperature analysis\n!  * LAEH2M  : .T. 2 meter humidity analysis\n!  * LAESNM  : .T. snow analysis\n!  * LAESST  : .F. SST analysis\n!  * LECSST  : .T. use ECMWF SST\n!  * LAEPDS  : .F. surface pressure analysis\n!  * LAEUVT  : .F. wind and temperature analysis\n!  * LAEHUM  : .F. humidity analysis\n!  * LAEV1M  : .F. 10 meter wind analysis","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/#Blacklisting-of-surface-observation","page":"Surface Analysis","title":"Blacklisting of surface observation","text":"","category":"section"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"It is possible, in HARMONIE data assimilation, to blacklist data from specific sites. Following example illustrate blacklisting of one automatic (type 24) ship measurement with code name DBKR starting from a certain date March 6, 2012,","category":"page"},{"location":"DataAssimilation/Surface/SurfaceAnalysis/","page":"Surface Analysis","title":"Surface Analysis","text":"   cd ~/hm_home/$exp\n   Harmonie co LISTE_NOIRE_DIAP  # check out blacklist from the repository, e.g., source:Harmonie/nam/LISTE_NOIRE_DIAP\n   (edit then nam/LISTE_NOIRE_DIAP to insert, e.g. at the last line, following\n\n    1 SHIP        24  11 DBKR     03062012\n","category":"page"},{"location":"Build/General/#General-software-requirements","page":"Software requirements","title":"General software requirements","text":"","category":"section"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"This page outlines, in a general way, the software requirements for compiling HARMONIE on a non-ECMWF platform","category":"page"},{"location":"Build/General/#Download-HARMONIE","page":"Software requirements","title":"Download HARMONIE","text":"","category":"section"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"To obtain the HARMONIE source, assuming the computer platform has a git client available, one may check out from the repository at the host hirlam.org:","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"git clone git@github.com:Hirlam/Harmonie.git\ncd Harmonie\ngit checkout release-43h2.beta.3 # For the latest tagged version\ngit checkout develop             # For the development branch","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"At ECMWF, the \"checked-out\" versions are available on ecgb:","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"ecgb:/home/ms/spsehlam/hlam/harmonie_release/git/tags\necgb:/home/ms/spsehlam/hlam/harmonie_release/develop","category":"page"},{"location":"Build/General/#Compilers-and-standard-software","page":"Software requirements","title":"Compilers and standard software","text":"","category":"section"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"The system requires the following standard unix/linux software ","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"A fortran compiler\nA C compiler\nflex & bison for lex & yacc\nksh and bash\nperl\npython","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"Read more about the tested compilers under installation.","category":"page"},{"location":"Build/General/#MPI-and-OpenMP","page":"Software requirements","title":"MPI and OpenMP","text":"","category":"section"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"Harmonie supports parallelization through message passing or shared memory multiprocessing. ","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"mpi libraries such as mpich2, openmpi or similar.\nOpenMP libraries","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"The system can be compiled without support for MPI, but not all parts of the system can be run without MPI. The forecast model should however work fine without MPI.","category":"page"},{"location":"Build/General/#BLAS-and-LAPACK-libraries","page":"Software requirements","title":"BLAS and LAPACK libraries","text":"","category":"section"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"You need BLAS and LAPACK-lite libraries.","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"If they are already on your system, verify they have been made with the correct compiler, or rebuild them. Instructions on how to (re-)build BLAS and LAPACK follow below:","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"Download BLAS and LAPACK\nFirst build BLAS (untarring blas.tgz places it in the BLAS directory). Go to that directory, and edit make.inc to set the compiler and linker to gfortran. Then type 'make'.\nSubsequenty, for LAPACK, after untarring lapack-lite-3.1.1.tgz, go to the lapack-lite-3.1.1 directory.\nCopy make.inc.example to make.inc.\nEdit make.inc to point to the proper compiler/loader (gfortran) and to put the variable PLAT to the empty string. Set TIMER to INT_ETIME.\nCopy the blas.a library from the BLAS directory to the lapack-lite-3.1.1 directory, run ranlib on it, then type 'make'.\nThen copy the libraries in /usr/local/lib with names libblas.a and liblapack.a, respectively, otherwise the default configuration will not find them. Run ranlib on them.","category":"page"},{"location":"Build/General/#NETCDF","page":"Software requirements","title":"NETCDF","text":"","category":"section"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"Netcdf is required for some routines. Make sure you have the development version installed on your system.","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"http://www.unidata.ucar.edu/software/netcdf","category":"page"},{"location":"Build/General/#ecCodes","page":"Software requirements","title":"ecCodes","text":"","category":"section"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"ecCodes is used to access GRIB1/GRIB2","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"https://confluence.ecmwf.int/display/ECC ","category":"page"},{"location":"Build/General/#GRIB,-BUFR-and-auxiliary-software","page":"Software requirements","title":"GRIB, BUFR and auxiliary software","text":"","category":"section"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"The old software for reading GRIB1 and BUFR is included in the HARMONIE system and comprises","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"bufr 000405\ngribex 000370","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"In addition there are some extra support libraries.","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"dummies_006\nrgb_001","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"Makeup build these libraries for you. ","category":"page"},{"location":"Build/General/#GMTED-processing","page":"Software requirements","title":"GMTED processing","text":"","category":"section"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"To process the GMTED tiff files gdal and python modules for gdal is required.","category":"page"},{"location":"Build/General/#Observation-monitoring","page":"Software requirements","title":"Observation monitoring","text":"","category":"section"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"To be able to extract observation feedback information sqlite3 is required.","category":"page"},{"location":"Build/General/","page":"Software requirements","title":"Software requirements","text":"","category":"page"},{"location":"DataAssimilation/DataAssimilation4DVAR/#Harmonie-4D-Var-(under-development)","page":"4DVAR","title":"Harmonie 4D-Var (under development)","text":"","category":"section"},{"location":"DataAssimilation/DataAssimilation4DVAR/#General","page":"4DVAR","title":"General","text":"","category":"section"},{"location":"DataAssimilation/DataAssimilation4DVAR/","page":"4DVAR","title":"4DVAR","text":"An option for doing assimilation with 4D-Var has been included in the HARMONIE mini-SMS system (in branch harmonie-35h1, which is called 35h1branch at ecgb). The 4D-Var tasks are invoked only if ANAATMO is set to 4DVAR in config_exp.h. It implies that a number of extra steps are carried out in the !MakeCycleInput loop of mini-SMS. Climate fields and lateral boundaries at low resolution are genenerated, in addition to the corresponding  fields at high resolution. These fields are needed in the 4D-Var tasks under the Date loop of Harmonie. Also the generation of observations  in ODB through BATOR is slightly modified in case of 4D-Var. The 4D-Var tasks carry out the data assimilation. At present it is working only for one outer loop and 6 hour centered data assimilation time window. It has been tested only on domain SCANDINAVIA and at the ECMWF computer platform. 4DVAR consists of the following steps (rather closely following the ARPEGE 4D-Var structure):","category":"page"},{"location":"DataAssimilation/DataAssimilation4DVAR/","page":"4DVAR","title":"4DVAR","text":"4DVprolog - converting high res. first guess to low res.\n4DVscreen - 4D-Var screening of observations\nfph2l - (not used presently, but when multiple outer loops)\n4DVminim - 4D-Var minimization at low res producing low res analysis (at the beginning of assimilation time window)\nfpl2h_fg - converting low res. first guess to high res. using FULLPOS\nblend_an - Merge surface fields from low res. first guess to low res. analysis by applying BLENDSUR\nfpl2h_an - converting low res. analysis (updated with first guess surface fields) to high res. analysis with FULLPOS\n4DVtraj - trajectory run propagating analysis 3 hours forward in time to the center of the assimilation time window ","category":"page"},{"location":"DataAssimilation/DataAssimilation4DVAR/","page":"4DVAR","title":"4DVAR","text":"As a first step, we have chosen to apply the CANARI surface data assimilation after the trajectory run has been carried out (with 3D-Var the CANARI surface data assimilation is carried out before the minimization). The 4D-Var analysis at the center of the assimilation time window is used as first guess for CANARI and the output of CANARI is used as initial state for the forecast model.","category":"page"},{"location":"DataAssimilation/DataAssimilation4DVAR/","page":"4DVAR","title":"4DVAR","text":"Note that if the HARMONIE mini-SMS system is run with 4D-Var, for the first assimilation time cycle only a forecast is carried out, no data assimilation. The reason is that 4D-Var requires a first guess from the previous assimilation time cycle and we have chosen not to use an interpolated lateral boundary condition file as first guess for the first assimilation cycle. ","category":"page"},{"location":"DataAssimilation/DataAssimilation4DVAR/#Try-to-run-HARMONIE-4D-Var-on-ecgb/cca?","page":"4DVAR","title":"Try to run HARMONIE 4D-Var on ecgb/cca?","text":"","category":"section"},{"location":"DataAssimilation/DataAssimilation4DVAR/","page":"4DVAR","title":"4DVAR","text":"Login to ecgb and create your experiment har4d under $HOME/hm_home and thereafter go to $HOME/hm_home/har4d\nPlace in HOME/hmhome/har4d setup your experiment by typing: `~nhz/Harmonie setup -r ~nhz/harmonierelease/35h1branch`\nDo some modifications in $HOME/hm_home/har4d/ecf/config_exp.h : ANAATMO=4DVAR, BUILD_ROOTPACK=${BUILD_ROOTPACK-yes},DFI=\"fdfi\"\nGo to directory HOME/hm_home/har4d and start 12 h forecast by typing: ~nhz/Harmonie start DTG=2009021012 LL=12\nWhen the forecast has finnished, start 4dvar by typing: ~nhz/Harmonie prod LL=12","category":"page"},{"location":"Observations/Oulan/#OBSOUL-creation:-Oulan","page":"Oulan","title":"OBSOUL creation: Oulan","text":"","category":"section"},{"location":"Observations/Oulan/#General-Description","page":"Oulan","title":"General Description","text":"","category":"section"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"The pre-processing step creates ODB (Observational Data Base) from various observation data files possibly in different formats.","category":"page"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"Software: The programs used for pre-processing (Shufflebufr, oulan and BATOR) are not part of the IFS code. oulan is software developed at Météo France to extract observations from their local database (BDM). The output of oulan (OBSOUL) is one of the inputs of BATOR. BATOR is also software developed at Météo France to generate the ODB (Observational !DataBase) database for the ARPEGE/ALADIN/HARMONIE analysis system. ODB is a tailor made database software developed at ECMWF to manage very large observational data volumes assimilated in the IFS 4DVAR system, and to enable flexible post-processing of this data (Sami Saarinen, 2006). We use oulan to generate an OBSOUL file from different BUFR files (note you can easily change the oulan program to handle data in different format than BUFR. For example in OPLACE data processing some files are in netCDF format). OBSOUL file is an ASCII formatted file, the content of which is similar to that of the CMA (Central Memory Array, packing format actually in use in the HIRLAM data assimilation system). Our version of ouland is placed under “util” directory in the repository. HARMONIE BATOR originates from the MF export-pack. The figure bellow describes the mechanism of the observation pre-processing in HARMONIE DA. To sum it up, !ShuffleBufr splits different observations into BUFR files, then oulan creates the OBSOUL file, and BATOR creates the ODB file using satellite BUFR/GRIB/BIN files and the OBSOUL one.\nCompilation: oulan, Shufflebufr are compiled using gmkpack or makeup.\nScripts: Oulan\nInput/output\noulan  input: BUFR files; output: the OBSOUL file in ASCII format","category":"page"},{"location":"Observations/Oulan/#ShuffleBufr","page":"Oulan","title":"ShuffleBufr","text":"","category":"section"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"ShuffleBufr splits different observations into separate BUFR files according the IFS observation type/sub-type definition. Some of them (essentially those of conventional observations) are then fed to OULAN; the others go directly into BATOR.","category":"page"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"    PROGRAM SHUFFLEBUFR\n    Split and shuffle BUFR file into  specific BUFR files for OULAN\n\n    Usage: SHUFFLEBUFR -i <bufr_file> [-s1|-s2|-s3]  [-a] [-r]\n\n           -s1 : Synop ship will be extracted in <synop>\n           -s2 : Synop ship will be extracted in <buoy>\n           -s3 : Synop ship will be extracted in <ship>\n\n           Nota Bene: If -s1,-s2 or -s3 are not specified\n                      synop_ship will not be extracted\n\n           -a  : Extracts ATOVS in files amsua and amsub\n\n           -r  : Extracts also record messages (synop)","category":"page"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"The splitting is done with the following command (BUFRFILE is a file containing all observations) in the Oulan script:","category":"page"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"      $BINDIR/ShuffleBufr -i ${BUFRFILE} -s3 -a","category":"page"},{"location":"Observations/Oulan/#oulan","page":"Oulan","title":"oulan","text":"","category":"section"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"oulan reads (primarily conventional observation) BUFR data and converts them into ASCII format OBSOUL files. Note, we can make observation selection in oulan. More details about how to do data selection can be found here (Randriamampianina, ALADIN/HIRLAM Workshop 2005)","category":"page"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"namelist description:\nNADIRS \nANZA=90. If LZONEA is true then only extract observations south of 90N\nALASZA=0. If LZONEA is true then only extract observations north of 0\nALOEZA=-180. If LZONEA is true then only extract observations west of 180W\nALOOZA=180. If LZONEA is true then only extract observations west of 180E\nLNEWSYNOPBUFR=.FALSE. Process new format BUFR SYNOP experimental\nLNEWSHIPBUFR=.FALSE. Process new format BUFR SHIP experimental\nLNEWBUOYBUFR=.FALSE. Process new format BUFR BUOY experimental\nLNEWTEMPBUFR=.FALSE. Process new format BUFR TEMP experimental\nLACAR=.TRUE. Process ACARS BUFR data\nLAIREP=.TRUE. Process AIREP BUFR data\nLAMDAR=.TRUE. Process AMDAR BUFR data\nLBUOY=.TRUE. Process BUOY BUFR data\nLEUROPROFIL=.FALSE. Process European Profiler BUFR data\nLPILOT=.TRUE. Process PILOT BUFR data\nLRH2Q=.FALSE. Extract 2m RH from SYNOP, BUOY and TEMP BUFR data\nLSHIP=.TRUE. Process SHIP BUFR data\nLSYNOP=.TRUE. Process SYNOP BUFR data\nLTEMP=.TRUE. Process TEMP BUFR data\nLTEMPDROP=.TRUE. Process DROPTEMP BUFR data\nLTEMPSHIP=.TRUE. Process TEMPSHIP BUFR data\nLTOVSAMSUA=.FALSE. Process AMSUA data\nLTOVSAMSUB=.FALSE. Process AMSUB data\nLTOVSHIRS=.FALSE. Process HIRS data\nLZONEA=.TRUE. Switch to extract data in defined lat-lon domain (N,S,E,W)\nNDATE=DDATE OBSOUL Date\nNDIFFM1=30 Define analysis window (T-NDIFFM1)\nNDIFFM2=300 Define analysis window (T-NDIFFM2)\nNDIFFP1=30 Define analysis window (T+NDIFFP1)\nNDIFFP2=259 Define analysis window (T+NDIFFP2)\nNINIT=0 flag used by oulan to prevent writing if problem is found\nNRESO=HHOUR OBSOUL Hour\n**NANBOB* Namelist to define number of observations to be extracted\nNBACAR=750000 Number of ACAR obs\nNBAIREP=750000 Number of AIREP obs\nNBAMDAR=750000 Number of AMDAR obs\nNBBUOY=  4000 Number of BUOY obs\nNBEUROPROFIL= 15000 Number of European profiler obs\nNBPILOT=  2000 Number of PILOT obs\nNBSHIP= 30000 Number of SHIP obs\nNBSYNOP= 60000 Number of SYNOP obs\nNBTEMP=  2000 Number of Land TEMP obs (since r14078)\nNBTEMPDROP=  1000 Number of DROPTEMP obs (since r14078)\nNBTEMPSHIP=  1000 Number of Ship TEMP obs (since r14078)\nNBTOVSAMSUA= 80000 Number of AMSUA obs\nNBTOVSAMSUB= 80000 Number of AMSUB obs\nNBTOVSHIRS=  8000 Number of HIRS obs\nmake a namelist\n  NAMELIST=$WRK/$WDIR/namelist_oulan\n  Get_namelist oulan $NAMELIST\n  sed -e \"s/DDATE/$SDATE/\" \\\n      -e \"s/HHOUR/$SHOUR/\"\n      -e \"s/SALOOZA/$OULWEST/\"  \\\n      -e \"s/SALANZA/$OULNORTH/\"  \\\n      -e \"s/SALOEZA/$OULEAST/\"  \\\n      -e \"s/SALASZA/$OULSOUTH/\" \\\n      -e \"s/SLNEWSYNOPBUFR/$SLNEWSYNOPBUFR/\" \\\n      -e \"s/SLNEWSHIPBUFR/$SLNEWSHIPBUFR/\" \\\n      -e \"s/SLNEWBUOYBUFR/$SLNEWBUOYBUFR/\" \\\n      -e \"s/SLNEWTEMPBUFR/$SLNEWTEMPBUFR/\" \\\n      ${NAMELIST} >NAMELIST\nrun oulan\n$BINDIR/oulan\nprocess GNSS data. If $GNSS_OBS is set to 1 then GNSS observations are added to the OBSOUL file and whitelisting is carried out using PREGPSSOL","category":"page"},{"location":"Observations/Oulan/#New-BUFR-templates","page":"Oulan","title":"New BUFR templates","text":"","category":"section"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"Valid for HARMONIE 40h1 and later","category":"page"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"The use of new format (GTS WMO) BUFR is controlled in scr/include.ass by LNEWSYNOPBUFR, LNEWSHIPBUFR, LNEWBUOYBUFR, LNEWTEMPBUFR (set to 0 or 1). These environment variables control namelist settings in the Oulan script. GTS and ECMWF BUFR were used to guide the code changes so Oulan assumes either \"flavour\" of BUFR. Local changes may be required if your locally produced BUFR, in particular section 1 data sub-type settings, do not follow WMO and/or ECMWF practices.","category":"page"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"The ECMWF wiki contains updates regarding the quality of the new BUFR HR observations. See the following ECMWF wiki pages for furher information:","category":"page"},{"location":"Observations/Oulan/","page":"Oulan","title":"Oulan","text":"https://software.ecmwf.int/wiki/display/TCBUF/TAC+To+BUFR+Migration\nhttps://software.ecmwf.int/wiki/display/TCBUF/Statistics+of+High+resolution+BUFR+TEMP","category":"page"},{"location":"System/TheHarmonieScript/#The-Harmonie-main-script","page":"The Harmonie script","title":"The Harmonie main script","text":"","category":"section"},{"location":"System/TheHarmonieScript/","page":"The Harmonie script","title":"The Harmonie script","text":"The Harmonie script is the main user interface to the harmonie system. It is used to setup, start, check and control your experiment and environment. Below follows the most useful commands. There are other commands inherited from the HIRLAM environment that may or may not work. For a full list check scr/Start, scr/Actions, scr/Actions.pl.","category":"page"},{"location":"System/TheHarmonieScript/","page":"The Harmonie script","title":"The Harmonie script","text":"Harmonie setup [ -r REVISION] [ -h HOST] [ -d DOMAIN] [ -c CONFIGURATION] [ -l LEVELS] where:\nREVISION is the path to the version of harmonie you are working with.\nHOST is the name of the host you are working on. There should exist corresponding config-sh/config.HOST. \nCONFIGURATION is one of the predefined configurations in scr/Harmonie_testbed.pl. It a fast way to setup your favourite configuration.\nDOMAIN is one of the predefined domains in ecf/config_exp.h \nLEVELS is one of the predefined level definitions in scr/Vertical_levels.pl\nHarmonie start DTG=YYYYMMDDHH [ DTGEND=YYYYMMDDHH] [ optional environment variables] launches a cold start run.\nDTG is the initial time of your experiment\nSeveral other optional variables can be given like\nPLAYFILE=FILENAME use a different ecflow suite definition file. Default is harmonie.tdf\nBUILD=yes|no to turn on and off compilation\nCREATE_CLIMATE=yes|no to turn on and off generation of climate files\nAny environment variable that you would like to send to the system.\nHarmonie prod will continue from the DTG given in your progress.log file. The rest of the arguments is as for Harmonie start. This should be used to continue and experiment. It is assumed that a first guess file is available and the run will fail if this is not found.    \nHarmonie mon will restart your ecflow_ui window and try to connect to an existing ecflow server.\nHarmonie co [FILE|PATH/FILE] will copy the request file from the version chosen in your setup ( as pointed out in the config-sh/hm_rev file ) to your local directory. If the PATH is not given a search will be done. If the name matches several files you will be given a list to choose from.\nHarmonie install will build your libraries and binaries but not start any experiment\nHarmonie testbed will launch the Harmonie testbed\nHarmonie diff [--xxdiff] will look for differences between the revision in config-sh/hm_rev and HM_LIB.\nHarmonie CleanUp -ALL -go will clean the following directories: HM_DATA, HM_LIB, HM_EXP. Instructions from src/Actions.pl:","category":"page"},{"location":"System/TheHarmonieScript/","page":"The Harmonie script","title":"The Harmonie script","text":"# args: if -go: remove, (default is to list but not remove the matching files)\n#       if -k*: do not do the long term archive HM_EXP - so keep it\n#       if -d*: combination of -k and -ALL (-d* means: disks)\n#       if -ALL: treat all files and also (if -go) remove the directories\n#       a pattern is usually a string without meta-characters. To this\n#       a * is appended (so e.g. ob will affect all files ob*); this can\n#       be inhibited by appending ~ (so ob~! translates to ob).\n#       Also, files in all subdirectories P*_* will be affected\n#       where P is the pattern [0-9][0-9], This resembles\n#       a `CYCLEDIR'. So ob will result in 'ob* P*_*/ob*'.\n#       The pattern P*_* will be prepended to every / in the pattern,\n#       unless the / is preceded by ~ (which will be removed).\n#       Hence, to remove e.g. all analyses from 1995, use 1995/an,\n#       which translates to 1995[0-9][0-9]*_*/an*\n#       (to be precise: use: CleanUp(\"REMOVE:1995/an\", \"-go\");","category":"page"},{"location":"PostProcessing/Interpolation/#Interpolations-with-gl","page":"Interpolation GL","title":"Interpolations with gl","text":"","category":"section"},{"location":"PostProcessing/Interpolation/#Introduction","page":"Interpolation GL","title":"Introduction","text":"","category":"section"},{"location":"PostProcessing/Interpolation/","page":"Interpolation GL","title":"Interpolation GL","text":"In the following we describe the geometrical routines in gl. gl can handle the following projections","category":"page"},{"location":"PostProcessing/Interpolation/","page":"Interpolation GL","title":"Interpolation GL","text":"lat/lon\nRotated lat/lon\nLambert\nPolar stereographic\nRotated Mercator","category":"page"},{"location":"PostProcessing/Interpolation/#Interpolation","page":"Interpolation GL","title":"Interpolation","text":"","category":"section"},{"location":"PostProcessing/Interpolation/","page":"Interpolation GL","title":"Interpolation GL","text":"All interpolations are handled within the module util/gl/mod/module_interpol.f90. The module contains ","category":"page"},{"location":"PostProcessing/Interpolation/","page":"Interpolation GL","title":"Interpolation GL","text":"clear_interpol to clear the interpolation setup\nsetup_interpol where the position of the output gridpoints in the input grid are calculated\nsetup_weights where we calculate the interpolation weights. Interpolation can be nearest gridpoint or bilinear. The interpolation can be masked with a field that tells which gridpoints from the input fields that can be used. ","category":"page"},{"location":"PostProcessing/Interpolation/","page":"Interpolation GL","title":"Interpolation GL","text":"The setup routines are only called once.","category":"page"},{"location":"PostProcessing/Interpolation/","page":"Interpolation GL","title":"Interpolation GL","text":"interpolate runs the interpolation\nresample works like the interpolation if the input grid is coarser than the output grid. If reversed it takes the averages of the input gridpoints belonging to each output gridpoit.","category":"page"},{"location":"PostProcessing/Interpolation/","page":"Interpolation GL","title":"Interpolation GL","text":"Interpolation can be done between different projections as wall as to geographical points. The most general example on the usage of the interpolatin can be found in util/gl/grb/any2any.F90.","category":"page"},{"location":"PostProcessing/Interpolation/","page":"Interpolation GL","title":"Interpolation GL","text":"For practical usage see the section about postprocessing","category":"page"},{"location":"PostProcessing/Interpolation/#Rotations","page":"Interpolation GL","title":"Rotations","text":"","category":"section"},{"location":"PostProcessing/Interpolation/","page":"Interpolation GL","title":"Interpolation GL","text":"All rotations are handled within the module util/gl/mod/module_rotations.f90. The module contains ","category":"page"},{"location":"PostProcessing/Interpolation/","page":"Interpolation GL","title":"Interpolation GL","text":"clear_rotation to clear the rotation setup\nprepare_rotation prepare rotations from input geometry to output geometry via north south components.\nrotate_winds runs the actual rotation.","category":"page"},{"location":"PostProcessing/Interpolation/#Staggering","page":"Interpolation GL","title":"Staggering","text":"","category":"section"},{"location":"PostProcessing/Interpolation/","page":"Interpolation GL","title":"Interpolation GL","text":"The staggering of an input file is based on the knowledge about the model and is in util/gl/mod/module_griblist.f90.  The restaggering is done in util/gl/grb/restag.f90 as a simple average between gridpoints. The staggering of the output geomtery is defined by OUTGEO@ARKAWA, where A and C are available options.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#How-to-Introduce-New-High-Resolution-Topography-into-Harmonie","page":"HiRes Topography","title":"How to Introduce New High-Resolution Topography into Harmonie","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#Introduction","page":"HiRes Topography","title":"Introduction","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"This page describes how to set up and use an ultra-high resolution topographic data set for your Harmonie domain, instead of the current standard GTOPO30 data set.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The data replacing GTOPO30 is likely to be much denser (by a factor of 100 or more), so it probably doesn’t make much sense for each centre to store a complete quasi-global set.  It is much more practical for each centre to generate and store a local sub-set of the high-resolution topography to encompass just their own computational domains. First the principal process is described and in [#DoitallinsideHARMONIE here] the streamlined implementation, with the coarser GMTED2010 data, in the system is summarized.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#Background","page":"HiRes Topography","title":"Background","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The standard topographic data set currently used by Harmonie is the global “GTOP030” set from NASA.   This is a “Digital Elevation Model”  (DEM) with a horizontal resolution of 30 arc seconds (approx. 1km).   As Harmonie model configurations start to use grid-sizes of 1km or smaller, the computational grid can have finer resolution than the topographic grid, and so topography becomes a new limiting factor in the full model resolution.  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"It is possible now to overcome this limitation of the relatively coarse GTOPO30 topography by replacing it with a much finer-scale DEM.  One such DEM representation of the earth’s surface has been available since Oct. 2011.  This is version 2 of the DEM derived from the Aster instrument on board the Terra satellite, as part of the collaboration between Japan’s Ministry of Economy, Trade and Industry (METI), and NASA in the U.S.  In the Aster dataset, surface elevations are reported at a horizontal resolution of approx. 30m. Thus the Aster data is about 900 times denser than GTOPO30, i.e., has about 30 times higher resolution in each horizontal dimension.  The average error in the vertical elevation estimates is approx. 6-10m (I think of this as the typical height of a tree or a house – the kind of things that can confuse the satellite radiometer into reporting a false surface elevation).  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"More information about the Aster DEM is available here .  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"Even for “relatively” coarse Harmonie grids (perhaps even 2.5km meshes), the “slope”, “roughness”, “silhouette” and other physical attributes of the topography used by Harmonie can be provided much more accurately by Aster data than by GTOPO30.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#Obtaining-ASTER-high-resolution-DEM-data","page":"HiRes Topography","title":"Obtaining ASTER high-resolution DEM data","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The data are publicly and freely available from NASA’s “reverb” web-site.   Since the resolution is so fine, the complete dataset is quite voluminous: the compressed file for each 1^o x 1^o (longitude-latitude) tile or “granule” at 50^o N is about 15MB if totally land-covered.  To obtain the data you want:","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"Draw a bounding rectangle around your domain of interest (even all of Europe!) at that “reverb” web-site;\nSelect“ASTER Global Digital Elevation Model V0002” from the list of data sets further down the same web-page;\nClick the “Search for Granules” box at the bottom of the page.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"From here you will be brought through a standard registration process familiar to anyone who has ever bought a train ticket online – the main difference being that the Aster data are free.   Once registration is complete, you should receive an email after 1-2 days telling you that your data is ready, and how you can ftp it to your own computer.  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"I obtained 97 “granules” of such data (i.e., DEM files for an area approximately 1^o^ square), covering the islands of Ireland and the UK (14^o longitude x 11^o latitude).  The 57 “missing” granules are simply ocean regions that encompass no land at all.  No granules are provided for any 1-degree tile that is completely over the open ocean, since sea-level elevation is assumed to be zero.   Each granule is a separate zip file, varying in size up to about 15MB, depending on the fraction of land area in each 1-degree tile and the complexity of the topography itself.  Granules are identified by the latitude and longitude of the southwest (lower-left) corner (or more precisely, of the geometric centre of the southwest corner pixel), which is given in the file-name.  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#Processing-the-Raw-Aster-Data","page":"HiRes Topography","title":"Processing the Raw Aster Data","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"Each “granule” of data is a zip file that unzips to 2 “tiff” files (ASTGM2_*_dem.tif, containing the actual data, and ASTGM2_*_num.tif, containing “quality assessment”), along with a generic README.pdf.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The Quality Assessment files (*num.tif) contains the number of “stereo scene pairs” used to determine elevation at each pixel (if positive) or the source of non-ASTER elevation data used to  replace bad ASTER DEM (if negative).  Values less than 5 are associated with relatively larger errors in the elevation measurement.  Larger values are associated with more accurate final estimates of surface elevation.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"This information can help to identify those regions where the Aster values may need to be merged or replaced with elevation data from some other source.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"For the purposes of formatting the DEM data for Harmonie, the first step is to extract the (longitude, latitude, elevation) triplet for each “pixel” (or  “grid-point”) from the ASTGM2_*_dem.tif  files.  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"This can be done using software such as the Geospatial Data Abstraction Library (GDAL) open-source tools, available here.  Once installed, these can be used to read, merge, and otherwise manipulate geoTIFF files.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The command I used to generate a simple text file containing longitude, latitude and elevation from an ASTGM2_*_dem.tif file is, e.g.,","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"gdal_translate –of XYZ ASTGM2_N59W006_dem.tif ASTGM2_N59W006_xyz.txt","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"This fills the ascii output file  ASTGTM2_N59W006_xyz.txt with rows of (longitude, latitude, elevation) values, starting from the northwest (top-left) corner (-6, 60), and proceeding eastwards to (-5,60), then moving south to the next row, ending up at the southeast (bottom-right) corner (-5,59).  Each output text file contains 3601 x 3601 data-points, i.e., 12,967,201 rows, and uses over 550 MB of storage.  Note that the various granules overlap each other at the boundaries (the line of boundary data is included in each bounding file).  Elevation is in units of metres.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"This is relatively straightforward and only needs to be done once for each file,  even if there may be more efficient ways to do it.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The command above can easily be scripted to process the full collection of *dem.tif files:","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"for i in $( ls ASTGTM2_*_dem.tif ); do\n  gdal_translate -of XYZ $i $i.xyz\ndone","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"To obtain some information about any particular tif file, use the gdalinfo command.  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"Next, the data from each separate 1^o x 1^o text file were combined into a single flat unformatted little-endian binary file consisting of surface elevation every 30m or so encompassing the entire Harmonie domain.  For an Ireland/UK domain, bounded between latitudes 49 and 60 deg. N, and between longitudes 11 deg. W and 3 deg. E, this contains approx. 40,000 (latitude) x 50,000 (longitude) data points – or about 2 billion points altogether.  (2 billion points stored in 2-byte integer format use about 4GB of storage).  A domain like this can easily be extended to the west and north, where there is no land and where sea-level elevation is simply zero.  In order to extend it to the south or east, however, where there is land, extra Aster granules would be required.  See the attached standalone program (or [wiki:hirestopoggather_tiles.f]) for the details of what I did (crude but effective; nothing fancy and probably not the best way, but simple and it works).  The file is written with the first element at the northwest corner, progressing eastwards, then south, with the last element at the southeast corner.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#Errors-and-Data-Gaps","page":"HiRes Topography","title":"Errors and Data Gaps","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"I did not notice any gaps or \"bad data\" in the Aster topography over Ireland and the UK, but there do seem to be some gaps, negative pixels or positive \"spikes\" elsewhere, esp. over Scandanavia and other high latitudes.  The \"quality assessment\" numbers in the ASTGM2_*_num.tiffiles mentioned above can help to identify bad or dubious elevation values.  Nicolas Bauer (FMI) has a procedure for detecting and correcting these.  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#Formatting-topographic-data-for-use-by-Harmonie","page":"HiRes Topography","title":"Formatting topographic data for use by Harmonie","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The main topographic data files used by Harmonie are in $HM_CLDATA/PGD, and are called gtopo30.hdr and gtopo30.dir .  In principle, all that is required now is to replace these 2 files (containing GTOPO30 data) with equivalent files containing Aster data.  There is no need to change the file names or to edit any Harmonie source code – just create new files with the old names, and containing Aster instead of GTOPO30 data.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The gtopo30.hdr file is the “header” file, containing meta-data about the main data file (gtopo30.dir).   The header file contains all the information needed by the MASTERODB executable to read in the real data from the gtopo30.dir file, and to use it appropriately within the rest of Harmonie.   The original gtopo30.hdr file contains:","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"GTOPO30 orography model, rewritten by V. Masson, CNRM, Meteo-France, 16/07/98\nnodata: -9999\nnorth: 90.\nsouth: -90.\nwest: -180.\neast: 180.\nrows: 21600\ncols: 43200\nrecordtype: integer 16 bytes","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"Note that this file specifies a global domain.  To use Aster data for the Ireland/UK domain, this content ended up being replaced with:","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"ASTER orography model, starting UL\nnodata: -9999\nnorth: 60\nsouth: 49\nwest: -11\neast: 3\nrows: 39601\ncols: 50401\nrecordtype: integer 16 bits","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"While the number of rows and columns is about the same as before, the topographic domain is much smaller.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"In the Harmonie source, the key files that are involved in reading and parsing the gtopo30.hdr and gtopo30.dir files are surfex/pgd/zoom_pgd_orography.F90 and surfex/aux/read_direct.F90.  Parsing these files reveals  what is needed to write gtopo30.hdr and gtopo30.dir files containing Aster data.  I wrote another simple program to do this, based largely on programs already written by Nicolas Bauer and Imanol Guerrero, and provided by Laura Rontu.   This program is available as attachement, or at:  [wiki:hires_convert.f] .","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The part of this file that writes the header data is:","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"!    Header file for harmonie:\n       open(unit=23,file=\"hires_irluk_hm.hdr\",form='formatted',\n     &      status='new',IOSTAT=ios)\n            write(23,FMT='(A)') \"GTOPO30 orography model, starting UL\"\n            write(23,FMT='(A)') \"nodata: -9999\"\n                  write(caux,*) north\n            write(23,FMT='(A)') \"north: \"//adjustl(trim(caux))\n                  write(caux,*) south\n            write(23,FMT='(A)') \"south: \"//adjustl(trim(caux))\n                  write(caux,*) west\n            write(23,FMT='(A)') \"west: \"//adjustl(trim(caux))\n                  write(caux,*) east\n            write(23,FMT='(A)') \"east: \"//adjustl(trim(caux))\n                  write(caux,*) npts_ns\n            write(23,FMT='(A)') \"rows: \"//adjustl(trim(caux))\n                  write(caux,*) npts_ew\n            write(23,FMT='(A)') \"cols: \"//adjustl(trim(caux))\n            write(23,FMT='(A)') \"recordtype: integer 16 bits\"","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The character variable “caux” is used here as an “internal” file to hold the various domain parameters corresponding to the Aster data.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The main DEM data file is written in integer*2 format to the new gtopo30.dir file, as in:","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"      integer*2, allocatable :: elev_hm(:,:)\n      integer*2                   ::  idata\n      character*2                :: cdata, ctmp\n      equivalence (idata,cdata)\n...   \n      allocate (elev_hm(npts_ew,npts_ns))\n!  Start of main (outer) loop:\n       do j=1,npts_ns\n          read(21) elevation\n          elev_hm(:,j) = elevation(:)\n       enddo\n          close(21)\n          deallocate(elevation)\n\n!  So now elev_hm should be filled.\n\n!  Byte-swap from little to big-endian (for sake of Harmonie build...):\n       do j=1,npts_ns\n          do i=1,npts_ew\n              idata = elev_hm(i,j)\n              ctmp(1:1) = cdata(1:1)\n              cdata(1:1) = cdata(2:2)\n              cdata(2:2) = ctmp(1:1)\n              elev_hm(i,j) = idata\n          enddo\n       enddo\n\n       open(unit=24,file=\"gtopo30.dir\",form=\"unformatted\",\n     &      access=\"direct\",recl=npts_ns*npts_ew*2,status=\"new\",err=999)\n\n!  Write starting from NW corner, working east, then south one row:\n        write(24,rec=1) ((elev_hm(i,j),i=1,npts_ew), j=1,npts_ns)","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"Note the explicit conversion to big-endian (so no -convert big-endian compiler flag should be used with this).   ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"Note too that when using Intel compilers, the -assume byterecl option must be used so that record lengths are in bytes instead of (4-byte) words.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"Once those simple programs are run to perform straightforward file-format conversions, the resulting gtopo30.hdr and gtopo30.dir files (containing Aster data) can be used directly in $CLDATA/PGD in place of the original “real” ones (containing GTOPO30 data).  Those files are read by Harmonie during the “Climate” phase, and generate reference “climate” files PGD.lfi and PGD.fa.  These remain constant and are not generated again for each Harmonie installation.  They are read in by the surfex module during each Forecast phase, and used wherever surface topographic data is needed.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#Do-it-all-inside-HARMONIE","page":"HiRes Topography","title":"Do it all inside HARMONIE","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"We discovered that there are a lot of \"holes\" in the ASTER data over Scandinavia, so instead we focused on the GMTED2010 data set. This almost global dataset is compiled of different high resolution DEM sources and the highest available resolution is 7.5 arc seconds (~250 meters). It is used by Meteo-France for their 1.3 km domain. This data also comes in tiles of big tif-files like the ASTER data so a similar method described above can be used on this dataset. The different tiles cover an area that is 30 by 20 degrees longitude/latitude. There are no valid data north of 84 degrees north and south of 56 degrees south. The two steps described above have been streamlined in harmonie-40h1 and the main tasks have been gathered in Preparegmted which is called from Preparepgd. What's required from the user is to download (http://topotools.cr.usgs.gov/gmtedviewer/viewer.htm) the GMTED2010 data and locate them in the appropriate location and point the following variable in Envsystem to the location:","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"   export GMTED2010_INPUT_PATH=/project/hirlam/harmonie/climate/GMTED2010","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"To use the GMTED2010 data and not GTOPO30 we have to define the input source, once again in ecf/config_exp.h .","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"   TOPO_SOURCE=gmted2010","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The Prepare_gmted script checks for the chosen domain size and selects the necessary tiles and then combines them together into one geotiff file under $CLIMDIR. Small python script tif2bin.py then converts the new geotiff to the binary (dir/hdr) format needed for PGD generation.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"On cca (ECMWF) the following input files are available under /project/hirlam/harmonie/climate/GMTED2010:","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":" 10N030W_20101117_gmted_mea075.tif\n 30N030E_20101117_gmted_mea075.tif\n 30N000E_20101117_gmted_mea075.tif\n 30N030W_20101117_gmted_mea075.tif\n 30N060W_20101117_gmted_mea075.tif\n 30N090W_20101117_gmted_mea075.tif\n 50N030E_20101117_gmted_mea075.tif\n 50N000E_20101117_gmted_mea075.tif\n 50N030W_20101117_gmted_mea075.tif\n 50N060W_20101117_gmted_mea075.tif\n 50N090W_20101117_gmted_mea075.tif\n 70N030E_20101117_gmted_mea075.tif\n 70N000E_20101117_gmted_mea075.tif\n 70N030W_20101117_gmted_mea075.tif\n 70N060W_20101117_gmted_mea075.tif\n 70N090W_20101117_gmted_mea075.tif","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#Tests-of-Aster-vs.-GTOPO30-Topography","page":"HiRes Topography","title":"Tests of Aster vs. GTOPO30 Topography","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The images below shows the topography of the Burren, a hilly region in the west of Ireland, approx. 30km square, as represented by GTOPO30 (top) and Aster (bottom).  The extra detail provided by Aster is apparent.  The segments of straight line represent the coastline as plotted by the GrADS \"hires\" map.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The difference fields of surface winds and sea-level pressure between a Harmonie run using GTOPO30 PGD files and another Harmonie run using PGD files from the Aster topography are shown in the image below.  Both runs were at 0.5km resolution over a West of Ireland domain, and the difference fields were taken after a 24hr forecast.  The only difference between these two runs was the PGD.lfi and PGD.fa topographic files used as input – or more precisely, between the topographic datasets used to generate those 2 PGD files.  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The first point to emphasize about this chart is that the differences are very small: the scale on the right is in units of 0.02hPa, and maximum sea-level pressure difference is only about 0.1hPa in magnitude (though it can be of either sign), and surface wind-speed differences are no larger than 04m s^-1.  So refining the PGD topography has a relatively small impact on the overall forecast.  The main feature of the sea-level pressure difference field is the wave-train originating in the mountains of Connemara and propagating downstream eastwards.  The slight difference in representation of those mountains between GTOPO30 and Aster is enough to generate that weak but surprisingly coherent wave-train.  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The topographic input used by the “Aladin” part of Harmonie (from OroMean, NbPeaks, and other files) were unchanged for these runs.  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#What-about-the-Aladin-Topography-Files-(Oro_Mean,-etc.)?","page":"HiRes Topography","title":"What about the Aladin Topography Files (Oro_Mean, etc.)?","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The directory HM_CLDATA/GTOPT030 contains 9 files, 7 of which are derived in some way from the full GTOPO30 topography data set:","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"File Field Unit Nb bits\nOro_Mean Mean orography (mean of Hmean) m 16\nSigma Sub-grid std dev of Hmean m 16\nNb_Peaks Number of sub-grid peaks  8\nHmax-HxH-Hmin_ov4 mean of (Hmax-Hmean)(Hmean-Hmin)/4 m^2 32\nDh_over_Dx_Dh_over_Dy mean of dHmean/dx x dHmean/dy m^2 km^-2 32\nDh_over_Dx_square mean of (dHmean/dx)^2^ m^2 km^-2 32\nDh_over_Dy_square mean of (dHmean/dy)^2^ m^2 km^-2 32\nWater_Percentage Land/Sea mask % water 8\nUrbanisation Fraction of urbanisation % city 8","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"2 of the files (Urbanisation, Water_Percentage) are from a NOAA/Navy Global95 dataset and are not related to GTOPO30 at all.  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The other 7 files each contain information derived from 5x5 sub-grid boxes of the main gtopo30.dir.  (So each is 25 times smaller than gtopo30.dir, accounting for data-type).","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"The word \"mean\" in the table above means \"averaged over the GTOPO30 pixels contained in one 2'30\" box (usually 5x5 GTOPO30 pixels)\".   ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"All these files are read in by ald/c9xx/eincli1.F90 .  At run-time, the files are only read during the \"Climate\" phase of each run, and the information in them ultimately written out to the m01, m02, etc. monthly files.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"After that, however, these files have no effect whatsoever on Forecast output.  ","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"If the 7 secondary GTOPO30 files above are replaced with equivalent ones generated from Aster data, they are read during the \"Climate\" phase and embedded into m01, m02, etc., but then forecast runs that use these generate output that is bit-wise identical to that produced by a \"control\" executable (and \"control\" m01, etc.) based on GTOPO30.  (Originally, I found some very small differences in output between my \"Aster\" and \"GTOPO30\" runs, but that must have been due to me changing something too much in my \"Aster\" executable, which necessarily uses modified eincli1.F90 and einter1.F90 input files, or not using a totally \"clean slate\" of input files to start with).","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"So those 9 derived files in $HM_CLDATA/GTOPT030 are ultimately redundant, and there is no point replacing them with equivalent ones based on high-resolution Aster topography.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#Comment-Laura-Rontu-21-July-2013","page":"HiRes Topography","title":"Comment Laura Rontu 21 July 2013","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"In my understanding, the 7 secondary files are used to derive the following three fields in m01 etc. (example from gl listing of some m01):","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":" SURFET.GEOPOTENT> 001:220-00000-105@   10115_00:00+0000 000 Standard deviation of orograph   0.000E+000  195.020E+000    4.812E+003  326.546E+000\n SURFVAR.GEOP.ANI> 001:221-00000-105@   10115_00:00+0000 000 Anisotropy coeff of topography   0.000E+000  542.720E-003    1.000E+000  313.600E-003\n SURFVAR.GEOP.DIR> 001:222-00000-105@   10115_00:00+0000 000 Direction of main axis of topo  -1.571E+000   96.202E-003    1.939E+000  661.270E-003","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"These are used by the ALADIN/ALARO buoyancy wave parametrization for generation and dissipation of subgrid-scale waves due to orography. Such a parametrization is not applied in AROME, thus these fields are not used there and do not have any influence on AROME results. However, these or similar variables can be found also in the PGD file, among other orography-related variables derived from GTOPO30 (another example of gl listing of some PGD.lfi):","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":" ZS              > 001:008-00000-105@20051219_00:00+0000 000 Oro hgt.     0.000E+000  137.923E+000    1.354E+003  152.975E+000\n AVG_ZS          > 001:001-00600-105@20051219_00:00+0000 000 Average oro     0.000E+000  137.923E+000    1.436E+003  154.681E+000\n SIL_ZS          > 001:002-00600-105@20051219_00:00+0000 000 Silhouette oro     0.000E+000  150.633E+000    1.575E+003  168.821E+000\n SSO_STDEV       > 001:005-00600-105@20051219_00:00+0000 000 Stdv SSO    -0.000E+000   16.335E+000  435.020E+000   24.560E+000\n MIN_ZS          > 001:003-00600-105@20051219_00:00+0000 000 Min subgrid oro     0.000E+000  119.170E+000    1.250E+003  136.836E+000\n MAX_ZS          > 001:004-00600-105@20051219_00:00+0000 000 Max subgrid oro     0.000E+000  162.376E+000    1.776E+003  181.778E+000\n SSO_ANIS        > 001:006-00600-105@20051219_00:00+0000 000 Aniso SSO     0.000E+000  543.880E-003    1.000E+000  194.160E-003\n SSO_DIR         > 001:007-00600-105@20051219_00:00+0000 000 Direction SSO   -90.000E+000   29.056E+000   90.000E+000   40.999E+000\n SSO_SLOPE       > 001:008-00600-105@20051219_00:00+0000 000 Slop SSO     0.000E+000   28.708E-003  718.360E-003   41.450E-003\n HO2IP           > 001:009-00600-105@20051219_00:00+0000 000 h/2 i+     0.000E+000    6.905E+000  274.755E+000   11.246E+000\n HO2JP           > 001:010-00600-105@20051219_00:00+0000 000 h/2 j+     0.000E+000    7.030E+000  290.501E+000   11.472E+000\n HO2IM           > 001:011-00600-105@20051219_00:00+0000 000 h/2 i-     0.000E+000    6.964E+000  256.012E+000   11.534E+000\n HO2JM           > 001:012-00600-105@20051219_00:00+0000 000 h/2 j-     0.000E+000    7.190E+000  352.647E+000   11.734E+000\n AOSIP           > 001:013-00600-105@20051219_00:00+0000 000 A/S i+     0.000E+000    8.055E-003  548.990E-003   14.087E-003\n AOSJP           > 001:014-00600-105@20051219_00:00+0000 000 A/S j+     0.000E+000    8.297E-003  461.020E-003   14.306E-003\n AOSIM           > 001:015-00600-105@20051219_00:00+0000 000 A/S i-     0.000E+000    8.280E-003  521.020E-003   14.863E-003\n AOSJM           > 001:016-00600-105@20051219_00:00+0000 000 A/S i-     0.000E+000    8.454E-003  471.930E-003   15.079E-003","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"Presently, derivations are done automatically, so there is nothing to worry for the user from the point of view of technical implementation. However, eventually there are needs for further development and improvements when the high-resolution source data on topography will be used.","category":"page"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/#Conclusion","page":"HiRes Topography","title":"Conclusion","text":"","category":"section"},{"location":"ExperimentConfiguration/How_to_use_hires_topography/","page":"HiRes Topography","title":"HiRes Topography","text":"In order to replace the (relatively) coarse-resolution GTOPO30 topography with higher-resolution data (e.g., from Aster), it is enough to generate replacements for the gtopo30.hdr and gtopo30.dir files in the $HM_CLDATA/PGD directory, as described in the upper part of this page.","category":"page"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/#Surface-variables-assimilated-/-read-in-OI_main","page":"CANARI OI MAIN","title":"Surface variables assimilated / read in OI_main","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/","page":"CANARI OI MAIN","title":"CANARI OI MAIN","text":"CANARI_OI_MAIN is the surface assimilation scheme which emulates what is done in CANARI for old_surface, but by using the external surface schme SURFEX.","category":"page"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/","page":"CANARI OI MAIN","title":"CANARI OI MAIN","text":"The default surface model is SURFEX and the default surface assimilation scheme is CANARI_OI_MAIN.","category":"page"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/#NATURE","page":"CANARI OI MAIN","title":"NATURE","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/#WG2/WG1/TG2/TG1","page":"CANARI OI MAIN","title":"WG2/WG1/TG2/TG1","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/","page":"CANARI OI MAIN","title":"CANARI OI MAIN","text":"The uppermost two levels in ISBA of soil moisture and temperature are assimilated. With CANARI/CANARI_OI_MAIN by an OI method, by CANARI_SURFEX_EKF by an Extended Kalman Filter (EKF).","category":"page"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/#SNOW","page":"CANARI OI MAIN","title":"SNOW","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/","page":"CANARI OI MAIN","title":"CANARI OI MAIN","text":"The snow analysis is performed in CANARI and is controlled by the key: LAESNM. This is set default to be true in scr/RunCanari. And if running with SURFEX this will need to be true also in scr/OI_main as the SURFEX snow then needs to be updated by the analysis done in CANARI.","category":"page"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/#SEA","page":"CANARI OI MAIN","title":"SEA","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/#SST/SIC","page":"CANARI OI MAIN","title":"SST/SIC","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/","page":"CANARI OI MAIN","title":"CANARI OI MAIN","text":"The only option for SST/SIC at the moment is to take it from the boundaries.","category":"page"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/","page":"CANARI OI MAIN","title":"CANARI OI MAIN","text":"ecf/config_exp.h :SST=BOUNDARY","category":"page"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/","page":"CANARI OI MAIN","title":"CANARI OI MAIN","text":"If you are using boundaries from IFS the task Interpolecsst will interpolate sst from your boundary file and take into account that SST in the IFS files is not defined over land (as for HIRLAM) and also use an extra-polation routine to propagate the SST into narrow fjords. ","category":"page"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/","page":"CANARI OI MAIN","title":"CANARI OI MAIN","text":"There is a SST analysis built-in in CANARI but not used by HARMONIE or METEO-FRANCE.","category":"page"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/#WATER","page":"CANARI OI MAIN","title":"WATER","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/#LAKE-temperature","page":"CANARI OI MAIN","title":"LAKE temperature","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/","page":"CANARI OI MAIN","title":"CANARI OI MAIN","text":"Lake temperatures are updated in OI_main and are extrapolated from the land surface temperatures.","category":"page"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/#TOWN","page":"CANARI OI MAIN","title":"TOWN","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/#ROAD-temperature","page":"CANARI OI MAIN","title":"ROAD temperature","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI_OI_MAIN/","page":"CANARI OI MAIN","title":"CANARI OI MAIN","text":"Only used when TEB is activated (key: LAROME). Increment for TG2 is added to to ROAD layer 3. ","category":"page"},{"location":"#Harmonie-System-Documentation","page":"Home","title":"Harmonie System Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"note: Note\nIf there is information missing from our old trac/wiki pages let us know. See this issue for pages that have been moved.  Suggestions for improvements are very welcome. ","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To update a page:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Click the \"Edit on Github\" button at the top right of the page\nEdit the markdown file on github.com\ncommit (this creates a new branch in your fork) and start a pull request","category":"page"},{"location":"","page":"Home","title":"Home","text":"When adding new pages also add them to docs/pages.jl so they appear in the navigation bar.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Instructions how to build the system documentation locally are here. ","category":"page"},{"location":"Observations/RadarData/#Assimilation-of-Radar-Data","page":"Radar","title":"Assimilation of Radar Data","text":"","category":"section"},{"location":"Observations/RadarData/","page":"Radar","title":"Radar","text":"This documentation outlines how to retrieve, process and assimilate HDF5 radar data","category":"page"},{"location":"Observations/RadarData/#HARMONIE-compilation","page":"Radar","title":"HARMONIE compilation","text":"","category":"section"},{"location":"Observations/RadarData/","page":"Radar","title":"Radar","text":"HIRLAM have made code changes to BATOR to allow the direct reading of HDF5 radar data and conversion to ODB suitable for use in the HARMONIE data assimilation system. If you wish to use these changes you must compile HARMONIE with support for HDF5. This requires the addition of -DUSE_HDF5 to the FDEFS in your makeup config file as well has adding hdf5 to EXTMODS. util/makeup/config.ECMWF.atos.gnu is an example of a makeup config file ","category":"page"},{"location":"Observations/RadarData/#Format","page":"Radar","title":"Format","text":"","category":"section"},{"location":"Observations/RadarData/","page":"Radar","title":"Radar","text":"The BATOR code assumes the HDF5 radar data being read uses the OPERA Data Information Model (ODIM). See http://www.eumetnet.eu/sites/default/files/OPERA2014O4ODIM_H5-v2.2.pdf for further information.","category":"page"},{"location":"Observations/RadarData/#Data-retreival","page":"Radar","title":"Data retreival","text":"","category":"section"},{"location":"Observations/RadarData/","page":"Radar","title":"Radar","text":"Local HDF5 ODIM radar data can be used or, alternatively, can be retrieved from a BALTRAD ftp site.","category":"page"},{"location":"Observations/RadarData/#Data-processing","page":"Radar","title":"Data processing","text":"","category":"section"},{"location":"Observations/RadarData/","page":"Radar","title":"Radar","text":"The HARMONIE script system requires that the OPERA HDF5 data files be stored in RADARDIR (defined in ecf/config_exp.h ) and have a file name using the format: ${HDFID}_qcvol_${DATE}T${HH}00.h5 where: ","category":"page"},{"location":"Observations/RadarData/","page":"Radar","title":"Radar","text":"HDFID is a 5 digit OPERA radar identifier\nDATE is the date\nHH is the hour","category":"page"},{"location":"Observations/RadarData/#Common-pitfalls","page":"Radar","title":"Common pitfalls","text":"","category":"section"},{"location":"Observations/RadarData/","page":"Radar","title":"Radar","text":"Forgetting to add -DUSE_HDF5 correctly to your config file\nIncorrect RADARDIR\nIncorrect file names\nIncorrect format entered in refdata - BATOR is quite strict about how it reads the information in refdata:","category":"page"},{"location":"Observations/RadarData/","page":"Radar","title":"Radar","text":"02918zh  HDF5     radarv           20100808 03 ","category":"page"},{"location":"Observations/RadarData/#Further-reading","page":"Radar","title":"Further reading","text":"","category":"section"},{"location":"Observations/RadarData/","page":"Radar","title":"Radar","text":"Martin Ridal's radar data assimilation presentation","category":"page"},{"location":"Build/CompileHarmonie/#Install-Harmonie-31h1-on-an-independent-platform","page":"Compile Harmonie","title":"Install Harmonie 31h1 on an independent platform","text":"","category":"section"},{"location":"Build/CompileHarmonie/#Build-HARMONIE-from-the-HARMONIE-repository","page":"Compile Harmonie","title":"Build HARMONIE from the HARMONIE repository","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"The following assumes you have a working GNU/Linux system with gfortran and gcc (version >= 4.3.0) in your PATH. You can obtain binaries here. Note also that the description below is valid for HARMONIE system around cycle 31h1. Please consult relevant updates in [wiki:HarmonieSystemDocumentation HARMONIE documentation for later cycles]","category":"page"},{"location":"Build/CompileHarmonie/#Preparation","page":"Compile Harmonie","title":"Preparation","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"First you need some auxiliary libraries (attached to this page), provided by ECMWF and Meteo France. Create a directory $HOME/auxlibs and put the following files there:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"-rw-r--r--  1 harmonie harmonie   873979 2007-05-28 11:39 auxlibs_ecmwf.0.2.tgz            ECMWF's sources\n-rw-r--r--  1 harmonie harmonie   156049 2007-05-28 11:40 auxlibs_meteo-france.0.2.tgz     Meteo France's sources","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Note that auxlibs_ecmwf.0.2/bufrdc_000240/bufrdc/buevar.F contains a superfluous EXTERNAL GETENV.  Surround this by #ifndef linux and #endif before make-ing.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Unpack both gzipped tar files.  Until we fix this in the online copy, inauxlibs_ecmwf.0.2/, add the flag -DINTEGER_IS_INT to each CFLAGS, FASTCFLAGS variable in the various config.linux*_gnu* files.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"In each of the auxlibs_ecmwf.0.2 / auxlibs_meteo_france.0.2 directory, build the libraries by running the script","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        ./make_everything","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"If your GNU/Linux system doesn't have the libraries BLAS and LAPACK installed (see ls -l /usr/lib/blas /usr/lib/lapack), get them from http://netlib.org and build them.  Install them as libblas.a and liblapack.a in $HOME/auxlibs.","category":"page"},{"location":"Build/CompileHarmonie/#Get-the-source-and-build","page":"Compile Harmonie","title":"Get the source and build","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Now check out HARMONIE from the repository at the host hirlam.org:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        svn export https://svn.hirlam.org/trunk/harmonie                              # for the latest trunk version\n        svn export https://svn.hirlam.org/tags/harmonie-31h1 harmonie                 # for the tagged version 31h1\n        svn export https://svn.hirlam.org/branches/harmonie-31h1 harmonie             # for the stable branch 31h1","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"[ If you can't reach hirlam.org from the machine you want to build HARMONIE on, you'll have to do this on another machine and tar the result and copy it to the build machine]","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Get the GMKPACK utilities from the HARMONIE source code repository, i.e., in $HOME, perform the following:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        cp -R <harmonie-repository-root>/harmonie/util/gmkpack .","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Export the following environment variables or add them to ~/.bash_profile:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        export GMKROOT=$HOME/gmkpack\n        export ROOTPACK=$HOME/HARMONIE\n        export HOMEPACK=$ROOTPACK\n        export HOMEBIN=$HOMEPACK\n        export GMKFILE=GFORTRAN.LINUX\n        export GMKTMP=/tmp\n        export PATH=$GMKROOT/util:$PATH\n        export MANPATH=$MANPATH:$GMKROOT/man","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Then run build_gmkpack in directory gmkpack.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Create the directory for the new pack:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        mkdir $ROOTPACK","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Then run gmkpack in that directory:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        cd $ROOTPACK\n        gmkpack -r 31h1 -a -p arome                  # 31h1 here refers to the source code cycle number ","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"This creates the subdirectory 31h1_main.01.420.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Go to directory 31h1_main.01.420./src/local and copy the HARMONIE sources to it.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        cp -R <harmonie-repository-root>/harmonie/src/. .","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Then run the compile script from the 31h1_main.01.420. directory:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        ./ics_arome","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"If the version of flex installed on your system is newer than 2.5.4, the build will abort because of a bug in flex.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Go to directory sys/odb98 and change lex.yy.c so that the defines","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        #define INITIAL 0\n        #define LEX_NORMAL 1\n        #define LEX_INCLUDE 2\n        #define LEX_SET 3\n        #define LEX_TYPE 4\n        #define LEX_TABLE 5\n        #define LEX_VIEW 6\n        #define LEX_FROM 7\n        #define LEX_ORDERBY 8\n        #define LEX_EXCLUDED_BY_IFDEF 9","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"precede their uses.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Re-compile the lex.yy.c file:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        gcc -g -c -I. -I../../src/local/odb/compiler lex.yy.c","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"This is a bug, confirmed by the flex maintainers (http://flex.sourceforge.net).","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"There are two violations of the Fortran Standard in the sources:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        src/local/mpa/micro/internals/ini_rain_ice.mnh","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"and","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        src/local/mpa/micro/internals/budget.mnh","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"in both cases, comment out the declaration of integer variable ILUOUT0.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Recompile by by running the compile script from the 31h1_main.01.420. directory:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"        ./ics_arome","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"This completes the build of the main 31h1 cycle.","category":"page"},{"location":"Build/CompileHarmonie/#Derived-pack-and-make-local-changes","page":"Compile Harmonie","title":"Derived pack and make local changes","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Now create a derived pack so that you can add local updates:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Start by creating a separate directory in your home directory (here chosen to be ARO):","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"         mkdir AROME\n         cd AROME","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Then, reflect this derived pack in your exported environment variables:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"         export HOMEPACK=$HOME/AROME\n         export HOMEBIN=$HOMEPACK","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Enter the HOMEPACK directory:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"         cd $HOMEPACK","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"and create the derived pack (identified by the string EXP):","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"         gmkpack -r 31h1 -u EXP -p arome","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Go to this derived pack's directory:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"         cd $HOMEPACK/EXP","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"and copy the HARMONIE scripts:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"         cp -R <harmonie-repository-root>/harmonie/scr .","category":"page"},{"location":"Build/CompileHarmonie/#Additional-local-changes-for-cycle-31h1","page":"Compile Harmonie","title":"Additional local changes for cycle 31h1","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"For code in cycle 31h1, we have to update a few source files because they lead to Segmentation Violations (two because of errors in the source code, one because of a compiler error):","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          cp ~/HARMONIE/31h1_main.01.420./src/local/arp/utility/echien.F90 ./src/local/arp/utility/\n          cp ~/HARMONIE/31h1_main.01.420./src/local/arp/pp_obs/openfpfa.F90 ./src/local/arp/pp_obs/","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"In those files, add a logical variable LDGARDS, set it to .FALSE. and change the last argument to subroutine FACIES to LDGARDS.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"The following is filed as a bug report. As the report indicates, this has been fixed as of 2007/09/06.  If your compiler is of this date or later (see gfortran -v), you don't have to perform this update:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"In the following file:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          cp ~/HARMONIE/31h1_main.01.420./src/local/mse/internals/soil_albedo_1d_patch.mnh ./src/local/mse/internals/","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"change the following lines:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"CASE ('DRY ')\n  PALBVIS_SOIL(:,:) = SPREAD(PALBVIS_DRY(:),2,IPATCH)\n  PALBNIR_SOIL(:,:) = SPREAD(PALBNIR_DRY(:),2,IPATCH)\n  PALBUV_SOIL (:,:) = SPREAD(PALBUV_DRY (:),2,IPATCH)","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"into:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"CASE ('DRY ')\n  DO JPATCH = 1, IPATCH\n     PALBVIS_SOIL(:,JPATCH) = PALBVIS_DRY(:)\n     PALBNIR_SOIL(:,JPATCH) = PALBNIR_DRY(:)\n     PALBUV_SOIL (:,JPATCH) = PALBUV_DRY (:)\n  ENDDO","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Filed as a bug report. As the report indicates, this has been fixed as of 2007/09/06.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Go back to the derived pack's directory and build the derived AROME executable.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          ./ics_arome","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"You will see the three updated routines being recompiled and added to their respective libraries.","category":"page"},{"location":"Build/CompileHarmonie/#Prepare-scripts","page":"Compile Harmonie","title":"Prepare scripts","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Now change two scripts you definitely have to change (Env_expdesc, because it describes your experiment and Climate and ExtractBD, because the original scripts assumes the original (non-interpolated) climate data and boundary files reside on ECFS at ECMWF [change ecp to cp]):","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"This assumes you have accumulated all HARMONIE climate data residing at ec:/rmt/clim_database in a equal tree of plain files.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          cd scr\n          edit Env_expdesc Climate ExtractBD","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"In Env_expdesc, at least change the following:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"TSTEP=60                                # Time step\nMODEL=AROME                             # Forecast model to execute\n                                        # ALADIN,AROME,ALDODB,AROMODB\n\nFLAG=\"arome\"                            # Model/NH flag for finding right namelist etc\nHOST_MODEL=\"hir\"                        # Host model, could be hir,ifs,ald\nTRGT_MODEL=\"aro\"                        # Target model, could be ald,aro","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"and set the domain of your choice.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Now unpack your climate data so that it resides under the directory indicated in Env_expdesc (environment variable CLIMDATA) (we can actually not use it yet, because some steps in the climate file generation currently have to be executed at ECMWF).","category":"page"},{"location":"Build/CompileHarmonie/#Build-additional-utilities","page":"Compile Harmonie","title":"Build additional utilities","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Prepare the source for the HARMONIE conversion utilities:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          mkdir util\n          cd util\n          cp -R <harmonie-repository-root>/harmonie/util/gl  .","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"and build them:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          cd ../util/gl","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Change the ARCH variable in the Makefile to linuxgfortran.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Then repair this error:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":".../prg/gl.F90:75: undefined reference to `iargc_'\n.../prg/gl.F90:84: undefined reference to `getarg_'","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"iargc and getarg are not external functions, they are (non-standard) intrinsics (remove their declarations).","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"And this one","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":".../prg/dumpfld.f90:52/3/4: Error: Nonnegative width required in format string at (1)","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"by changing the format specification to fmt=*.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Then issue the make command:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          make","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"[ You can clean up if something goes wrong by typing \"make clean\".]","category":"page"},{"location":"Build/CompileHarmonie/#Install-namelists","page":"Compile Harmonie","title":"Install namelists","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Subsequently, we copy the namelists from the HARMONIE repository to HOMEPACK:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          cp -R <harmonie-repository-root>/harmonie/nam .","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Now, unfortunately, the run scripts assume that the experiment name (environment variable MYLIB) is test_31h1.  Therefore, in the HOMEPACK directory we make a soft link:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          ln -s EXP test_31h1","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"This is also the reason why you have to make soft links for all namelists in the nam directory with \"31t1\" in it to use \"31h1\", e.g.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          ln -s namelist_31t1_ald2ald_nh_default namelist_31h1_ald2ald_nh_default\n          ...","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"and change LMPOFF to .TRUE. in namelist_31h1_fcstarome_default [don't you love negative logic].","category":"page"},{"location":"Build/CompileHarmonie/#Climate-Files","page":"Compile Harmonie","title":"Climate Files","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Now create the climate files for the intermediate and final model domain on ECMWF.  A good choice for the intermediate grid is to use the same center as the high resolution grid and a resolution 4 times as coarse and with 1/3rd the number of grid points in the X and Y directory.  In this way you will be sure that the high resolution domain is completely contained in the intermediate one.  Of course you also have to ensure that the intermediate grid is completely contained inside the HIRLAM domain you derive the boundaries from.","category":"page"},{"location":"Build/CompileHarmonie/#Prepare-the-input-data","page":"Compile Harmonie","title":"Prepare the input data","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"First, make a scratch directory, for instance in the $HOME directory, and let the environment variable $TEMP point to it:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          mkdir $HOME/scratch\n          export TEMP=$HOME/scratch","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Make a directory for the intermediate climate files and copy them there:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          mkdir -p $TEMP/HARMONIE/RCRa/climate\n          for m in 01 02 03 04 05 06 07 08 09 10 11 12\n          do\n             cp <intermediate-climate-dir>/m$m $TEMP/HARMONIE/RCRa/climate\n          done","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Make a directory for the high resolution climate files and copy them there:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          mkdir -p $TEMP/HARMONIE/test_31h1/climate\n          for m in 01 02 03 04 05 06 07 08 09 10 11 12\n          do\n             cp <highres-climate-dir>/m$m $TEMP/HARMONIE/test_31h1/climate\n          done\n          cp <highres-climate-dir>/PGD.lfi $TEMP/HARMONIE/test_31h1/climate","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Make a directory for the HIRLAM history files to use as boundaries for your HARMONIE run:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          mkdir -p $TEMP/HARMONIE/RCRa/archive\n          for fp in 000 003 006 009 012 015 018 021 024\n          do\n             cp fcYYYYMMDD_HH+$fp $TEMP/HARMONIE/RCRa/archive\n          done","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"[ Note that this assumes that you have changed ExtractBD ($HOST_MODEL=hir) to copy them from there, instead of ecp -o them from ECFS:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          ...\n          cp $TEMP/HARMONIE/RCRa/archive/$FILE .\n          ...","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"]","category":"page"},{"location":"Build/CompileHarmonie/#Get-the-binaries-in-place","page":"Compile Harmonie","title":"Get the binaries in place","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Go to $HOMEPACK/EXP/scr.","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Copy the AROME binary to the scratch tree:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          mkdir -p $TEMP/HARMONIE/test_31h1/bin\n          cp ../bin/AROME $TEMP/HARMONIE/test_31h1/bin","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Also copy the gl utilities over there:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          cp ../util/gl/bin/* $TEMP/HARMONIE/test_31h1/bin","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Finally, steal a mandtg from a HIRLAM repository you have lying around:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          cp <HIRLAM-repository>/scripts/mandtg $TEMP/HARMONIE/test_31h1/bin","category":"page"},{"location":"Build/CompileHarmonie/#Running-the-created-AROME","page":"Compile Harmonie","title":"Running the created AROME","text":"","category":"section"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"Set the start and end date for which you have input data in the script Start_harmonie and run it:","category":"page"},{"location":"Build/CompileHarmonie/","page":"Compile Harmonie","title":"Compile Harmonie","text":"          ./Start_harmonie","category":"page"},{"location":"Observations/Modes/#Mode-S-Enhanced-Surveillance","page":"Modes","title":"Mode-S Enhanced Surveillance","text":"","category":"section"},{"location":"Observations/Modes/#Introduction","page":"Modes","title":"Introduction","text":"","category":"section"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"From http://mode-s.knmi.nl:","category":"page"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"A novel method to measure wind and temperature is related to tracking and ranging by an enhanced surveillance (EHS) air traffic control (ATC) radar.","category":"page"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"Modern aircraft carry sensors to measure the Mach number (using pitotstatic probe) and the total air temperature (T). An EHS radar interrogates all aircraft in sight in a selective mode (Mode-S), on which the aircraft replies with a message containing, for example, magnetic heading, airspeed and Mach number. From this information wind and temperature can be extracted.","category":"page"},{"location":"Observations/Modes/#Mode-S-EHS-data","page":"Modes","title":"Mode-S EHS data","text":"","category":"section"},{"location":"Observations/Modes/#Description","page":"Modes","title":"Description","text":"","category":"section"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"The data description is available here: http://mode-s.knmi.nl/data/","category":"page"},{"location":"Observations/Modes/#Access","page":"Modes","title":"Access","text":"","category":"section"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"Access to MUAC Mode-S EHS data can be requested from KNMI by signing a Non Disclosure Agreement. Send an e-mail to mode-s@knmi.nl with a request like this:","category":"page"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"Dear Sir/Madam,\n\nOn behalf of MyNHMS, My NHMS Full Name, I would like to request access to Mode-S EHS derived meteorological data made available by Maastricht Upper Area Centre (MUAC) of EUROCONTROL and KNMI.\n\nMyNHMS intend to use these data for NWP (Numerical Weather Prediction) research and development with the hope of assimilating these data in operational NWP system(s) in the near future.\n\nMy contact details are as follows:\nName\nAddress\ne-mail\nTelephone\n\nYours Sincerely,\nName","category":"page"},{"location":"Observations/Modes/#Prepare-the-BUFR-data","page":"Modes","title":"Prepare the BUFR data","text":"","category":"section"},{"location":"Observations/Modes/#ftp-server","page":"Modes","title":"ftp server","text":"","category":"section"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"Mode-S EHS bufr, netCDF and ASCII data are available on the KNMI ftp server, ftpservice.knmi.nl. The ASCII and netCDF file contain all observations, while BUFR file contains about 10% of the available observations. The thinning is for descending and ascending aircraft evry 300m in pressure altitude, and for level flight one observation per 2 minutes.","category":"page"},{"location":"Observations/Modes/#BUFR-data-sub-category-change","page":"Modes","title":"BUFR data sub-category change","text":"","category":"section"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"At present the BUFR-sub category needs to be changed: bufr_set, an ecCodes tool, can be used to change the data sub-category from 146 to 147. This is required by HARMONIE.","category":"page"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"bufr_set -sdataSubCategory=147 Mode-S-EHS_MUAC_20170507_2345.bufr Mode-S-EHS_MUAC_20170507_2345_147.bufr","category":"page"},{"location":"Observations/Modes/#HARMONIE-changes","page":"Modes","title":"HARMONIE changes","text":"","category":"section"},{"location":"Observations/Modes/#scr/include.ass","page":"Modes","title":"scr/include.ass","text":"","category":"section"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"In scr/include.ass should be edited to \"switch on\" the use of Mode-S EHS data:","category":"page"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"export MODESEHS_OBS=1          # Mode-S EHS","category":"page"},{"location":"Observations/Modes/#Processing-using-Bator","page":"Modes","title":"Processing using Bator","text":"","category":"section"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"The BUFR template (WMO AMDAR v7) used by your Mode-S EHS data should be defined in the param.cfg file used by Bator. param.cfg files for Bator are in the nam namelist directory. The modes param.cfg template should be something like this:","category":"page"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"BEGIN amdar\n\nEND","category":"page"},{"location":"Observations/Modes/#Processing-using-Oulan","page":"Modes","title":"Processing using Oulan","text":"","category":"section"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"The processing of Mode-S EHS BUFR using Oulan is controlled by the following namelist entry in scr/Oulan:","category":"page"},{"location":"Observations/Modes/","page":"Modes","title":"Modes","text":"LMODES=.FALSE.","category":"page"},{"location":"Observations/Cope/#ODB-creation-(COPE)","page":"Cope","title":"ODB creation (COPE)","text":"","category":"section"},{"location":"Observations/Cope/#General-Description","page":"Cope","title":"General Description","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"HIRLAM, ALADIN and Météo France are working together with ECMWF to develop COPE, Continuous Observation Pre-processing Environment, to replace Oulan/Bator (and BUFR2ODB at ECMWF), to improve the pre-processing of observations for use in NWP. COPE developments are made in ECMWF's git repository.","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"note: Note\nnot available. A COPE 40h1 HARMONIE branch has been created to test COPE in the HARMONIE framework.","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"Here are some links that may be of interest:","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"https://software.ecmwf.int/wiki/display/COPE/COPE: COPE wiki (restricted access)\nhttp://www.rclace.eu/File/DataAssimilation/2014/201406COPEReadingreport.pdf: Report on the COPE technical meeting, Alena Trojáková.  ECMWF, Reading 9-12, June 2014\nhttp://www.cnrm.meteo.fr/aladin/IMG/pdf/copeovervieweoinwhelan.pdf: Overview of COPE, Eoin Whelan. Joint 24th ALADIN Workshop & HIRLAM All Staff Meeting 2014, 7-11 April 2014, Romania.","category":"page"},{"location":"Observations/Cope/#\"Support\"-software-packages","page":"Cope","title":"\"Support\" software packages","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"This section provides a step-by-step set of instruction on how to compile COPE and COPE related software.","category":"page"},{"location":"Observations/Cope/#Preparation","page":"Cope","title":"Preparation","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"These instructions rely on the ODB API source code bundle odbapibundle-0.15.2-Source.tar.gz and emoslib libemos-4.4.2-Source.tar.gz. The default install location for software packages is in $HOME/metapp/.","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"mkdir -p $HOME/test_ecmwf_releases\nmkdir -p $HOME/test_ecSource\ncp odb_api_bundle-0.15.2-Source.tar.gz $HOME/test_ecmwf_releases/\ncp libemos-4.4.2-Source.tar.gz $HOME/test_ecmwf_releases/\ncd $HOME/test_ecmwf_releases\ngunzip odb_api_bundle-0.15.2-Source.tar.gz\ntar -xvf odb_api_bundle-0.15.2-Source.tar\ngunzip libemos-4.4.2-Source.tar.gz\ntar -xvf libemos-4.4.2-Source.tar","category":"page"},{"location":"Observations/Cope/#ecBuild-(2.4.0)","page":"Cope","title":"ecBuild (2.4.0)","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"ecBuild is a set of cmake macros used by other ECMWF software packages.","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"cd $HOME/test_ecmwf_releases/odb_api_bundle-0.15.2-Source/ecbuild\nmkdir build\ncd build/\ncmake .. -DCMAKE_INSTALL_PREFIX=$HOME/metapp/ecbuild/2.4.0/gnu/\nmake\nmake check\nmake install","category":"page"},{"location":"Observations/Cope/#eckit-(0.14.0)","page":"Cope","title":"eckit (0.14.0)","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"cd $HOME/test_ecmwf_releases/odb_api_bundle-0.15.2-Source/eckit\nmkdir build\ncd build/\ncmake .. -DCMAKE_INSTALL_PREFIX=$HOME/metapp/eckit/0.14.0/gnu/ -DCMAKE_MODULE_PATH=$HOME/metapp/ecbuild/2.4.0/gnu/share/ecbuild/cmake\nmake -j 4\nmake check\nmake install","category":"page"},{"location":"Observations/Cope/#metkit-(0.3.0)","page":"Cope","title":"metkit (0.3.0)","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"cd $HOME/test_ecmwf_releases/odb_api_bundle-0.15.2-Source/metkit\nmkdir build\ncd build/\ncmake .. -DCMAKE_INSTALL_PREFIX=$HOME/metapp/metkit/0.3.0/gnu/ -DCMAKE_MODULE_PATH=$HOME/metapp/ecbuild/2.4.0/gnu/share/ecbuild/cmake/ -DECKIT_PATH=$HOME/metapp/eckit/0.14.0/gnu/\nmake -j 4\nmake check\nmake install","category":"page"},{"location":"Observations/Cope/#libemos-(4.4.2)","page":"Cope","title":"libemos (4.4.2)","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"cd $HOME/test_ecmwf_releases/libemos-4.4.2-Source\nmkdir build\ncd build/\ncmake .. -DCMAKE_INSTALL_PREFIX=$HOME/metapp/libemos/4.4.2/gnu -DGRIB_API_PATH=PATH_TO_GRIBAPI\nmake\nmake check\nmake install","category":"page"},{"location":"Observations/Cope/#\"Main\"-software-packages","page":"Cope","title":"\"Main\" software packages","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"These instructions assume you have access to the ODB-API and COPE ECMWF git repositories. The default install location for software packages is in HOME/metapp/.","category":"page"},{"location":"Observations/Cope/#odb-(40t1.01)","page":"Cope","title":"odb (40t1.01)","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"ECMWF maintain a \"standalone\" version of ODB software that is compiled with cmake. A 40t1 tag, 40t1.01, has been created to support the flavour of ODB used by harmonie-40h1.1.","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"cd $HOME/test_ecmwf_releases\ngit clone https://dui@software.ecmwf.int/stash/scm/odb/odb.git\ncd $HOME/test_ecmwf_releases/odb\ngit pull\ngit archive --format=tar -o $HOME/test_ecSource/odb-40t1.01-Source.tar --prefix=odb-40t1.01/ 40t1.01\ncd $HOME/test_ecSource\ntar -xvf odb-40t1.01-Source.tar\ncd odb-40t1.01/\nmkdir build\ncd build/\ncmake .. -DCMAKE_INSTALL_PREFIX=$HOME/metapp/odb/40t1.01/gnu/ -DCMAKE_MODULE_PATH=$HOME/metapp/ecbuild/2.4.0/gnu/share/ecbuild/cmake/ -DODB_SCHEMAS=\"ECMA;CCMA\"\nmake -j 8\nmake check\nmake install","category":"page"},{"location":"Observations/Cope/#ODB-API-(0.15.4)","page":"Cope","title":"ODB API (0.15.4)","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"ODB API is a software developed at ECMWF for encoding and processing of observational data. It includes a SQL filtering and statistics engine, command line tools and APIs for C/C++, Fortran and Python. ODB API works with data format used in ECMWF observational feedback archive. Development of ODB API has been partially funded by the Met Office. More details here: https://software.ecmwf.int/wiki/display/ODBAPI/ODB+API+Home","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"ODB API provides is required by COPE as well as for ODB2 to ODB1 conversion.","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"cd $HOME/test_ecmwf_releases\ngit clone https://dui@software.ecmwf.int/stash/scm/odb/odb_api.git\ncd $HOME/test_ecmwf_releases/odb_api\ngit pull\ngit archive --format=tar -o $HOME/test_ecSource/odb_api-0.15.4-Source.tar --prefix=odb_api-0.15.4/ 0.15.4\ncd $HOME/test_ecSource\ntar -xvf odb_api-0.15.4-Source.tar\ncd odb_api-0.15.4/\nmkdir build\ncd build/\ncmake .. -DCMAKE_INSTALL_PREFIX=$HOME/metapp/odb_api/0.15.4/gnu/ -DCMAKE_MODULE_PATH=$HOME/metapp/ecbuild/2.4.0/gnu/share/ecbuild/cmake/  -DECKIT_PATH=$HOME/metapp/eckit/0.14.0/gnu/ -DMETKIT_PATH=$HOME/metapp/metkit/0.3.0/gnu -DENABLE_MIGRATOR=ON -DODB_PATH=$HOME/metapp/odb/40t1.01/gnu -DENABLE_FORTRAN=ON -DENABLE_PYTHON=ON -DENABLE_NETCDF=ON\nmake -j 8\nmake check\nmake install","category":"page"},{"location":"Observations/Cope/#b2o-(40t1.01)","page":"Cope","title":"b2o (40t1.01)","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"b2o is a library and command line tool to extract ODB data from BUFR files. A 40t1 tag, 40t1.01, has been created to support the flavour of ODB used by harmonie-40h1.1.","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"cd $HOME/test_ecmwf_releases\ngit clone https://dui@software.ecmwf.int/stash/scm/cope/b2o.git\ncd b2o\ngit pull\ngit archive --format=tar -o $HOME/test_ecSource/b2o-40t1.01-Source.tar --prefix=b2o-40t1.01/ 40t1.01\ncd $HOME/test_ecSource\ntar -xvf b2o-40t1.01-Source.tar\ncd b2o-40t1.01/\nmkdir build\ncd build/\ncmake .. -DCMAKE_INSTALL_PREFIX=$HOME/metapp/b2o/40t1.01/gnu/ -DCMAKE_MODULE_PATH=$HOME/metapp/ecbuild/2.4.0/gnu/share/ecbuild/cmake/ -DLIBEMOS_PATH=$HOME/metapp/libemos/4.4.2/gnu/ -DECKIT_PATH=$HOME/metapp/eckit/0.14.0/gnu/ -DODB_API_PATH=$HOME/metapp/odb_api/0.15.4/gnu\nmake -j 4\nmake check\nmake install","category":"page"},{"location":"Observations/Cope/#COPE","page":"Cope","title":"COPE","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"These instructions are based on building the develop branch of COPE. If you wish to use a tagged version you must use version 0.5.3 or greater.","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"cd $HOME/test_ecmwf_releases\ngit clone https://dui@software.ecmwf.int/stash/scm/cope/cope.git\ncd $HOME/test_ecmwf_releases/cope\ngit pull\nmkdir build\ncd build/\ncmake .. -DCMAKE_INSTALL_PREFIX=$HOME/metapp/cope/develop/gnu -DCMAKE_MODULE_PATH=$HOME/metapp/ecbuild/2.4.0/gnu/share/ecbuild/cmake/ -DECKIT_PATH=$HOME/metapp/eckit/0.14.0/gnu/ -DODB_API_PATH=$HOME/metapp/odb_api/0.15.4/gnu -DB2O_PATH=$HOME/metapp/b2o/40t1.01/gnu -DCMAKE_PREFIX_PATH=$HOME/metapp/libemos/4.4.2/gnu/\nmake -j 4\n# make check ## BROKEN DUE TO CHANGES TO ODB SCHEMA IN THIS BRANCH\nmake install","category":"page"},{"location":"Observations/Cope/#COPE-in-HARMONIE-system","page":"Cope","title":"COPE in HARMONIE system","text":"","category":"section"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"COPE is currently only available in branches/harmonie40h1cope and has only been tested on a local Linux server.","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"The use of COPE in HARMONIE relies on ODB-API, b2o and COPE itself.","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"ODB-API tools must be included in PATH\nThe ECMA.sch used by COPE is maintained in the b2o version described above.\nmf_vertco_type specific changes are included in the feature/mfvertcotype branch of COPE\nscr/Cope includes the setting of the following environment variables which rely on COPE_DIR and B2O_DIR. These can be set in your Env_system file.","category":"page"},{"location":"Observations/Cope/","page":"Cope","title":"Cope","text":"export COPE_DEFINITIONS_PATH=${COPE_DIR}/share/cope\nexport ODB_SCHEMA_FILE=${B2O_DIR}/share/b2o/ECMA.sch\nexport ODB_CODE_MAPPINGS=${B2O_DIR}/share/b2o/odb_code_mappings.dat\nexport ODBCODEMAPPINGS=${B2O_DIR}/share/b2o/odb_code_mappings.dat","category":"page"},{"location":"ForecastModel/Outputlist/#Parameter-list-and-GRIB-definitions","page":"Output List","title":"Parameter list and GRIB definitions","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#HARMONIE-system-output","page":"Output List","title":"HARMONIE system output","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"The HARMONIE system writes its primary output, in FA format, to the upper air history files ICMSHHARM+llll and the SURFEX history files ICMSHHARM+llll.sfx, where HARM is the four-character experiment identifier set in the configuration file config_exp.h, and llll is normally the current timestep in hours. The files are designed to be complete snapshots of respective model state described by the system for a particular time point. In addition more model output including post-processing/diagnostic fields can be written out during the forecast model integration, such as those model diagnostics or pressure level diagnostics, also in FA format, as PFHARMDOMAIN+llll. The FA files can be considered to be internal format files. All of them can be converted to GRIB files during the run for external usage. The name convention is as follows:","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Forecast upper air history files: ICMSHHARM+llll -> fcYYYYMMDDHH+lll_grib (GRIB1) or fcYYYYMMDDHH+lll_grib2 (GRIB2) \nForecast Surfex history files: ICMSHHARM+llll.sfx -> fcYYYYMMDDHH+lll_grib_sfx (GRIB1 only) \nForecast Surfex selected output: ICMSSELE+llll.sfx -> fcYYYYMMDDHH+lll_grib_sfxs (GRIB1 only) \nPostprocess files: PFHARMDOMAIN+llll.hfp -> fcYYYYMMDDHH+lllgrib_fp (GRIB1) or fcYYYYMMDDHH+lllgrib2_fp (GRIB2) \nAnalysis upper air history files: ICMSHANAL+0000 -> anYYYYMMDDHH+000grib (GRIB1) or anYYYYMMDDHH+000grib2 (GRIB2) (1)\nAnalysis SURFEX history files: ICMSHANAL+0000.sfx -> sa2019041600+000grib_sfx (only GRIB1 for the time being)","category":"page"},{"location":"ForecastModel/Outputlist/#GRIB1-table-2-version-in-HARMONIE","page":"Output List","title":"GRIB1 table 2 version in HARMONIE","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"To avoid conflicts with archived HIRLAM data HARMONIE uses version 253 of table 2. The table is based on the standard WMO version 3 of table 2 and postion 000-127 is kept the same as in the WMO. Note that accumulated and instantaneous versions of the same parameter differ only by the time range indicator. It is thus not sufficient to specify parameter, type and level when you refer to an accumulated parameter, but the time range indicator has to be included as well.","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"The translation of SURFEX files to GRIB1 is still incomplete and contains several WMO violations. This is not changed in the current release but will revised later. However, the upper air history file also includes the most common surface parameters and should be sufficient for most users.","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"The current table 2 version 253 definition files for gribapi can be found in `util/glgrib_api/definitions/`. These local definition files assume centre=233 (Dublin) and should be copied to your own GRIB-API installation. You are strongly recommended to set your own code for generating centre fore operational usage of the data.","category":"page"},{"location":"ForecastModel/Outputlist/#GRIB2-in-HARMONIE","page":"Output List","title":"GRIB2 in HARMONIE","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"The possibility to convert to GRIB2 has been introduced in release-43h2. So far the conversion is restricted to atmospheric history and fullpos files only. To get the output in GRIB2 set ARCHIVE_FORMAT=GRIB2 in ecf/config_exp.h. Please notice that if ARCHIVE_FORMAT=GRIB2 is selected, SURFEX files will be converted to GRIB1 anyway (for the time being). To convert from GRIB1 with GRIB2 using grib_filter we have to tell EcCodes how to translate the parameters. This is done by using the internal HARMONIE tables and setting","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"export ECCODES_DEFINITION_PATH=$SOME_PATH_TO_GL/gl/definitions:$SOME_PATH_TO_ECCODES/share/eccodes/definitions","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Note that there are a few parameters that are not translated to GRIB2 to and those has to be excluded explicitly.","category":"page"},{"location":"ForecastModel/Outputlist/#List-of-parameters","page":"Output List","title":"List of parameters","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#D-model-state-variables-on-model-levels-(1-NLEV),-levelTypehybrid","page":"Output List","title":"3D model state variables on model levels (1-NLEV), levelType=hybrid","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#indicatorOfParameter","page":"Output List","title":"indicatorOfParameter","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#parameterCategory","page":"Output List","title":"parameterCategory","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#parameterNumber","page":"Output List","title":"parameterNumber","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#discipline","page":"Output List","title":"discipline","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"FA name shortName iOP d pC pN stepType unit Description\nSNNNHUMI.SPECIFI q 51 0 1 0 instant kg kg**-1 Specific humidity\nSNNNLIQUID_WATER cwat_cond 76 0 1 83 instant kg kg**-1 Specific cloud liquid water content\nSNNNSOLID_WATER ciwc_cond 58 0 1 84 instant kg kg**-1 Specific cloud ice water content\nSNNNSNOW snow_cond 184 0 1 86 instant kg kg**-1 Specific snow water content\nSNNNRAIN rain_cond 181 0 1 85 instant kg kg**-1 Specific rain water content\nSNNNGRAUPEL grpl_cond 201 0 1 32 instant kg kg**-1 Specific graupel\nSNNNTKE tke 200 0 19 11 instant J kg**-1 Turbulent Kinetic Energy\nSNNNCLOUD_FRACTI tcc 71 0 6 192 instant 0-1 Total cloud cover\nSNNNPRESS.DEPART pdep 212 0 3 8 instant Pa Pressure departure\nSNNNTEMPERATURE t 11 0 0 0 instant K Temperature\nSNNNVERTIC.DIVER vdiv 213 0 2 192 instant s**-1 Vertical Divergence\nSNNNWIND.U.PHYS u 33 0 2 2 instant m s**-1 u-component of wind\nSNNNWIND.V.PHYS v 34 0 2 3 instant m s**-1 v-component of wind","category":"page"},{"location":"ForecastModel/Outputlist/#D-Surface,-prognostic/diagnostic-near-surface-and-soil-variables","page":"Output List","title":"2D Surface, prognostic/diagnostic near-surface and soil variables","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"FA name Unit Parameter Type Level Note\nSURFPRESSION Pa 1 105 0 Surface pressure\nSURFTEMPERATURE K 11 105 0 Surface temperature\nCLSTEMPERATURE K 11 105 2 T2m\nCLSMAXI.TEMPERAT K 15 105 2 Max temperature over 3h\nCLSMINI.TEMPERAT K 16 105 2 Min temperature over 3h\nCLSVENT.ZONAL ms-1 33 105 10 U10m, relative to the model coordinates\nCLSVENT.MERIDIEN ms-1 34 105 10 V10m, relative to the model coordinates\nCLSHUMI.SPECIFIQ kg kg-1 51 105 2 Q2m\nCLSHUMI.RELATIVE % 52 105 2 RH2m\nSURFRESERV.NEIGE kg m-2 65 105 0 Snow depth in water equivalent\n[[Color(green, CLPMHAUT.MOD.XFU)]] m 67 105 0 Height (in meters) of the PBL out of the model\nSURFNEBUL.TOTALE % 71 105 0 Total cloud cover\nSURFNEBUL.CONVEC % 72 105 0 Convective cloud cover\nSURFNEBUL.BASSE % 73 105 0 Low cloud cover\nSURFNEBUL.MOYENN % 74 105 0 Medium cloud cover\nSURFNEBUL.HAUTE % 75 105 0 High cloud cover\nSURFRAYT.SOLAIRE W/m2 117 105 0 [[Color(blue, Parameter identifier was 116)]] Instantaneous surface solar radiation (SW down global)\nSURFRAYT.TERREST W/m2 115 105 0 Instantaneous surface thermal radiation (LW down)\n[[Color(green, SURFCAPE.MOD.XFU)]] J kg-1 160 105 0 Model output CAPE (not calculated by AROME physics)\n~~SURFCAPE.MOD.F04~~ J kg-1 160 105 0 Postprocessed CAPE\n[[Color(green, SURFCAPE.MOD.F00)]] J kg-1 160 105 0 Postprocessed CAPE\n[[Color(green, SURFDIAGHAIL)]] % 161 105 0 AROME  hail diagnostic. LXXDIAGH = .TRUE.\nCLSU.RAF.MOD.XFU m/s 162 105 10 U-momentum of gusts from the model. LXXGST = .TRUE. in NAMXFU. gives gust between current and previous output time step.\nCLSV.RAF.MOD.XFU m/s 163 105 10 V-momentum of gusts from the model. LXXGST = .TRUE. in NAMXFU. gives gust between current and previous output time step.\nSURFINSPLUIE kg/m2 181 105 0 Instantaneous rain.\nSURFINSNEIGE kg/m2 184 105 0 Instantaneous snow.\nSURFINSGRAUPEL kg/m2 201 105 0 Instantaneous graupel.\n[[Color(green, CLSMINI.HUMI.REL)]] % 241 105 2 Min relative humidity over 3h\n[[Color(green, CLSMAXI.HUMI.REL)]] % 242 105 2 Max relative humidity over 3h\n[[Color(green, CLSRAFALES.POS)]] m/s 228 105 10 Gust wind speed","category":"page"},{"location":"ForecastModel/Outputlist/#D-Surface,-accumulated-near-surface-and-soil-variables","page":"Output List","title":"2D Surface, accumulated near-surface and soil variables","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Note that all these are coded with stepType=accum","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"FA name shortName iOP d pC pN level unit Description\nS065RAYT SOL CL cssw 130 0 4 11 65 J m**-2 SW net clear sky rad\nS065RAYT THER CL cslw 131 0 5 6 65 J m**-2 LW net clear sky rad\nSURFACCGRAUPEL grpl 201 0 1 75 0 kg m-2 s-1 Graupel precipitation rate\nSURFACCNEIGE snow 184 0 1 53 0 kg m**-2 Total snowfall rate water equivalent\nSURFACCPLUIE rain 181 0 1 65 0 kg m**-2 Rain\nSURFDIR NORM IRR dni 140 3 6 2 0 J m**-2 Direct normal irradiance\nSURFFLU.CHA.SENS shf 122 0 0 11 0 J m**-2 Sensible heat flux\nSURFFLU.LAT.MEVA lhe 132 0 1 193 0 J m**-2 Latent heat flux through evaporation\nSURFFLU.LAT.MSUB lhsub 244 0 1 202 0 J kg**-1 Latent Heat Sublimation\nSURFFLU.MEVAP.EA wevap 245 0 1 6 0 kg m**-2 Water evaporation\nSURFFLU.MSUBL.NE snsub 246 0 1 62 0 kg m**-2 Snow sublimation\nSURFFLU.RAY.SOLA nswrs 111 0 4 9 0 J m**-2 Net shortwave radiation flux (surface)\nSURFFLU.RAY.THER nlwrs 112 0 5 5 0 J m**-2 Net longwave radiation flux (surface)\nSURFRAYT DIR SUR swavr 116 0 4 7 0 J m**-2 Shortwave radiation flux\nSURFRAYT SOLA DE grad 117 0 4 3 0 J m**-2 Global radiation flux\nSURFRAYT THER DE lwavr 115 0 5 4 0 J m**-2 Longwave radiation flux\nSURFTENS.TURB.ME uflx 124 0 2 198 0 N m**-2 Momentum flux, u-component\nSURFTENS.TURB.ZO vflx 125 0 2 199 0 N m**-2 Momentum flux, v-component\n# tp 61 0 1 8 0 kg m**-2 Total precipitation","category":"page"},{"location":"ForecastModel/Outputlist/#D-TOA,-diagnostic-and-accumulated-variables","page":"Output List","title":"2D TOA, diagnostic and accumulated variables","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Note that all these are coded with time range indicator = 4.","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"     \nSOMMFLU.RAY.SOLA J m-2 113 8 0 TOA SW net radiation (Accumulated Top Solar radiation)\nSOMMFLU.RAY.THER J m-2 114 8 0 TOA LW net radiation (Accumulated Top Thermal radiation)\nSOMMRAYT.SOLAIRE W m-2 113 8 0 TOA Instantaneous SW net radiation\nSOMMRAYT.TERREST W m-2 114 8 0 TOA Instantaneous LW net radiation\nTOPRAYTDIRSOM W m-2 116 8 0 [[Color(blue, Parameter identifier was 117)]]  TOA Accumulated SW down radiation\n[[Color(green, SOMMTBOZCLEAR)]] K 170 8 0 Brightness temperature OZ clear\n[[Color(green, SOMMTBOZCLOUD)]] K 171 8 0 Brightness temperature OZ cloud\n[[Color(green, SOMMTBIRCLEAR)]] K 172 8 0 Brightness temperature IR clear\n[[Color(green, SOMMTBIRCLOUD)]] K 173 8 0 Brightness temperature IR cloud\n[[Color(green, SOMMTBWVCLEAR)]] K 174 8 0 Brightness temperature WV clear\n[[Color(green, SOMMTBWVCLOUD)]] K 175 8 0 Brightness temperature WV cloud","category":"page"},{"location":"ForecastModel/Outputlist/#D-Surface,-Postprocessed-variables","page":"Output List","title":"2D Surface, Postprocessed variables","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"|  FA name         | Unit  | Parameter | Type | Level | Note | | :– | :– | :– | :– | :– | :– |","category":"page"},{"location":"ForecastModel/Outputlist/#D-Surface,-constant-near-surface-and-soil-variables","page":"Output List","title":"2D Surface, constant near-surface and soil variables","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"FA name Unit Parameter Type Level Note\nSPECSURFGEOPOTEN m-2 s-2 6 105 0 Geopotential relative to mean sea level. \"... contains a GRID POINT orography which is the interpolation of the departure orography\"\nSURFIND.TERREMER % 81 105 0 Fr. Land (Land/sea mask)\nSURFAEROS.SEA  251 105 0 Surface aerosol  sea (Marine aerosols, locally defined GRIB)\nSURFAEROS.LAND  252 105 0 Surface aerosol land (Continental aerosols, locally defined GRIB)\nSURFAEROS.SOOT  253 105 0 Surface aerosol soot (Carbone aerosols, locally defined GRIB)\nSURFAEROS.DESERT  254 105 0 Surface aerosol desert (Desert aerosols, locally defined GRIB)\nSURFAEROS.VOLCAN  197 105 0 Surface aerosol volcan (Stratospheric ash, to be locally defined GRIB)\nSURFAEROS.SULFAT  198 105 0 Surface aerosol desert (Stratospheric sulfate, to be locally defined GRIB)\nSURFA.OF.OZONE  248 105 0 SURFA OZONE. First ozone profile (A), locally defined GRIB\nSURFB.OF.OZONE  249 105 0 SURFB OZONE. Second ozone profile (B), locally defined GRIB\nSURFC.OF.OZONE  250 105 0 SURFC OZONE. Third ozone profile (C), locally defined GRIB\nPROFTEMPERATURE K 11 112 0 Soil temperature\nSURFCAPE.POS.F00 J kg-1 160 105 0 Convective available potential energy (CAPE)\nSURFCIEN.POS.F00 J kg-1 165 105 0 Convective inhibition (CIN)\nSURFLIFTCONDLEV m 167 105 0 Lifting condensation level (LCL)\nSURFFREECONVLEV m 168 105 0 Level of free convection (LFC)\nSURFEQUILIBRLEV m 169 105 0 Level of neutral buoyancy (LNB)\nPROFRESERV.EAU kg m-2 86 112 0 Soil Wetness\nPROFPROP.RMAX.EA kg m-2 238 112 0 Deep soil wetness\nPROFRESERV.GLACE kg m-2 193 112 0 Soil ice\nPROFRESERV.GLACE kg m-2 193 112 0 Soil ice","category":"page"},{"location":"ForecastModel/Outputlist/#D-variables-on-special-surfaces","page":"Output List","title":"2D variables on special surfaces","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"FA name Unit Parameter Type Level Note\nKT273ISOT_ALTIT m 8 20 27315 Altitude of 0-degree isotherm\nKT263ISOT_ALTIT m 8 20 26315 Altitude of -0-degree isotherm\nSURFISOTPW0.MAL m 8 5 0 Altitude of T'w=0 isotherm\nSURFTOT.WAT.VAPO kg m-2 54 200 0 Total column water vapour","category":"page"},{"location":"ForecastModel/Outputlist/#Postprocessed-variables-on-different-surface-types","page":"Output List","title":"Postprocessed variables on different surface types","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Through the postprocessing sofware fullpos HARMONIE offers a number of variables postprocessed on different surface types. For the current choice of variables, surfaces and levels please see scr/Select_postp.pl.","category":"page"},{"location":"ForecastModel/Outputlist/#State-variables-and-diagnostics-on-pressure-levels,-leveltype","page":"Output List","title":"State variables and diagnostics on pressure levels,  leveltype","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"FA name shortName iOP d pC pN stepType unit Description\nPNNNNNWIND.U.PHY u 33 0 2 2 instant m s**-1 u-component of wind\nPNNNNNWIND.V.PHY v 34 0 2 3 instant m s**-1 v-component of wind\nPNNNNNTEMPERATUR t 11 0 0 0 instant K Temperature\nPNNNNNHUMI.SPECI q 51 0 1 0 instant kg kg**-1 Specific humidity\nPNNNNNLIQUID_WAT cwat_cond 76 0 1 83 instant kg kg**-1 Specific cloud liquid water content\nPNNNNNSOLID_WATE ciwc_cond 58 0 1 84 instant kg kg**-1 Specific cloud ice water content\nPNNNNNCLOUD_FRAC tcc 71 0 6 192 instant 0-1 Total cloud cover\nPNNNNNSNOW snow_cond 184 0 1 86 instant kg kg**-1 Specific snow water content\nPNNNNNRAIN rain_cond 181 0 1 85 instant kg kg**-1 Specific rain water content\nPNNNNNGRAUPEL grpl_cond 201 0 1 32 instant kg kg**-1 Specific graupel\nPNNNNNGEOPOTENTI z 6 0 3 4 instant m2 s-2 Geopotential\nPNNNNNHUMI_RELAT r 52 0 1 192 instant 0-1 Relative humidity\nPNNNNNTHETA_PRIM papt 14 0 0 3 instant K Pseudo-adiabatic potential temperature\nPNNNNNVERT.VELOC w 40 0 2 9 instant m s**-1 Geometrical vertical velocity\nPNNNNNPOT_VORTIC pv 4 0 2 14 instant K m2 kg-1 s**-1 Potential vorticity\nPNNNNNABS_VORTIC absv 41 0 2 10 instant s**-1 Absolute vorticity","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"NNNNN is in Pascals. \nFrom FullPos documentation: \"Warning: fields on pressure levels bigger or equal to 1000 hPa are written out with truncated names; for example, temperature at 1000 hPa is P00000TEMPERATURE while P00500TEMPERATURE could be as well the temperature at 5 hPa or the temperature at 1005 hPa!\"","category":"page"},{"location":"ForecastModel/Outputlist/#State-variables-and-diagnostics-on-height-levels,-levelTypeheightAboveGround","page":"Output List","title":"State variables and diagnostics on height levels, levelType=heightAboveGround","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"FA name shortName iOP d pN pN stepType unit Description\nHNNNNNWIND.U.PHY u 33 0 2 2 instant m s**-1 u-component of wind\nHNNNNNWIND.V.PHY v 34 0 2 3 instant m s**-1 v-component of wind\nHNNNNNTEMPERATUR t 11 0 0 0 instant K Temperature\nHNNNNNLIQUID_WAT cwat_cond 76 0 1 83 instant kg kg**-1 Specific cloud liquid water content\nHNNNNNSOLID_WATE ciwc_cond 58 0 1 84 instant kg kg**-1 Specific cloud ice water content\nHNNNNNCLOUD_FRAC tcc 71 0 6 192 instant 0-1 Total cloud cover\nHNNNNNSNOW snow_cond 184 0 1 86 instant kg kg**-1 Specific snow water content\nHNNNNNRAIN rain_cond 181 0 1 85 instant kg kg**-1 Specific rain water content\nHNNNNNGRAUPEL grpl_cond 201 0 1 32 instant kg kg**-1 Specific graupel\nHNNNNNHUMI_RELAT r 52 0 1 192 instant 0-1 Relative humidity\nHNNNNNPRESSURE pres 1 0 3 0 instant Pa Pressure","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"NNNNN is in meters. ","category":"page"},{"location":"ForecastModel/Outputlist/#State-variables-and-diagnostics-on-PV-levels,-GRIB1-level-type-117","page":"Output List","title":"State variables and diagnostics on PV levels, GRIB1 level type 117","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"FA name Unit Par Typ Lev gl name Note\nVNNNGEOPOTENTI m2 s-2 6 117 nnn Geo.Pot. \nVNNNTEMPERATUR K 11 117 nnn Temp. \nVNNNWIND.U.PHY m s-1 33 117 nnn U-comp. \nVNNNWIND.V.PHY m s-1 34 117 nnn V-comp. \nVNNNVITESSE_VE Pa s-1 39 117 nnn !VertVel. DYNAMICS=h\nVNNNVERT.VELOC m s-1 40 117 nnn !VertVel. DYNAMICS=nh\nVNNNABS_VORTIC s-1 41 117 nnn A.Vort. \nVNNNDIVERGENCE s-1 44 117 nnn Div. \nVNNNHUMI.SPECI kg kg-1 51 117 nnn Sp.Hum. \nVNNNHUMI_RELAT % 52 117 nnn Rel.Hum. ","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"\"pv\" stream is not available by default\nNNN is in deci-PVU (1PVU = 1x10-6 K m2 kg-1 s-1) in FA files. PV levels must be in SI units in namelists\nGRIB1 levels are in milli-PVU. Currently gl does not convert devi-PVU (FA) to milli-PVU (GRIB1)","category":"page"},{"location":"ForecastModel/Outputlist/#State-variables-and-diagnostics-on-Theta-levels,-GRIB1-level-type-113","page":"Output List","title":"State variables and diagnostics on Theta levels, GRIB1 level type 113","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"FA name Unit Par Typ Lev gl name Note\nTNNNPOT_VORTIC s-1 4 113 nnn Pot. Vorticity \nTNNNGEOPOTENTI m2 s-2 6 113 nnn Geo.Pot. \nTNNNTEMPERATUR K 11 113 nnn Temp. \nTNNNWIND.U.PHY m s-1 33 113 nnn U-comp. \nTNNNWIND.V.PHY m s-1 34 113 nnn V-comp. \nTNNNVITESSE_VE Pa s-1 39 113 nnn !VertVel. DYNAMICS=h\nTNNNVERT.VELOC m s-1 40 113 nnn !VertVel. DYNAMICS=nh\nTNNNABS_VORTIC s-1 41 113 nnn A.Vort. \nTNNNDIVERGENCE s-1 44 113 nnn Div. \nTNNNHUMI.SPECI kg kg-1 51 113 nnn Sp.Hum. \nTNNNHUMI_RELAT % 52 113 nnn Rel.Hum. ","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"\"th\" stream is not available by default\nNNN is in Kelvin.","category":"page"},{"location":"ForecastModel/Outputlist/#FA-fields-without-any-default-GRIB1-translation","page":"Output List","title":"FA fields without any default GRIB1 translation","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Some very special fields are left without any default translation. Please see in the gl documentation on how to add you own translation.","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"FA name Unit Comment\nCUF1PRESSURE  Coupling error field.\nTHETAPWP_FLUX K m-4 s-1 Instantaneous thetaprimwprim surface flux\nCLPMOCON.MOD.XFU kg kg-1 s-1 MOCON model output\nATMONEBUL.TOTALE  Accumulated Total cloud cover.\nATMONEBUL.CONVEC  Accumulated Convective cloud cover.\nATMONEBUL.BASSE  Accumulated Low cloud cover.\nATMONEBUL.MOYENN  Accumulated Medium cloud cover.\nATMONEBUL.HAUTE  Accumulated High cloud cover.\nSURFCFU.Q.TURBUL  Accumulated contribution of Turbulence to Q.\nSURFCFU.CT.TURBUL  Accumulated contribution of Turbulence to CpT.\nSUNSHI. DURATION  Sunshine duration.\nSURFFL.U  TURBUL  Contribution of Turbulence to U.\nSURFFL.V  TURBUL  Contribution of Turbulence to V.\nSURFFL.Q  TURBUL  Contribution of Turbulence to Q.\nSURFFL.CT TURBUL  Contribution of Turbulence to CpT.\nSNNNSRC  Second order flux.","category":"page"},{"location":"ForecastModel/Outputlist/#Variables-postprocessed-by-gl","page":"Output List","title":"Variables postprocessed by gl","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"The following fields are can be generated by gl from a history file and are thus not necessarily available as FA fields.","category":"page"},{"location":"ForecastModel/Outputlist/#Single-level-fields","page":"Output List","title":"Single level fields","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Name Unit Par Typ Lev Note\nMSLPRESSURE Pa 1 103 0 MSLP. gl calculates MSLP independent of AROME/!FullPos\n# m 20 105 0 Visibility\n#  31 ttt lll Wind direction. gl calculates based on u[33,ttt,lll] and v[34,ttt,lll] wind components\n# m s-1 32 ttt lll Wind speed. gl calculates based on u[33,ttt,lll] and v[34,ttt,lll] wind components\n# kg m-2 61 105 0 Total precipitation (TP).  gl calculates TP![61,105,0]=rain![181,105,0]+snow![184,105,0]+graupel![201,105,0]+hail![204,105,0]\n# m 67 105 0 Boundary layer height\n# - 71 105 2 [[Color(Red, Fog)]]\n# ? 135 105 0 Icing index\n# K 136 105 0 Pseudo satellite image, cloud top temperature (infrared)\n# K 137 105 0 Pseudo satellite image, water vapour brightness temperature\n# K 138 105 0 Pseudo satellite image, water vapour br. temp. + correction for clouds\n# ? 139 105 0 Pseudo satellite image, cloud water reflectivity (visible)\n# ? 144 105 0 Precipitation type\n# kg m-2 185 105 0 Total solid precipitation.  gl calculates ![185,105,0]=snow![184,105,0]+graupel![201,105,0]+hail![204,105,0]\n# m/s 228 105 10 Wind gust speed","category":"page"},{"location":"ForecastModel/Outputlist/#Integrated-quantities","page":"Output List","title":"Integrated quantities","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"| Name | Unit   | Par | Typ | Lev | Note                                        |                                                         | | :–  | :–    | :– | :– | :– | :–                                         |                                                         | | #    | kg m-2 | 58  | 200 | 0   | Vertical integral of cloud ice              |                                                         | | #    | kg m-2 | 76  | 200 | 0   | Vertical integral of cloud water            |                                                         | | #    | ?      | 133 | 200 | 0   | Mask of significant cloud amount            |                                                         | | #    | J kg-1 | 160 | 200 | 0   | CAPE, comes in two flavours, capeversion=1 | 2 where the second is compatible with the ECMWF version | | #    | J kg-1 | 165 | 200 | 0   | CIN, comes in two flavours, capeversion=1  | 2 where the second is compatible with the ECMWF version | | #    | kg m-2 | 181 | 200 | 0   | Vertical integral of rain                   |                                                         | | #    | kg m-2 | 184 | 200 | 0   | Vertical integral of snow                   |                                                         | | #    | kg m-2 | 201 | 200 | 0   | Vertical integral of graupel                |                                                         | | #    | m      | 186 | 200 | 0   | Cloud base                                  |                                                         | | #    | m      | 187 | 200 | 0   | Cloud top                                   |                                                         | | #    | -      | 209 | 200 | 0   | [[Color(Red, Lightning)]]                   |                                                         |","category":"page"},{"location":"ForecastModel/Outputlist/#GRIB-encoding-information","page":"Output List","title":"GRIB encoding information","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#Time-units,-WMO-code-table-4","page":"Output List","title":"Time units, WMO code table 4","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"The following time units are used to encode GRIB edition 1 data","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Code Unit\n0 Minute\n1 Hour\n13 15 minutes\n14 30 minutes","category":"page"},{"location":"ForecastModel/Outputlist/#Time-range-indicator,-WMO-code-TABLE-5","page":"Output List","title":"Time range indicator, WMO code TABLE 5","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Code Definition\n0 Forecast product valid for reference time + P1 (P1 > 0), or Uninitialized analysis product for reference time (P1 = 0)\n2 Product with a valid time ranging between reference time + P1 and reference time + P2. Used for min/max values\n4 Accumulation (reference time + P1 to reference time + P2) product considered valid at reference time + P2","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Note that fields available as both instanteous and accumulated values like e.g. rain has the same parameter values and can only be distinguished by the time range indicator.","category":"page"},{"location":"ForecastModel/Outputlist/#Level-types,-WMO-Code-table-3","page":"Output List","title":"Level types, WMO Code table 3","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"level type WMO/HIRLAM type definition Units notes\n001 Ground or water surface 0 WMO\n002 Cloud base level 0 WMO\n004 Level of 0°C isotherm 0 WMO\n006 Maximum wind level 0 WMO\n007 Tropopause 0 WMO\n008 Top-of-atmosphere  WMO\n020 Isothermal level Temperature in 1/100 K WMO\n100 Isobaric level hPa WMO\n103 Specified altitude above mean sea level Altitude in m WMO\n105 Specified height above ground Altitude in m WMO\n107 Sigma level Sigma value in 1/10000 WMO\n109 Hybrid level  WMO\n113 Isentropic (theta) level Potential temperature in K WMO\n117 Potential vorticity surface 10-9 K m2 kg-1 s-1 WMO\n200 Entire atmosphere (considered as a single layer)  WMO, vertically integrated","category":"page"},{"location":"ForecastModel/Outputlist/#Harmonie-GRIB1-code-table-2-version-253-Indicator-of-parameter","page":"Output List","title":"Harmonie GRIB1 code table 2 version 253 - Indicator of parameter","text":"","category":"section"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Below the indicator of parameter code table for the Harmonie model. It is based on the WMO code table 2 version 3 with local parameters added. Parameter indicators 128-254 are reserved for originating center use. Parameter indicators 000-127 should not be altered. In HARMONIE, radiation fluxes are assumed positive downwards (against the recommendation by WMO).","category":"page"},{"location":"ForecastModel/Outputlist/","page":"Output List","title":"Output List","text":"Par Description SI Units\n000 Reserved n/a\n001 Pressure Pa\n002 Pressure reduced to MSL Pa\n003 Pressure tendency Pa s-1\n004 Potential vorticity K m2 kg-1 s-1\n005 ICAO Standard Atmosphere reference height m\n006 Geopotential m2 s-2\n007 Geopotential height gpm\n008 Geometrical height m\n009 Standard deviation of height m\n010 Total ozone Dobson\n011 Temperature K\n012 Virtual temperature K\n013 Potential temperature K\n014 Pseudo-adiabatic potential temperature K\n015 Maximum temperature K\n016 Minimum temperature K\n017 Dew-point temperature K\n018 Dew-point depression (or deficit) K\n019 Lapse rate K m-1\n020 Visibility m\n021 Radar spectra (1) -\n022 Radar spectra (2) -\n023 Radar spectra (3) -\n024 Parcel lifted index (to 500 hPa) K\n025 Temperature anomaly K\n026 Pressure anomaly Pa\n027 Geopotential height anomaly gpm\n028 Wave spectra (1) -\n029 Wave spectra (2) -\n030 Wave spectra (3) -\n031 Wind direction Degree true\n032 Wind speed m s-1\n033 u-component of wind m s-1\n034 v-component of wind m s-1\n035 Stream function m2 s-1\n036 Velocity potential m2 s-1\n037 Montgomery stream function m2 s-1\n038 Sigma coordinate vertical velocity s-1\n039 Vertical velocity Pa s-1\n040 Vertical velocity m s-1\n041 Absolute vorticity s-1\n042 Absolute divergence s-1\n043 Relative vorticity s-1\n044 Relative divergence s-1\n045 Vertical u-component shear s-1\n046 Vertical v-component shear s-1\n047 Direction of current Degree true\n048 Speed of current m s-1\n049 u-component of current m s-1\n050 v-component of current m s-1\n051 Specific humidity kg kg-1\n052 Relative humidity %\n053 Humidity mixing ratio kg kg-1\n054 Precipitable water kg m-2\n055 Vapor pressure Pa\n056 Saturation deficit Pa\n057 Evaporation kg m-2\n058 Cloud ice kg m-2\n059 Precipitation rate kg m-2 s-1\n060 Thunderstorm probability %\n061 Total precipitation kg m-2\n062 Large scale precipitation kg m-2\n063 Convective precipitation kg m-2\n064 Snowfall rate water equivalent kg m-2 s-1\n065 Water equivalent of accumulated snow depth kg m-2\n066 Snow depth m\n067 Mixed layer depth m\n068 Transient thermocline depth m\n069 Main thermocline depth m\n070 Main thermocline anomaly m\n071 Total cloud cover %\n072 Convective cloud cover %\n073 Low cloud cover %\n074 Medium cloud cover %\n075 High cloud cover %\n076 Cloud water kg m-2\n077 Best lifted index (to 500 hPa) K\n078 Convective snow kg m-2\n079 Large scale snow kg m-2\n080 Water temperature K\n081 Land cover (1 = land, 0 = sea) Proportion\n082 Deviation of sea level from mean m\n083 Surface roughness m\n084 Albedo %\n085 Soil temperature K\n086 Soil moisture content kg m-2\n087 Vegetation %\n088 Salinity kg kg-1\n089 Density kg m-3\n090 Water run-off kg m-2\n091 Ice cover (1 = ice, 0 = no ice) Proportion\n092 Ice thickness m\n093 Direction of ice drift Degree true\n094 Speed of ice drift m s-1\n095 u-component of ice drift m s-1\n096 v-component of ice drift m s-1\n097 Ice growth rate m s-1\n098 Ice divergence s-1\n099 Snow melt kg m-2\n100 Significant height of combined wind waves and swell m\n101 Direction of wind waves Degree true\n102 Significant height of wind waves m\n103 Mean period of wind waves s\n104 Direction of swell waves Degree true\n105 Significant height of swell waves m\n106 Mean period of swell waves s\n107 Primary wave direction Degree true\n108 Primary wave mean period s\n109 Secondary wave direction Degree true\n110 Secondary wave mean period s\n111 Net short-wave radiation flux (surface) W m-2\n112 Net long-wave radiation flux (surface) W m-2\n113 Net short-wave radiation flux (top of atmosphere) W m-2\n114 Net long-wave radiation flux (top of atmosphere) W m-2\n115 Long-wave radiation flux W m-2\n116 Short-wave radiation flux W m-2\n117 Global radiation flux W m-2\n118 Brightness temperature K\n119 Radiance (with respect to wave number) W m-1 sr-1\n120 Radiance (with respect to wave length) W m-3 sr-1\n121 Latent heat flux W m-2\n122 Sensible heat flux W m-2\n123 Boundary layer dissipation W m-2\n124 Momentum flux, u-component N m-2\n125 Momentum flux, v-component N m-2\n126 Wind mixing energy J\n127 Image data -\n128 Analysed RMS of PHI (CANARI) m2 s-2\n129 Forecasted RMS of PHI (CANARI) m2 s-2\n130 SW net clear sky rad W m-2\n131 LW net clear sky rad W m-2\n132 Latent heat flux through evaporation W m-2\n133 Mask of significant cloud amount #\n134 Available #\n135 Icing index Code table\n136 Pseudo satellite image, cloud top temperature (infrared) K\n137 Pseudo satellite image, water vapour brightness temperature K\n138 Pseudo satellite image, water vapour br. temp. + correction for clouds K\n139 Pseudo satellite image, cloud water reflectivity (visible) ?\n140 Direct normal irradiance J m-2\n141 Available #\n142 Available #\n143 Available #\n144 Precipition Type Code table\n145 Available #\n146 Available #\n147 Available #\n148 Available #\n149 Available #\n150 Available #\n151 Available #\n152 Available #\n153 Available #\n154 Available #\n155 Available #\n156 Available #\n157 Available #\n158 Surface downward moon radiation W m-2\n159 Available #\n160 CAPE J kg-1\n161 AROME hail diagnostic %\n162 U-momentum of gusts out of the model m s-1\n163 V-momentum of gusts out of the model m s-1\n164 Available #\n165 Convective inhibition (CIN) J kg-1\n166 MOCON out of the model kg/kg s-1\n167 Lifting condensation level (LCL) m\n168 Level of free convection (LFC) m\n169 Level of neutral boyancy (LNB) m\n170 Brightness temperature OZ clear K\n171 Brightness temperature OZ cloud K\n172 Brightness temperature IR clear K\n173 Brightness temperature IR cloud K\n174 Brightness temperature WV clear K\n175 Brightness temperature WV cloud K\n176 Virtual potential temperature K\n177 Available #\n178 Available #\n179 Available #\n180 Available #\n181 Rain kg m-2\n182 Stratiform Rain kg m-2\n183 Convective Rain kg m-2\n184 Snow kg m-2\n185 Total solid precipitation kg m-2\n186 Cloud base m\n187 Cloud top m\n188 Fraction of urban land Proportion\n189 Available #\n190 Snow Albedo Proportion\n191 Snow density kg/m3\n192 Water on canopy kg/m2\n193 Soil ice kg/m2\n194 Available #\n195 Gravity wave stress U-comp N/m2\n196 Gravity wave stress V-comp N/m2\n197 Available #\n198 Available #\n199 Vegetation type -\n200 TKE m2 s-2\n201 Graupel kg m-2\n202 Stratiform Graupel kg m-2\n203 Convective Graupel kg m-2\n204 Hail kg m-2\n205 Stratiform Hail kg m-2\n206 Convective Hail kg m-2\n207 Available #\n208 Available #\n209 Lightning -\n210 Simulated reflectivity dBz\n211 Available #\n212 Pressure departure Pa\n213 Vertical divergence s-1\n214 UD_OMEGA ms-1?\n215 DD_OMEGA ms-1?\n216 UDMESHFRAC -\n217 DDMESHFRAC -\n218 PSHICONVCL -\n219 Surface albedo for non snow covered areas Proportion\n220 Standard deviation of orography * g J/Kg\n221 Anisotropy coeff of topography -\n222 Direction of main axis of topography rad\n223 Roughness length of bare surface * g m2 s-2\n224 Roughness length for vegetation * g m2 s-2\n225 Fraction of clay within soil Proportion\n226 Fraction of sand within soil Proportion\n227 Maximum proportion of vegetation Proportion\n228 Gust wind speed m s-1\n229 Albedo of bare ground Proportion\n230 Albedo of vegetation Proportion\n231 Stomatal minimum resistance s/m\n232 Leaf area index m2/m2\n233 Thetaprimwprim surface flux Km/s\n234 Dominant vegetation index -\n235 Surface emissivity -\n236 Maximum soil depth m\n237 Soil depth m\n238 Surface liquid water content kg/m2\n239 Thermal roughness length * g m2 s-2\n240 Resistance to evapotransiration s/m\n241 Minimum relative moisture at 2 meters %\n242 Maximum relative moisture at 2 meters %\n243 Duration of total precipitations s\n244 Latent Heat Sublimation W/m2\n245 Water evaporation kg/m2\n246 Snow sublimation kg/m2\n247 Snow history ???\n248 A OZONE ???\n249 B OZONE ???\n250 C OZONE ???\n251 Surface aerosol sea ???\n252 Surface aerosol land ???\n253 Surface aerosol soot ???\n254 Surface aerosol desert ???\n255 Missing value n/a","category":"page"},{"location":"DataAssimilation/Screening/#Screening","page":"Screening","title":"Screening","text":"","category":"section"},{"location":"DataAssimilation/Screening/#Introduction","page":"Screening","title":"Introduction","text":"","category":"section"},{"location":"DataAssimilation/Screening/","page":"Screening","title":"Screening","text":"Screening (configuration 002 of ARPEGE/IFS model) carries out quality control of observations. ","category":"page"},{"location":"DataAssimilation/Screening/","page":"Screening","title":"Screening","text":"A useful presentation (Martin Ridal) from the \"Hirlam-B Training Week on HARMONIE system\" training course is available here: MR_screenandminim.pdf. Most of the information on this page is based on his presentation.","category":"page"},{"location":"DataAssimilation/Screening/#Inputs","page":"Screening","title":"Inputs","text":"","category":"section"},{"location":"DataAssimilation/Screening/","page":"Screening","title":"Screening","text":"First guess (the same file with 5  different names):\nICMSHMIN1INIT\nICMSHMIN1IMIN\nICMRFMIN10000\nELSCFMIN1ALBC000\nELSCFMIN1ALBC\nInput/output ODB directory structure\n${d_DB}/ECMA\n${d_DB}/ECMA.${base1}\nConstants and statistics (MAY NEED TO BE UPDATED)\ncorrel.dat\nsigmab.dat\nrszcoef_fmt\nerrgrib\nrt_coef_atovs_newpred_ieee.dat\nbcor_noaa.dat\nchanspec_noaa.dat\nrmtberr_noaa.dat\ncstlim_noaa.dat\nNamelist: See %screening in nam/harmonie_namelists.pm","category":"page"},{"location":"DataAssimilation/Screening/#Screening-tasks","page":"Screening","title":"Screening tasks","text":"","category":"section"},{"location":"DataAssimilation/Screening/","page":"Screening","title":"Screening","text":"(Based on Martin Ridal's presentation).","category":"page"},{"location":"DataAssimilation/Screening/","page":"Screening","title":"Screening","text":"Preliminary check of observations\nCheck of completeness of the reports\nCheck if station altitude is present\nCheck of the reporting practice for SYNOP & TEMP mass observations \nBlacklisting: A blacklist is applied to discard observations of known poor quality and/or that cannot be properly handled by the data assimilation. A selection of variables for assimilation is done using the data selection part of the blacklist file and the information hard-coded in Arpege/Aladin (orographic rejection limit, land-sea rejection...). Decisions based on the blacklist are feedback to the CMA. Blacklisting is defined in src/bla/mf_blacklist.b \nBackground quality control: flags are assigned to observations – 1 =>  probably correct, 2 => probably incorrect, 3 => incorrect.\nVertical consistency of multilevel report:\nThe duplicated levels, in multi-level reports, are removed from the reports\nIf 4 consecutive layers are found to be of suspicious quality then these layers are rejected\nRemoval of duplicated reports\nIn case of co-located airep reports of the same observation types (time, position), some or all of the content of one of the reports is rejected\nRedundancy check\nperformed for active reports that are co-located and originate from the same station\nLAND SYNOP: the report closest to the centre of the screening time window with most active data is retained\nSHIP SYNOP: redundant if the moving platforms are within a circle of 1^o^ radius src/arpifs/obs_preproc/sufglim.F90 RSHIDIS = 111000._JPRB\nTEMP and PILOT: same stations are considered at the same time in the redundancy check\nA SYNOP mass observation is redundant if there are any TEMP geopotential height observations (made in the same time and the same station) that are no more than 50hPa above the SYNOP mass observation\nThinning: High resolution data needs to be reduced to reduce correlated errors and reduce the amount of data","category":"page"},{"location":"DataAssimilation/Screening/#Output","page":"Screening","title":"Output","text":"","category":"section"},{"location":"DataAssimilation/Screening/","page":"Screening","title":"Screening","text":"The quality control information will be put into the input ECMA ODB(s) and a newly created CCMA to used by the 3DVAR minimization.","category":"page"},{"location":"DataAssimilation/Screening/","page":"Screening","title":"Screening","text":"A valuable summary about screening decisions can be found in HM_Date_YYYYMMDDHH.html:","category":"page"},{"location":"DataAssimilation/Screening/","page":"Screening","title":"Screening","text":"Look for “SCREENING STATISTICS” to get:\nSTATUS summary\nEVENT summary\nNumber of variables, departures and missing departures\nDiagnostic JO-table\nCCMA ODB and updated ECMA ODB","category":"page"},{"location":"DataAssimilation/Screening/","page":"Screening","title":"Screening","text":"Screening Events listed under \"EVENT SUMMARY OF REPORTS:\"","category":"page"},{"location":"DataAssimilation/Screening/","page":"Screening","title":"Screening","text":" Description\n1 NO DATA IN THE REPORT\n2 ALL DATA REJECTED\n3 BAD REPORTING PRACTICE\n4 REJECTED DUE TO RDB FLAG\n5 ACTIVATED DUE TO RDB FLAG\n6 ACTIVATED BY WHITELIST\n7 HORIZONTAL POSITION OUT OF RANGE\n8 VERTICAL POSITION OUT OF RANGE\n9 TIME OUT OF RANGE\n10 REDUNDANT REPORT\n11 REPORT OVER LAND\n12 REPORT OVER SEA\n13 MISSING STATION ALTITUDE\n14 MODEL SUR. TOO FAR FROM STAT. ALT.\n15 REPORT REJECTED THROUGH THE NAMELIST\n16 FAILED QUALITY CONTROL","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#HARMONIE-Vertical-Model-Level-Definitions","page":"Vertical Grid","title":"HARMONIE Vertical Model Level Definitions","text":"","category":"section"},{"location":"ExperimentConfiguration/VerticalGrid/#HARMONIE-vertical-coordinate","page":"Vertical Grid","title":"HARMONIE vertical coordinate","text":"","category":"section"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"HARMONIE model, similar to that of HIRLAM, is constructed for a general pressure based and terrain following vertical coordinate eta(pp_s), where","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"eta(0P_s) = 0","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"and ","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"eta(p_sp_s) = 1","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"The formulation corresponds to the ECMWF hybrid system. The model is formulated for a spherical coordinate system (lambda, theta), but in the code two metric coefficients (h_xh_y) have been introduced. This is done to prepare the model for any orthogonal coordinate system or map projection with axes (x,y). ","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"To represent the vertical variation of the dependent variables (U, V, T and Q), the atmosphere is divided into \"nlev\" layers. These layers are defined by the pressures at the interfaces between them (the `half-levels'). From the general expression","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"p_k+12 = A_k+12 (n) + B_k+12(n) * p_s(xy)","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"for  k=01nlev","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"the vertical surfaces for half-levels are defined. Pure pressure surfaces are obtained for B=0 and pure sigma surfaces for A=0. `full-level' pressure associated with each model level (middle of two half layers) is then determined accordingly.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#Definition-of-model-levels-in-HARMONIE","page":"Vertical Grid","title":"Definition of model levels in HARMONIE","text":"","category":"section"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"The script src/Vertical_levels.pl contains definition of vertical levels that have been used in the HIRLAM community for research and/or operational purposes. Currently the default model setup defines 65-level structure as derived by Per Unden, SMHI. Model level definitions for commonly used vertical structures in HARMONIE are listed below.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"FourtyLevel: HIRLAM_40 model levels (same as Hirlam 6.2.1, Nov 2003 - HIRLAM 7.0, 2006 )\nSixtyLevel: HIRLAM-60 model levels (same as Hirlam 7.1, March 2007 - 2012 )\n[wiki:MFSixtyLevel MF_60]: MF-60 model levels (same as Meteo France AROME since 2010 )\nSixtyfiveLevel: 65 model levels (same as Hirlam 7.4, March 2012 - )\nother levels: Prague87, MF70, 40 (ALADIN-40), ECMWF_60.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"Note that VLEV is the name of the set of A/B coefficients defining your levels set in ecf/config_exp.h. There are e.g. more than one definition for 60 levels. To print the levels just run  scr/Vertical_levels.pl","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"Usage: scr/Vertical_levels.pl [VLEV PRINT_OPTION] where:","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"VLEV: name of your level definition\nPRINT_OPTION=AHALF: print A coefficients for VLEV\nPRINT_OPTION=BHALF: print B coefficients for VLEV\nPRINT_OPTION=NLEV: print number of levels for VLEV\nPRINT_OPTION=NRFP3S: print NRFP3S namelist values for VLEV","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"For reference, we provide links detailing structure of the ECMWF 62 level (ensemble and seasonal forecast),  91 level (deterministic forecast) and the 137-level deterministic forecast (starting June 25 2013, 38r2)]","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"When performing HARMONIE experiment, users can select vertical levels by changing VLEV in ecf/config_exp.h. If a non-standard level number is to be chosen, the script scr/Vertical_levels.pl needs to be edited to add layer definition.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#Define-new-eta-levels","page":"Vertical Grid","title":"Define new eta levels","text":"","category":"section"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"A brief description and some code on how to create new eta levels can be found in here.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"There is also an interactive tool that can help you in creating a new set of levels.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"(Image: )","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"The method is based on a program by Pierre Bénard, Meteo France, that is described in this gmapdoc article.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#Relevant-corresponding-data-set-for-different-vertical-structure","page":"Vertical Grid","title":"Relevant corresponding data set for different vertical structure","text":"","category":"section"},{"location":"ExperimentConfiguration/VerticalGrid/","page":"Vertical Grid","title":"Vertical Grid","text":"HARMONIE 3D-VAR and 4DVAR upper air data assimilation needs background error structure function for each given vertical layer structure. It is noted that the structure function data included in the reference HARMONIE repository const/jb_data is only useful for reference configuration. Users that runs 3DVAR/4DVAR are strongly recommended to derive proper structure function data following instructions in the Harmonie wiki using own data archive to avoid improper use of structure function.","category":"page"},{"location":"Overview/Binaries/#HARMONIE-binaries","page":"Binaries","title":"HARMONIE binaries","text":"","category":"section"},{"location":"Overview/Binaries/","page":"Binaries","title":"Binaries","text":"An installation of HARMONIE produces the following binaries:","category":"page"},{"location":"Overview/Binaries/","page":"Binaries","title":"Binaries","text":"ACADFA1D : Tool to generate initial and boundary data for MUSC\nADDPERT : Create initial perturbations\nADDSURF : Allows you to mix different files and add different fields\nALTO : Also known as PINUTS. Contains several diagnostic tools.\nBATOR : Generate ODB from observations in various formats\nbl95.x : Blacklist compiler, help program to generate object files from the blacklist\nBLEND : Mixes to files\nBLENDSUR : Mixes to files\ncluster : Cluster ensemble members\nCONVERT_ECOCLIMAP_PARAM : Generate binary files from ECOCLIMAP ascii files\ndcagen : ODB handling tool\ndomain_prop : Helper program to return various model domain properties\nFESTAT : Background error covariance calculations.\nfldextr : Extracts data for verification from model history files. Reads FA from HARMONIE and GRIB from ECMWF/HIRLAM.\ngl : Converts/interpolates between different file formats and projections. Used for boundary interpolation.\nIOASSIGN/ioassign : ODB IO setup\nLSMIX : Scale dependent mixing of two model states.\njbconv : Interpolates/extrapolates background error statistics files. For technical experimentation\nlfitools : FA/LFI file manipulation tool\nMASTERODB : The main binary for the forecast model, surface assimilation, climate generation, 3DVAR, fullpos and much more.\nMTEN : Computation of moist tendencies\nobsextr : Extract data for verification from BUFR files. \nobsmon : Extract data for observation monitoring\nodb98.x : ODB manipulation program\nOFFLINE : The SURFEX offline model. Also called SURFEX\noulan : Converts observations in BUFR to OBSOUL format used by BATOR\nPERTCMA : Perturbation of observations in ODB\nPERTSFC : Surface perturbation scheme\nPGD : Generates physiography files for SURFEX.\nPREGPSSOL : Processing of GNSS data\nPREP : Generate SURFEX initial files. Interpolates/translates between two SURFEX domains.\nSFXTOOLS : Converts SURFEX output between FA and LFI format.\nshuffle : Manipulation of ODB. Also called ODBTOOLS\nShuffleBufr : Split bufr data according to observation type, used in the observation preprocessing.\nSODA : Surfex offline data assimilation\nSPG : Stochastic pattern generator, https://github.com/gayfulin/SPG\nSURFEX : The SURFEX offline model. Also called OFFLINE\ntot_energy : Calculates the total energy of a model state. Is used for boundary perturbation scaling.\nxtool : Compares two FA/LFI/GRIB files.","category":"page"},{"location":"EPS/Calibration/#ENSEMBLE-CALIBRATION","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"","category":"section"},{"location":"EPS/Calibration/#Introduction","page":"ENSEMBLE CALIBRATION","title":"Introduction","text":"","category":"section"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"In probabilistic forecasting the unavoidable forecast uncertainties are quantified and the aim is to estimate the probability forecast distribution based on information available at the time the forecast is to be issued. There are errors in the raw NWP ensemble distribution (both in the mean and the variance), and the systematic errors can be corrected by statistical post-processing, also called calibration. Attention is paid here to use predictors from HarmonEPS as a source of predictive information. ","category":"page"},{"location":"EPS/Calibration/#Statistical-methods","page":"ENSEMBLE CALIBRATION","title":"Statistical methods","text":"","category":"section"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"The book of Vannitsem et al. (2018) gives a good overview of the ensemble calibration techniques that have been developed and used since the early 2000's. Here we describe a number of distributional regression and non-parametric methods only briefly together with the R packages that can be used. For most distributional regression methods the R package gamlss can be used, unless mentioned otherwise.","category":"page"},{"location":"EPS/Calibration/#Distributional-regression","page":"ENSEMBLE CALIBRATION","title":"Distributional regression","text":"","category":"section"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"Normal. The well-known normal or Gaussian distribution can be used to calibrate ensemble temperature forecasts. It has 2 parameters: the mean and the standard deviation. \nTruncated normal. The truncated normal distribution is truncated at a specific value, e.g. at zero for wind speed as values < 0 are not physically possible. It also has 2 parameters: the mean and the standard deviation. \nLog-normal\nGamma\nZero-adjusted Gamma distribution\nBox-Cox t-distribution. The Box-Cox t-distribution has four parameters which are related to the location, scale, skewness and kurtosis, but in a composite way. Roughly, mu is the mean or median, sigma is the mean/sd, and nu is (mean-median)/sd. \nmixture Gamma\nmixture normal","category":"page"},{"location":"EPS/Calibration/#Non-parametric-methods","page":"ENSEMBLE CALIBRATION","title":"Non-parametric methods","text":"","category":"section"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"quantile regression. In quantile regression quantiles are modeled separately as functions of the covariates. Note that unless special care is taken quantiles may cross, but in any case they can be sorted afterwards. The R package quantreg can be used to fit quantiles.\nquantile random/regression forests. The R package ranger is an efficient package to fit quantile random/regression forests.\ngeneralized random forests. The R package grf can be used to fit generalized random forests. \ngradient boosting. The R package gbm can be used to fit gradient boosting trees.","category":"page"},{"location":"EPS/Calibration/#Experiments","page":"ENSEMBLE CALIBRATION","title":"Experiments","text":"","category":"section"},{"location":"EPS/Calibration/#Construction-of-spatial-features","page":"ENSEMBLE CALIBRATION","title":"Construction of spatial features","text":"","category":"section"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"Spatial features are variables describing the topography. In statistical calibration their main usage is to explain possible spatial variations in the relation between the NWP model and observations. A good basis for defining features are land type and elevation data. For the European domain the Corine Land Cover data and the EU DEM 3035 data are suitable. The former classify the land type into more than 40 categories with a spatial resolution of about 140 meter. The land type classes can either be used directly as covariates in statistical models or clustered before further use. The elevation data have a spatial resolution of about 25 meter and are useful as they are. In addition, further processing can create variables describing peaks, ridges, plateaus etc. gdalwarp and gdaldem are useful tools to work on these data. Here are some examples:","category":"page"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"Change of projection and domain to the MEPS (old):","category":"page"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"gdalwarp -te -922442.2 -1129322 922557.8 1240678 -t_srs \"+proj=lcc +lat_0=63 +lon_0=15 +lat_1=63 +lat_2=63 +no_defs +R=6.371e+06\" $PATH_DATA/eudem_dem_3035_europe.tif $PATH_DATA/eudem_dem_3035_meps.tif","category":"page"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"Change of resolution to 250 m:","category":"page"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"gdalwarp -r near -tr 250 250 $PATH_DATA/eudem_dem_3035_meps.tif $PATH_DATA/eudem_dem_3035_meps_250m_nearest.tif","category":"page"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"Computation of the topographical position index:","category":"page"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"gdaldem TPI $PATH_DATA/eudem_dem_3035_meps_250m_nearest.tif $PATH_DATA/eudem_dem_3035_meps_250m_nearest_TPI.tif","category":"page"},{"location":"EPS/Calibration/#Calibration-of-wind-speed","page":"ENSEMBLE CALIBRATION","title":"Calibration of wind speed","text":"","category":"section"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"The skill of a number of (empirical) distributions to calibrate MEPS wind speed over Denmark has been investigated by Ioannidis et al. (2018). Global models have been fitted and the truncated normal distribution turned out to be the most skillful parametric distribution and it was compared to quantile regression forests. The latter was less skillful for the larger thresholds, probably because of the relatively small training data set (only 2 months per season). For the mean of the truncated normal distribution the MEPS mean and land type from the 140-m resolution Corine land cover data set were selected most often, and for the standard deviation the MEPS standard deviation and the land type. In terms of the Brier skill score the truncated normal distribution shows skill until thresholds of at least 20 m/s in winter. More details can be found in Ioannidis et al. (2018). ","category":"page"},{"location":"EPS/Calibration/#References","page":"ENSEMBLE CALIBRATION","title":"References","text":"","category":"section"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"Ioannidis, E., K. Whan and M. Schmeits, 2018: Probabilistic Wind Speed Forecasting using Parametric and non-parametric Statistical Post-Processing Methods. KNMI internal report, to appear.","category":"page"},{"location":"EPS/Calibration/","page":"ENSEMBLE CALIBRATION","title":"ENSEMBLE CALIBRATION","text":"Vannitsem, S., D. Wilks and J. Messner (editors), 2018: Statistical Postprocessing of Ensemble Forecasts. Elsevier, 362 pp.   ","category":"page"},{"location":"Observations/Amv/#Atmospheric-Motion-Vectors-(AMV)","page":"AMV","title":"Atmospheric Motion Vectors (AMV)","text":"","category":"section"},{"location":"Observations/Amv/#Introduction","page":"AMV","title":"Introduction","text":"","category":"section"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"In this short information about the (pre-)processing, assimilation, and post-processing, as well as access to the AMV data in the Harmonie system is shown.","category":"page"},{"location":"Observations/Amv/#AMV-data","page":"AMV","title":"AMV data","text":"","category":"section"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"AMV data is available via EUMETCast (the EUMETAT processed), the MARS archive at ECMWF (both polar and geowind) or locally using NWCSAF software. Through the EUMETCast, both data are in BUFR format. An abstract from the 5th Winds Workshop on the quality control of EUMETSAT wind products (S2-3_Elliott-Parallel.pdf) provides some useful information on how AMV BUFR is encoded. We define two kinds of AMV data in the Harmonie system: Geostationary satellite based (GEOW) and polar satellite based (POLW). GEOW and POLW can be processed separately through the usual request in scr/include.ass as described below.","category":"page"},{"location":"Observations/Amv/#HARMONIE-changes","page":"AMV","title":"HARMONIE changes","text":"","category":"section"},{"location":"Observations/Amv/#scr/include.ass","page":"AMV","title":"scr/include.ass","text":"","category":"section"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"In scr/include.ass should be edited to \"switch on\" the use of AMVs (SATOB/geowinds):","category":"page"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"export GEOW_OBS=1               # Satob geowind / SAFNWC geowind\nexport GEOW_SOURCE=ears         # mars:MARS | else: file in $OBDIR\n[[  $GEOW_OBS -eq 1  ]] && types_BASE=\"$types_BASE geow\"\n\nexport POLW_OBS=1               # Polar winds\nexport POLW_SOURCE=ears         # mars:MARS | else: file in $OBDIR\n[[  $POLW_OBS -eq 1  ]] && types_BASE=\"$types_BASE polarw\"","category":"page"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"Important: Please note that data from MARS were not yet tested!!! Roger is happy to assist if you prefer to use this option. The Harmonie system is updated with the aim to be able to use these data in operational application.","category":"page"},{"location":"Observations/Amv/#param.cfg","page":"AMV","title":"param.cfg","text":"","category":"section"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"The BUFR template used by your AMV data should be defined in the param.cfg file used by Bator. param.cfg files for Bator are in the const/bator_param directory. The geowind param.cfg template should be something like this:","category":"page"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"BEGIN geowind\nA 0 4 18\ncodage       1  aaaaaa\n  :          :    :\ncodage       A  aaaaaa\noffset       1      24  offset for QI_3 (eumetsat & tokyo)\noffset       2      28  offset for QI_1 (NOAA)\noffset       3      14  offset for QI_2 (NOAA)\noffset       4      48  offset for QI_2 (eumetsat & tokyo)\nvalues       1  001007  SATELLITE IDENTIFIER\nvalues       2  001031  IDENTIFICATION OF ORIGINATING/GENERATING CENTRE (SEE NOTE 10)\nvalues       4  002221  SEGMENT SIZE AT NADIR IN X DIRECTION\nvalues       5  002222  SEGMENT SIZE AT NADIR IN Y DIRECTION\nvalues       6  004001  YEAR\nvalues      12  005001  LATITUDE (HIGH ACCURACY)\nvalues      13  006001  LONGITUDE (HIGH ACCURACY)\nvalues      14  002252  SATELLITE INSTRUMENT DATA USED IN PROCESSING\nvalues      15  002023  SATELLITE DERIVED WIND COMPUTATION METHOD\nvalues      16  007004  PRESSURE\nvalues      17  011001  WIND DIRECTION\nvalues      18  011002  WIND SPEED\nvalues      21  012193  COLDEST CLUSTER TEMPERATURE\nvalues      22  002231  HEIGHT ASSIGNMENT METHOD\nvalues      23  002232  TRACER CORRELATION METHOD\nvalues      24  008012  LAND/SEA QUALIFIER\nvalues      25  007024  SATELLITE ZENITH ANGLE\nvalues     211  033007  % CONFIDENCE\nEND geowind","category":"page"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"Please be reminded that the processing of data from MARS was not yet tested. From 43h2.1, we have the all necessary content of the param file for processing of both GEOW and POLW in const/bator_param/param_bator.cfg.geow.${GEOW_SOURCE/POLW_SOURCE}","category":"page"},{"location":"Observations/Amv/#BATOR-namelist","page":"AMV","title":"BATOR namelist","text":"","category":"section"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"Depending on the satellite and channel you may have to add entries to the NADIRS namelist in the Bator script like the following:","category":"page"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"TS_GEOWIND(isatid)%T_SELECT%LCANAL(ichanal)=.TRUE.,","category":"page"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"Satellite identifiers are available here: [https://software.ecmwf.int/wiki/display/ECC/WMO%3D27+code-flag+table]\nBator defaults for MSG AMV data are set in src/odb/pandor/module/batorinitmod.F90","category":"page"},{"location":"Observations/Amv/#Source-code","page":"AMV","title":"Source code","text":"","category":"section"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"The reading of BUFR AMVs is taken care of by src/odb/pandor/module/bator_decodbufr_mod.F90. This subroutine reads the following parameters defined in the param.cfg file:","category":"page"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"Name Description\nDate and time derived from the tconfig(004001) - assumes month, day, hour and minute are in consecutive entries in the values array\nLocation latitude and longitude are read from tconfig(005001) and tconfig(006001)\nSatellite the satellite identifier is read from tconfig(001007)\nOrigin. center the originating center (of the AMV) is read from tconfig(001031)\nCompu. method the wind computation method (type of channel + cloudy/clear if WV) is read from tconfig(002023)\nDerivation method the height assignment method is read from tconfig(002163) and the tracking method from tconfig (002164)\nChannel frequency the centre frequency of the satellite channel is read from tconfig(002153)\nHeight (pressure) the height of the AMV observation is read from tconfig(007004)\nWind the wind speed and direction  are read from tconfig(011002) and tconfig(011001)\nTemperature the coldest cluster temperature is read from tconfig(012071)\nFG QI The QI (including FG consistency) for MSG AMVs is read from the first location where descriptor 033007 appears\nnoFG-QI The FG-independent QI for MSG AMVs is read from the first location where 033007 appears + offset(1)=24\nSat zenith angle the satellite zenith angle is read from tconfig(007024)\nLand/sea/coast a land/sea/coast qualifier is read from tconfig(008012)","category":"page"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"The geowind routine was adapted to handle MSG AMVs from MARS and its module /src/odb/pandor/module/bator_decodbufr_mod.F90 uploaded to the trunk (Mar 2017) .","category":"page"},{"location":"Observations/Amv/#Blacklist","page":"AMV","title":"Blacklist","text":"","category":"section"},{"location":"Observations/Amv/","page":"AMV","title":"AMV","text":"The selection/blacklist of AMVs according to channel, underlying sea/land, QI, etc. is done in src/blacklist/mf_blacklist.b, section - SATOB CONSTANT DATA SELECTION -.","category":"page"},{"location":"PostProcessing/xtool/#Post-processing-with-xtool","page":"xtool","title":"Post processing with xtool","text":"","category":"section"},{"location":"PostProcessing/xtool/#xtool","page":"xtool","title":"xtool","text":"","category":"section"},{"location":"PostProcessing/xtool/","page":"xtool","title":"xtool","text":"Xtool, part of the gl package, provides a utility to calculate differences between GRIB/FA files and produce the result in a new GRIB file. See xtool part of util/gl/README. The main commands are:","category":"page"},{"location":"PostProcessing/xtool/","page":"xtool","title":"xtool","text":" \n                          xtool                                 \n \n Simple usage:                                                  \n  xtool -f1 FILE1 [ -fN FILEN] -op OPERATOR [ -o OFILE]       \n  i.e. apply OPERATOR on FILEN and output to OFILE              \n  operator is one of SUM/DIFF/AVE/STDV/RMS/MS                      \n  1 file : SUM/AVE/SQR                                          \n  2 files: SUM/AVE/PROD/DIFF/RMSE/STDV/SAL                      \n  3 files: DIFF/SAL                                             \n \n Accumulated usage:                                             \n  xtool -sdtg1 YYYYMMDDHH -edtg1 YYYYMMDDHH -ll1 LLL -ll2 LLL \\ \n        -p1 PATH1 -p2 PATH2 -fcint HH -op OPERATOR              \n  i.e. accumulate OPERATOR on FILE1 and FILE2 during the period \n  sdtg1 to edtg1 with step of fcint.                            \n  sdtg2/edtg2=sdtg1/edtg2 - (LL1-LL2) unless given              \n \n Time information in the path should be given as                \n  -p1 /data/test/fc@YYYY@@MM@@DD@@HH@+@LLL@ or something like   \n  -p2 /data/@YYYY@/@MM@/@DD@/@HH@/fc@YYYY@@MM@@DD@_@HH@+@LLL@   \n \n Different parameters can be compared by using the xkey         \n variable in the namelist                                       \n \n Flag summary :                                                 \n  -h                : Print this help                           \n  -fN FILE          : Single file name                          \n  -pN PATH          : Path name                                 \n  -sdtgN YYYYMMDDHH : Start date/time                           \n  -edtgN YYYYMMDDHH : End date/time                             \n  -llN LLL          : Forecast length to use                    \n  -fcint HH         : Forecast cycle interval in hours          \n  -iN               : format of the input file GRIB/FA          \n  -s                : Run silent                                \n  -of               : Output format GRIB/FA/SCREEN              \n  -g                : Prints ksec/cadre/lfi info                \n  -p                : Use FA file without extension zone        \n  -f                : Global FA switch                          \n  -a FILE           : Accumulation file to be read              \n  -n namelist       : Use namelist to set additional options    \n  -igd              : Set lignore_duplicates=F                  \n  -r VALUE          : Rescale the output with a constant VALUE  \n ","category":"page"},{"location":"PostProcessing/xtool/#DIFF","page":"xtool","title":"DIFF","text":"","category":"section"},{"location":"PostProcessing/xtool/","page":"xtool","title":"xtool","text":"One of the things xtool is useful for is to check if the result from two different experiments differ. This is done by applying the DIFF operator and writing the output to the screen like","category":"page"},{"location":"PostProcessing/xtool/","page":"xtool","title":"xtool","text":" xtool [-f] -f1 FILE1 -f2 FILE2 -of SCREEN","category":"page"},{"location":"PostProcessing/xtool/","page":"xtool","title":"xtool","text":"The is heavily used in the Harmonie testbed to check the difference between versions of the system.","category":"page"},{"location":"PostProcessing/xtool/","page":"xtool","title":"xtool","text":"Below is a simple example of how to use xtool. You may also check scr/Fldver to find examples. ","category":"page"},{"location":"PostProcessing/xtool/","page":"xtool","title":"xtool","text":"What is the difference between +24h and +48h MSLP forecasts during August 2008?","category":"page"},{"location":"PostProcessing/xtool/","page":"xtool","title":"xtool","text":"Namelist for xtool, which lists the parameters (here mean sea level pressure) to be examined:\n&NAMINTERP\n  PPPKEY%ppp = 001,\n  PPPKEY%lll = 000,\n  PPPKEY%ttt = 103,\n/\nRun xtool.\nxtool -sdtg1 2008080100 -edtg1 2008083000 -ll1 48 \\\n      -sdtg2 2008073100 -edtg2 2008082900 -ll2 24 \\\n      -p1 /your_model_data/YYYY/MM/HH/fcYYYYMMDD_HH+LLL \\\n      -p2 /your_model_data/YYYY/MM/HH/fcYYYYMMDD_HH+LLL \\\n      -fcint 6 -op DIFF -n your_namelist \\\n      -o output.grb\n-sdtg1, -edtg1, -ll1: The cycles to look for the +48h forecast.\n-sdtg2, -edtg2, -ll2: The cycles to look for the +24h forecast.\n-p1, -p2: Naming rules for the files in cycle 1 and 2, respectively.\n-fcint: Interval between forecast cycles.\n-op: Operation to be applied. Possible choices DIFF, SUM, AVE, STDV or SAL\n-n: Namelist file.\n-o: Name of the output grib file.\nOutput file (output.grb) now contains one 2D-field with accumulated 48-24h difference of mean sea level pressure.  ","category":"page"},{"location":"PostProcessing/xtool/#SAL","page":"xtool","title":"SAL","text":"","category":"section"},{"location":"PostProcessing/xtool/","page":"xtool","title":"xtool","text":"Structure Amplitude Location (SAL) is object based quality measure for the verification of QPFs (Wernli et al., 2008). SAL contains three independent components that focus on Structure, Amplitude and Location of the precipitation field in a specified domain. ","category":"page"},{"location":"PostProcessing/xtool/","page":"xtool","title":"xtool","text":"S: Measure of structure of the precipitation area (-2 - +2). Large S, if model predicts too large precipitation areas.\nA: Measure of strength of the precipitation (-2 - +2). Large A, if model predicts too intense precipitation.\nL: Measure of location of the precipitation object (0 - +2). Large L, if modelled precipitation objects are far from the observed conterparts. \nSAL can be activated in xtool by using -op SAL option. e.g.\n xtool -f1 model.grib -f2 observation.grib -op SAL -n namelist\nOutput of the SAL are 2 simple ascii-files:\nscatter_plot.dat containing date, S,A and L parameters.\nsal_output.dat containing more detailed statistics collected during the verification (location of center of mass, number of objects, measure of object size etc.).","category":"page"},{"location":"Observations/Ascat/","page":"AScat","title":"AScat","text":"See Scatt","category":"page"},{"location":"SuiteManagement/ECFLOW/#Running-Harmonie-under-ecFlow","page":"ECFLOW","title":"Running Harmonie under ecFlow","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#Introduction","page":"ECFLOW","title":"Introduction","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"This document describes how to run Harmonie under ecFlow scheduler at ECMWF. ecFlow is the ECMWF workflow manager and it has been written using python to improve maintainability, allow easier modification and introduce object orientated features as compared to the old scheduler SMS. ecFlow can be used in any HARMONIE version in and above harmonie-40h1.1.beta.1.","category":"page"},{"location":"SuiteManagement/ECFLOW/#New-users","page":"ECFLOW","title":"New users","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"On the ECMWF Atos machine in Bologna, each user has a virtual machine on which ecFlow is running. If you don't have a VM yet, ask ECMWF to set it up for you. If you are starting ecFlow for the first time at ECMWF, you may have to add your ssh key to the authorized_keys file to allow passwordless access, as ssh is used to communicate between the servers:","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"      cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys","category":"page"},{"location":"SuiteManagement/ECFLOW/#Start-your-experiment-supervised-by-ecFlow","page":"ECFLOW","title":"Start your experiment supervised by ecFlow","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"Launch the experiment in the usual manner by giving start time, DTG, end time, DTGEND and other optional arguments","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"      ~hlam/Harmonie start DTG=YYYYMMDDHH","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"If successful, ecFlow will identify your experiment name and start building your binaries and run your forecast. If not, you need to examine the ecFlow log file $HM_DATA/ECF.log. $HM_DATA is defined in your Env_system file. At ECMWF $HM_DATA=$SCRATCH/hm_home/$EXP where $EXP is your experiment name.","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"The ecFlow viewer starts automatically. To view any suite for your server or other servers, the server must be added to the ecFlow viewer (via Servers -> Manage servers, Add server) and selected in Servers. See below on how to find the port and server name.","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"More than one experiment is not allowed with the same name monitored in the same server so Harmonie will start the server and delete previous non-active suite for you.\nFor deleting a suite manually using ecflow_client --port XXXX --host XXXX --delete force yes /suite or using the GUI: right-click on the suite, then click \"Remove\" (if you don't see the Remove option, go to Tools -> Preferences -> Menus, and make yourself Administrator)\nIf other manual intervention in server or client is needed you can use ecflow commands. See here.","category":"page"},{"location":"SuiteManagement/ECFLOW/#ecFlow-control","page":"ECFLOW","title":"ecFlow control","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#Finding-the-port-and-host-of-the-ecFlow-server","page":"ECFLOW","title":"Finding the port and host of the ecFlow server","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"The server on which ecFlow is running is defined with variable $ECF_HOST, the port with ECF_PORT, set in Env_system or derived. On the VMs on ECMWF Atos machine in Bologna ECF_HOST=ecflow-gen-${USER}-001 and ECF_PORT=3141 for all users.","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"Information about server variables can be found by running:","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"On ECMWF's Atos:","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"      ssh ecflow-gen-${USER}-001 ecflow_server status ","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"Or if ecFlow is running on the machine you are logged into:","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"      ecflow_server status ","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"You can also find ECF_PORT/ECF_HOST by checking the files under $ECF_HOME, like:","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"hlam@ecgb11:~/hm_home/43_aug> ls -rlt ~hlam/ecflow_server\ntotal 12\n-rw-r--r-- 1 hlam hirald 2529 Jun 15 16:20 ecflow-gen-hlam-001.3141.ecf.check.b\n-rw-r--r-- 1 hlam hirald 2529 Jun 20 17:36 ecflow-gen-hlam-001.3141.ecf.check\n-rw-r--r-- 1 hlam hirald 3113 Jun 20 17:38 ecflow-gen-hlam-001.log","category":"page"},{"location":"SuiteManagement/ECFLOW/#Check-the-status-of-your-server","page":"ECFLOW","title":"Check the status of your server","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"To check the status of your server you can use","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"ecflow_client --stats  --port ECF_PORT  --host ECF_HOST","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"or","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"ecflow_client --port ECF_PORT  --host ECF_HOST  --ping","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"or go to the \"Info\" tab in the ecFlow viewer.","category":"page"},{"location":"SuiteManagement/ECFLOW/#Open-the-viewer-of-a-running-ecFlow-server","page":"ECFLOW","title":"Open the viewer of a running ecFlow server","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"If you know that your ecFlow server is running but you have no viewer attached to it you can restart the viewer:","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"ecflow_ui &","category":"page"},{"location":"SuiteManagement/ECFLOW/#Stop-your-ecFlow-server","page":"ECFLOW","title":"Stop your ecFlow server","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"If you are sure you're running the server on the login node of your machine you can simply run","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"ecflow_stop.sh","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"A more complete and robust way is","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"export ECF_PORT=<your port>\nexport ECF_HOST=<your server name>\necflow_client  --halt=yes\necflow_client  --check_pt\necflow_client  --terminate=yes","category":"page"},{"location":"SuiteManagement/ECFLOW/#Restart-your-ecFlow-server","page":"ECFLOW","title":"Restart your ecFlow server","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"The ecFlow servers on the virtual machines a ECMWF should be restarted automatically. If it doesn't, you may need to restart it with:","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"ssh ecflow-gen-${USER}-001 sudo systemctl restart ecflow-server","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"On other systems, if the server is not running you can start again using the script:","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":" ecflow_start.sh [-d $ECF_HOME]","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"If ecFlow is running on a different machine you have to login and start it on that machine:","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":" ssh <your server name>\n module load ecflow\n ecflow_start.sh [-d $ECF_HOME]","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"As an alternative you can let Harmonie start the server for you when starting your next experiment, or type","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"~hlam/Harmonie mon","category":"page"},{"location":"SuiteManagement/ECFLOW/#Keep-your-ecFlow-server-alive","page":"ECFLOW","title":"Keep your ecFlow server alive","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"If not using ecFlow at the ECMWF's VMs, the ecFlow server will eventually die causing an unexpected disruption in you experiments. To prevent this you can add a cron job restarting the server e.g. every fifth minute.","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"> crontab -l\n*/5 * * * * /home/$USER/bin/cronrun.sh ecflow_start.sh -d $ECF_HOME > ~/ecflow_start.out 2>&1","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"where tthe small script cronrun.sh makes sure you get the right environment","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"#!/bin/bash\nsource ~/.bash_profile\nmodule unload ecflow\nmodule load ecflow/5.7.0\n$@","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"The ecFlow server version may change over time.","category":"page"},{"location":"SuiteManagement/ECFLOW/#Add-another-user-to-your-ecFlow-viewer","page":"ECFLOW","title":"Add another user to your ecFlow viewer","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"Sometimes it's handy to be able to follow, and control, your colleagues experiments. To be able to do this do the following steps:","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"Find the port number of your colleague as described above.\nIn the ecFlow viewer choose Servers -> Manage servers, click on \"Add server\" and fill in the appropriate host and port and give it a useful name. Click on OK to save it.\nIf you click on Servers in the viewer the name should appear and you can make it visible by clicking on it.","category":"page"},{"location":"SuiteManagement/ECFLOW/#Changing-the-port","page":"ECFLOW","title":"Changing the port","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"By default, the port is set by ","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"export ECF_PORT=$((1500+usernumber))","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"in mSMS.job (40h1.1), Start_ecFlow.sh (up to #b6d58dd), or Main (currently).","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"For the VMs at ECMWF it is set to 3141 in Env_system. If you want to change this number (for example, if that port is in use already), you will also need to add a -p flag when calling ecflow_start.sh as follows:","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"ecflow_start.sh -p $ECF_PORT -d $JOBOUTDIR","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"Otherwise, ecflow_start.sh tries to open the default port.","category":"page"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"Note: if you already have an ecFlow server running at your new port number before launching an experiment, this won't be an issue. ","category":"page"},{"location":"SuiteManagement/ECFLOW/#More-info","page":"ECFLOW","title":"More info","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/","page":"ECFLOW","title":"ECFLOW","text":"ECMWF's ecFlow documentation\necFlow at ECMWF's ATOS","category":"page"},{"location":"Observations/ObservationData/#Observation-data","page":"ObservationData","title":"Observation data","text":"","category":"section"},{"location":"Observations/ObservationData/","page":"ObservationData","title":"ObservationData","text":"In off-line experiments the Prepare_ob script extracts observations from a data archive, e.g from MARS archive at ECMWF platform, or from existing observation files available locally.","category":"page"},{"location":"Observations/ObservationData/#ECMWF","page":"ObservationData","title":"ECMWF","text":"","category":"section"},{"location":"Observations/ObservationData/","page":"ObservationData","title":"ObservationData","text":"At ECMWF, the HARMONIE script prepares the retrieval (retrin) file for MARS request in scr/WriteMARSreq is executed by scr/Prepare_ob.","category":"page"},{"location":"Observations/ObservationData/","page":"ObservationData","title":"ObservationData","text":"WriteMARSreq -d $DATE -t $TIME -r $RANGE -o $OBSLIST -m ./retrin -z $BUFRFILE -g $GEOL","category":"page"},{"location":"Observations/ObservationData/","page":"ObservationData","title":"ObservationData","text":"The variables above denote","category":"page"},{"location":"Observations/ObservationData/","page":"ObservationData","title":"ObservationData","text":"DATE - analysis date\nTIME - analysis time\nRANGE - extraction time interval\nOBSLIST - list of the observations to be extracted according to MARS definition; \nBUFRFILE - name of the extracted BUFR file\nGEOL - Extraction of the model domain properties from the climate file as follows\n$BINDIR/domain_prop $CLIMDIR/m$MM -f -MAX_EXT > foo\n   EASTEC=$( tail -1 foo | head -1 | sed 's/ //g' )\n   NORTHEC=$( tail -2 foo | head -1 | sed 's/ //g' )\n   WESTEC=$( tail -3 foo | head -1 | sed 's/ //g' )\n   SOUTHEC=$( tail -4 foo | head -1 | sed 's/ //g' )","category":"page"},{"location":"Observations/ObservationData/#LOCAL","page":"ObservationData","title":"LOCAL","text":"","category":"section"},{"location":"Observations/ObservationData/","page":"ObservationData","title":"ObservationData","text":"Otherwise, this step consists of fetching (or waiting for) the observations stored in $OBDIR defined in ecf/config_exp.h . In that case one can use the command \"cat\" to merge different observations into one BUFR file, ob${DTG}. In general, HIRLAM services are adopting SAPP, ECMWF's scalable acquisition and pre-processing system, to process (conventional) GTS reports and other observational data for use in operational NWP. SAPP produces BUFR encoded in the same way as observational BUFR data available in the MARS archive.","category":"page"},{"location":"System/ReleaseProcess/#Harmonie-release-process","page":"Harmonie release process","title":"Harmonie release process","text":"","category":"section"},{"location":"System/ReleaseProcess/","page":"Harmonie release process","title":"Harmonie release process","text":"This page describes the release process for tagging new Harmonie versions  ","category":"page"},{"location":"System/ReleaseProcess/#Harmonie-repository-organization","page":"Harmonie release process","title":"Harmonie repository organization","text":"","category":"section"},{"location":"System/ReleaseProcess/","page":"Harmonie release process","title":"Harmonie release process","text":"In the past we used the concept of trunk(svn) or develop(git) for the development of Harmonie-Arome codes. Since CY46 we decided to used dev-CYXXhX as development branch to be more clear about the harmonie version under development. ","category":"page"},{"location":"System/ReleaseProcess/#Harmonie-AROME-naming-convection","page":"Harmonie release process","title":"Harmonie-AROME naming convection","text":"","category":"section"},{"location":"System/ReleaseProcess/","page":"Harmonie release process","title":"Harmonie release process","text":"Using a common T(Toulouse) cycle of the ACCORD consortium from the IAL repository the development of an Harmonie-Arome version starts.","category":"page"},{"location":"System/ReleaseProcess/","page":"Harmonie release process","title":"Harmonie release process","text":"The naming convention is using the number of their cycle of T cycle used as base. \nThe h letter is used to indicate that it is, or will be, an Harmonie-Arome CSC that differs from the T base code version. \nThe first number after the h refers to the version of T cycle version used as base. (e.g. CY46T1 is used as base for dev-CY46h1)","category":"page"},{"location":"System/ReleaseProcess/#Tagging","page":"Harmonie release process","title":"Tagging","text":"","category":"section"},{"location":"System/ReleaseProcess/","page":"Harmonie release process","title":"Harmonie release process","text":"In Hirlam, various tagging prior to official releases are made to provide user communities with a 'frozen' code set, even though the code has not necessarily been fully validated. These codes are often labeled as alpha, beta, rc.","category":"page"},{"location":"System/ReleaseProcess/","page":"Harmonie release process","title":"Harmonie release process","text":"Alpha release (e.g. harmonie-46h1.alpha.1): a snapshot of dev branch which is not mature both technically and meteorologically\nBeta release (e.g. harmonie-46h1.beta.1): a snapshot of dev branch which is deemed technically mature for evaluation and meteorological validation. On the other hand, there could still be possibility for more features to add\nTarget releases (e.g. harmonie-43h2.2.target.2 and harmonie-43h2.2.target.3): pre-release tagging for final meteorological evaluation\nRelease candidate(e.g. harmonie-43h2.2.rc1): pre-release tagging for final evaluation\nOfficial release (e.g. harmonie-43h2.2): mature for operational use\nThe second number refers the number of the Harmonie-Arome release technically and meteorological quality assured\nA third number could appear in the name for a minor update, a technical release necessities or other aspects.(e.g., harmonie-43h2.2.1\nAlso some bugfix branches could be active using the bf in the naming (e.g.harmonie-43h2.2_bf)","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Platform-Configuration","page":"Platform","title":"Platform Configuration","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Overview","page":"Platform","title":"Overview","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"This wiki page outlines the configuration files required by HARMONIE for successful compilation and running of the system.","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Basic-requirements","page":"Platform","title":"Basic requirements","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"All experiments require a valid host to \"setup\" an experiment using the Harmonie script. Recall from the quick start instructions that in order to setup a new experiment on your platform, called YOURHOST, using HARMONIE downloaded to PATH_TO_HARMONIE one must issue the following command:","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"cd hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie setup -r PATH_TO_HARMONIE -h YOURHOST","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"hm_home/my_exp contains:","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"Env_submit -> config-sh/submit.YOURHOST           ## YOURHOST platform specific settings\nEnv_system -> config-sh/config.YOURHOST           ## YOURHOST task submission settings\n./config-sh/hm_rev                                ## contains PATH_TO_HARMONIE\n./config-sh/Main                                  ## The script used to run HARMONIE\n./config-sh/submit.YOURHOST                       ## YOURHOST platform specific settings\n./config-sh/config.YOURHOST                       ## YOURHOST task submission settings\n./suites/harmonie.pm                              ## perl module to define ensemble settings\n./ecf/config_exp.h                                ## your experiment definition (scientific type options)\n./scr/include.ass                                 ## assimilation specific settings","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"But, what if your host configuration is not available in the HARMONIE system? Host specific configuration files in PATH_TO_HARMONIE/config-sh must be available for your host and configuration files for the compilation of the code must be available. This documentation attempts to describe what is required.","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Host-config-files","page":"Platform","title":"Host config files","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Env_system-config-sh/config.YOURHOST","page":"Platform","title":"Env_system -> config-sh/config.YOURHOST","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"The config.YOURHOST file defines host specific variables such as some input directory locations. If your YOURHOST is not already included in HARMONIE it may be work looking at config.* files in config-sh/ to see what other people have done. The table below outlines variables set in config-sh/config-sh.YOURHOST and what the variables do:","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"Variable name Description\nCOMPCENTRE controls special ECMWF solutions (such as MARS) where required. Set to LOCAL if you are unsure\nHARMONIE_CONFIG defines the config file used by Makeup compilation\nMAKEUP_BUILD_DIR location of where Makeup compiles the HARMONIE code\nMAKE_OWN_PRECOMPILED yes=>install pre-compiled code in $PRECOMPILED\nPRECOMPILED location of (optional) pre-compiled HARMONIE code\nE923_DATA_PATH location of input data for E923, climate generation\nPGD_DATA_PATH location of input data for PGD, surfex climate generation\nECOSG_DATA_PATH location of input data for ECOCLIMAP2G\nGMTED2010_DATA_PATH location of HRES DEM\nSOILGRID_DATA_PATH location of SOILGRID data\nHM_SAT_CONST location of constants for satellite assimilation\nRTTOV_COEFDIR location of RTTOV coefficients\nHM_DATA location of top working directory for the experiment\nHM_LIB location of src/scripts and compiled code\nTASK_LIMIT Maximum number of jobs submitted by ECFLOW\nRSYNC_EXCLUDE used to exclude .git* sub-directories from copy of source code for compilation\nDR_HOOK_IGNORE_SIGNALS environment variable used by Dr Hook to ignore certain \"signals\"\nHOST0 define primary host name\nHOSTN define other host name(s)\nHOST_INSTALL 0=> install on HOST0, 0:...:N => install on HOST0,...,HOSTN\nMAKE make command may need to be explicity defined. Set to make for most platforms\nMKDIR mkdir command (default: mkdir -p)\nJOBOUTDIR where ECFLOW writes its log files\nECCODES_DEFINITION_PATH location of local ecCodes definition files\nBUFR_TABLES location of local BUFR tables","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Env_submit-config-sh/submit.YOURHOST","page":"Platform","title":"Env_submit -> config-sh/submit.YOURHOST","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"The Env_submit file uses perl to tell the HARMONIE scheduler how to execute programs - which programs should be run on multiple processors and define batch submissions if required.","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"perl description\n%backg_job defines variables for jobs run in the background on HOST0\n%scalar_job defines variables for single processor batch jobs\n%par_job defines variables for multi-processor batch jobs\n@backg_list list of tasks to be submitted as a background job\n@scalar_list list of tasks to be submitted as a scalar job\n@par_list list of tasks to be submitted as parallel job\ndefault \"wildcard\" task name to defined default type of job for unlisted tasks","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Host-summary","page":"Platform","title":"Host summary","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"YOURHOST Host type batch Contact\nKNMI-Altix KNMI SGI HPC none \nLinuxPC General Linux PC no MPI none \nLinuxPC-MPI General Linux PC with MPI none \nLinuxPC-MPI-ubuntu Ubuntu Linux PC with MPI none \nMETIE.LinuxPC METIE CentOS 6 PC with MPI none Eoin Whelan\nMETIE.LinuxRH7gnu METIE Redhat 7 server with MPI none Eoin Whelan\nMETIE.fionn METIE SGI HPC PBS Eoin Whelan\nSMHI.Linda4 SMHI ???  \nSMHI.LinuxPC SMHI PC  \nbi   \ncrayx1   \ncrayxt5m   \necgb   \necgb-cca ECMWF HPC with MPI dual host slurm/PBS \nfmisms   \njumbo   \nxt5intel   \nxtpathscale   ","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Compilation-config-files","page":"Platform","title":"Compilation config files","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Makeup","page":"Platform","title":"Makeup","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"config files required for compilation of code using Makeup ...","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"More information on Makeup is available here: Build with Makeup","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Obsmon","page":"Platform","title":"Obsmon","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/","page":"Platform","title":"Platform","text":"For config files required for compilation of obsmon check util/obsmon/config","category":"page"},{"location":"Observations/Bator/#ODB-creation:-Bator","page":"Bator","title":"ODB creation: Bator","text":"","category":"section"},{"location":"Observations/Bator/#General-Description","page":"Bator","title":"General Description","text":"","category":"section"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"The pre-processing step creates ODB (Observational Data Base) from various observation data files possibly in different formats.","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"Software: The programs used for pre-processing (ShuffleBufr, oulan and BATOR) are not part of the IFS code. oulan is software developed at Météo France to extract (conventional) observations from their local database (BDM). The ASCII output from oulan, the OBSOUL file, is one of the inputs of BATOR. By default, oulan is no longer part of the observation processing chain. BATOR is also developed at Météo France to generate the ODB (Observational !DataBase) database for the ARPEGE/ALADIN/HARMONIE analysis system. ODB is a tailor made database software developed at ECMWF to manage very large observational data volumes assimilated in the IFS 4DVAR system, and to enable flexible post-processing of this data (Sami Saarinen, 2006). HARMONIE's BATOR originates from the MF export-pack. The figure below describes the mechanism of the observation pre-processing in HARMONIE DA. To sum it up, !ShuffleBufr splits different observations into BUFR files and BATOR creates the ODB file using from BUFR/HDF5/NetCDF input files.","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"Compilation: BATOR is compiled using gmakpack or makeup.\nScripts: scr/Bator.\nInput: BUFR/HDF5/NetCDF\nOutput: ODB databases for surface and upper-air data assimilation","category":"page"},{"location":"Observations/Bator/#BATOR","page":"Bator","title":"BATOR","text":"","category":"section"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"BATOR creates the ODB files from observational data in BUFR/HDF5/NetCDF format. BATOR also includes filtering (blacklisting) of parameters from stations of different observation types. To run the BATOR program one needs files containing blacklist rules/information, namelist(s), file containing information about observations and their format – refdata -, and some setting for the ODB environment. Documentation provided by Météo France is available at http://www.umr-cnrm.fr/gmapdoc/spip.php?article229. In particular: BATOR namelists, the param_bator.cfg file and the batormap files.","category":"page"},{"location":"Observations/Bator/#observation-window-and-timeslots","page":"Bator","title":"observation window and timeslots","text":"","category":"section"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"The timeslots characteristics are provided by BATOR using the following environment variables. These are defined in scr/Bator based on settings provided in scr/include.ass and ecf/config_exp.h.","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"Environment variable Description\nODB_ANALYSIS_DATE analysis date (YYYYMMDD)\nODB_ANALYSIS_TIME analysis time (hhmmss)\nBATOR_NBSLOT number of timeslots needed [1, 9999]\nBATOR_WINDOW_LEN width of the temporal assimilation window (in minutes) [1, 9999]\nBATOR_WINDOW_SHIFT shift of the temporal assimilation window relative to the analysis time (in minutes). Must be negative.\nBATOR_SLOT_LEN width of a standard timeslot (in minutes) [1, 9999]\nBATOR_CENTER_LEN width of the centred timeslot (in minutes) [1, 9999]","category":"page"},{"location":"Observations/Bator/#batormap","page":"Bator","title":"batormap","text":"","category":"section"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"The batormap file lists all the input data files (BUFR,NETCDF,HDF5,OBSOUL) to translate and put in a particular ODB database. Several records can be stored in this file, each one composed by the following 4 fields (blank spaces are used as separator). The batormap file is created by scr/Bator based on settings provided in scr/include.ass and task arguments.","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"The ECMA database extension in which data will be stored, up to 8 characters.\nThe data filename extension, up to 8 characters.\nData filename format, up to 8 characters.\nKind of data or instrument, up to 16 characters. Must match a kind of data in the subroutine bator_initlong (src/odb/pandormodule/bator_init_mod.F90)","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"For example:","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"conv     conv     OBSOUL   conv\nconv     synop    BUFR     synop","category":"page"},{"location":"Observations/Bator/#param.cfg","page":"Bator","title":"param.cfg","text":"","category":"section"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"BATOR reads BUFR data according to definitions describing BUFR templates in the param.cfg file. The general layout of definitions in the param.cfg file is as follows:","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"BUFR label\na b c d\ncodage  a1  desc_a1\n...\ncodage  an   desc_an\ncontrol b1   val1\n...\ncontrol bn   valn\noffset  c1   inc1\n...\noffset  cn   incn\nvalues  pos_d1 desc_d1\n...\nvalues  pos_dn desc_dn\n/BUFR label","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"where: ","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"a: number of unexpanded descriptors (NTDLEN if you are familiar with bufrdc software).\nb: number of \"control\" entries - \"control\" values used by src/odb/pandor/module/bator_decodbufr_mod.F90\nc: number of \"offset\" entries - \"jump\" values used by src/odb/pandor/module/bator_decodbufr_mod.F90\nd: number of \"values\" entries - bufrdc VALUES array indices of parameters\ncodage desc: FXY's of NTDLST array values\nvalues desc: FXY's of NTDEXP array values","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"param.cfg files are stored in const/bator_param and linked for use by BATOR in scr/Bator","category":"page"},{"location":"Observations/Bator/#Namelists","page":"Bator","title":"Namelists","text":"","category":"section"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"Namelists are needed for BATOR to deal with observations having format structure different than that of MF. For example to read local Seviri data in grib format, one should set some parameters (NLON_GRIB and NLAT_GRIB) in the NADIRS group. Note: If my last modifications will be accepted, we will need to set parameters in the mentioned group, so the use of this namelist become obligatory for “local” (outside MF) HARMONIE system. The namelist looks like this:","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":" &NADIRS\n   LMFBUFR=.FALSE.,                                         # we are not using Météo France BUFR\n   ASCAT_XYGRID=12500.,                                     # ASCAT XYGRID RESOLUTION /LR=25000m/HR=12500m/\n   GPSSOLMETHOD=\"CENT\",                                     # Selection method for GNSS data \"CENT\"/\"MEAN\"\n   NbTempMaxLevels=6000,                                    # Maximum number of radiosonde levels read\n   TempSondOrTraj=.FALSE.,                                  # .TRUE. = sondage vertical, .FALSE. = trajectoire\n   TempSondSplit=.FALSE.,                                   # .TRUE. = on coupe le radiosondage/timeslot, .FALSE. = profil simple\n   ElimTemp0=.FALSE.,                                       # suppression des TEMP sans delta lat/lon/time si .TRUE.\n   ElimPilot0=.FALSE.,                                      # suppression des PILOT sans delta lat/lon/time si .TRUE.\n   NFREQVERT_TPHR=100,                                      # thinning factor for radiosonde data\n   NbMinLevelHr=300,                                        # level threshold for high-resolution treatement of sonde data\n   TS_AMSUA(206)%t_select%ChannelsList(:) = -1,             # ...\n   TS_AMSUA(206)%t_select%TabFov(:) = -1,                   # ...\n   TS_AMSUA(206)%t_select%TabFovInterlace(:) = -1,          # ...\n   :                                                        # :\n   :                                                        # :\n   :                                                        # :\n   SIGMAO_COEF(1:18)=18*1.0,                                # Scaling of sigmo-o coefficients\n /\n &NAMSCEN\n /\n &NAMDYNCORE\n /\n &NAMSATFREQ\n /\n","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"Namelist to activate (bator) LAMFLAG (needed to extract the observations for the model + extension zone domain): one needs to fetch the bator_lamflag namelist using the following command\nlamflag_namelist VAR\n &NAMFCNT\n   LOBSONLY=.FALSE.,\n /\n &NAMFGEOM\n   EFLAT0=$LAT0,\n   EFLON0=$LON0,\n   EFLATC=$LATC,\n   EFLONC=$LONC,\n   EFLAT1=$LAT1,\n   EFLON1=$LON1,\n   EFDELX=$GSIZE,\n   EFDELY=$GSIZE,\n   NFDLUN=1,\n   NFDGUN=1,\n   NFDLUX=$NDLUXG,\n   NFDGUX=$NDGUXG,\n   Z_CANZONE=1500.,\n   REDZONE=$REDZONE_BATOR,\n   LVAR=$LVAR,\n   LNEWGEOM=.TRUE.,\n /\n &NAMFOBS\n   LSYNOP=$LSYNOP,\n   LAIREP=$LAIREP,\n   LDRIBU=$LDRIBU,\n   LTEMP=$LTEMP,\n   LPILOT=$LPILOT,\n   LPAOB=$LPAOB,\n   LSCATT=$LSCATT,\n   LSATEM=$LSATEM,\n   LSATOB=$LSATOB,\n   LSLIMB=$LSLIMB,\n   LRADAR=$LRADAR,","category":"page"},{"location":"Observations/Bator/#Environment-variables","page":"Bator","title":"Environment variables","text":"","category":"section"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"ODB settings for BATOR:\n\t\texport ODB_CMA=ECMA\n\t\texport ODB_SRCPATH_ECMA=${d_DB}/ECMA.${base}\n\t\texport ODB_DATAPATH_ECMA=${d_DB}/ECMA.${base} \n\t\texport ODB_ANALYSIS_DATE=${YMD}\n\t\texport ODB_ANALYSIS_TIME=${HH}0000\n\t\texport IOASSIGN=${d_DB}/ECMA.${base}/IOASSIGN\n\t\texport BATOR_NBPOOL=${NPOOLS}\n\n\t\t#--- prepare db dir\n\t\tRecreateDir ${d_DB}/ECMA.${base}\t   \n\t\t#-- create IOASSIGN file for the given sub-base\n\t\tcd ${d_DB}/ECMA.${base}\t\n\t\texport ODB_IOASSIGN_MAXPROC=${NPOOLS}\n\t\t$HM_LIB/scr/create_ioassign -l \"ECMA\" -n ${BATOR_NBPOOL}","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"where $base is the ODB base ($base can be conv (for conventional data), amsu (ATOVS/AMSU-A,AMSU-B/MHS), sev (for Sevir), iasi, radarv (radar) for example). Important: If you would like to have more bases, do not forget to take that into consideration when generating the \"batormap\" file for BATOR to define which observations you would like to have in each base.","category":"page"},{"location":"Observations/Bator/#Blacklisting","page":"Bator","title":"Blacklisting","text":"","category":"section"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"To avoid model forecast degradation, two files can be used to blacklist or exclude data from the analysis. They are also used to blacklist observations that the model cannot deal with because they are not representative (orography, breeze effects...). The reason for the existence of this method of 'blacklisting', built-in Bator, alongside with hirlam_blacklist.b (built-in Screening) is to allow simple and quick changes (and especially without changing binary) in the operational suite.","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"The selection of an observation to be 'blacklisted' can be done using multi-criteria (SID/STATID, obstype, codetype, varno, channel/level, production center, sub-center producer, network (s) concerned (s), cycle (prod / assim), ..).","category":"page"},{"location":"Observations/Bator/#LISTE_LOC","page":"Bator","title":"LISTE_LOC","text":"","category":"section"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"The LISTE_LOC file can be used to blacklist satellite data and also for other data by type and / or subtype for a given parameter (described by varno or not). The contents of the LISTE_LOC are as follows:","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"Column Description Format\n1 Type of action: N: blacklisted, E: exclude a1\n2 The observation type (obsytpe@hdr) i3\n3 The observation code-type (codetype@hdr) i4\n4 The satellite ID with leading zeros (satid@sat) a9\n5 The centre that produced the satellite data i4\n6 The parameter ID (varno@body) or the satellite sensor ID (sensor@hdr) i4\n7 Optional keywords of ZONx4, TOVSn, PPPPn, PROFn ","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"TOVSn C1 C2 ... Cn","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"can be aplied to ATOVS radiances\nn can be at most 9 indicating the involved channels\nthe Ci values specify the channels to be blacklisted","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"PPPPn P1 P2 ... Pn","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"can be aplied to blacklist different pressure levels\nn can be at most 9 indicating the involved levels\nthe Pi values specify the pressure levels (in hPa) to be blacklisted","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"PROFn P1a P2 ... Pn-1 I1 I2 ... In-1","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"n can be at most 9 indicating the involved layers\nthe Pi values specify the bottom and top levels of pressure layers (in hPa).\nThe first layer is always [1000,P1]\nthe Ii values indicate if blacklisting should be applied (=1) or not (=0) to the given layer.","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"ZONx4 latmin latmax lonmin lonmax","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"can be applied to SATOB/GEOWIND data\nif x=B then the pixels with lat < latmin or lat > latmax or lon < lonmin or lon > lonmax will be blacklisted\nif x=C then the pixels with lat < latmin or lat > latmax or (lon > lonmin and lon < lonmax) will be blacklisted.","category":"page"},{"location":"Observations/Bator/#LISTE_NOIRE_DIAP","page":"Bator","title":"LISTE_NOIRE_DIAP","text":"","category":"section"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"The LISTE_NOIRE_DIAP (const/bator_liste) can be used to blacklist conventional observations by station identifier. The contents of the LISTE_NOIRE_DIAP are as follows:","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"Column Description Format\n1 Observation type (obstype@hdr) i2\n2 Observation name a10\n3 Observation codetype (codetype@hdr) i3\n4 Parameter ID (varno@body) i3\n5 Station ID (statid@hdr) a8\n6 Start date of blacklisting yyyymmdd a8\n7 Optional layer blacklisting (PROFn) a180","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"PROFn P1a P2 ... Pn-1 I1 I2 ... In","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"n can be at most 9 indicating the involved layers\nthe Pi values specify the bottom and top levels of pressure layers (in hPa).The first layer is always [1000,P1]\nthe Ii values indicate if blacklisting should be applied (=1) or not (=0) to the given layer. \nThe Hxx keyword specifies the analysis hour that should be blacklisted e.g. H00 or H06 etc","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"Particularities - the blacklisting of certain parameters involves the automatic blacklisting of other parameter summarized in the table below:","category":"page"},{"location":"Observations/Bator/","page":"Bator","title":"Bator","text":"obstype specified parameter blacklisted parameters\nSYNOP 39 (t2) 39 (t2), 58 (rh2), 7 (q)\nSYNOP 58 (rh2) 58 (rh2), 7 (q)\nTEMP 1 (z) 1 (z), 29 (rh), 2 (t), 59 (td), 7 (q)\nTEMP 2 (t) 2 (t), 29 (rh), 7 (q)\nTEMP 29 (rh) 29 (rh), 7 (q)","category":"page"},{"location":"Verification/Obsmon/#OBSMON","page":"Obsmon","title":"OBSMON","text":"","category":"section"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"In 2014 a new version of the observational monitoring system entered trunk. The first official release containing obsmon was cy38h1.2","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"The obsmon package consists of two components. The first is a fortran-based code that is run, for all the active observations types (defined in scr/include.ass), at the post-processing stage of an experiment. It generates statistics from the ODB and store data in three SQLite tables (ECMA/CCMA/ECMA_SFC(CANARI)). In addition the SQLite tables are concatenated in tables in the /ts directory at the end of the run.","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"The second component is written in R using the Shiny web application framework. It allows the interactive visualization of the data contained in the SQLite tables produced by the first component of the package. This can be done either offline or via a server daemon (e.g. shiny.hirlam.org).","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"For disambiguation, we will hereinafter use the terms \"backend\" and \"frontend\" to refer to the first and second components of obsmon, respectively.","category":"page"},{"location":"Verification/Obsmon/#How-to-turn-on-backend-obsmon?","page":"Obsmon","title":"How to turn on backend obsmon?","text":"","category":"section"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"Obsmon is enabled by default in ecf/config_exp.h  vi OBSMONITOR=obstat","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"note: Note\nIf you don't have any log-files from the monitoring experiment, you should disable plotlog from the OBSMONITOR= string in ecf/config_exp.h ","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"note: Note\nMake sure that the -DODBMONITOR pre-processor flag is active during compilation of util/monitor. This should only be an issue on untested platforms and is by default enabled on ECMWF.","category":"page"},{"location":"Verification/Obsmon/#How-to-create-statistics-and-SQLite-tables-offline/stand-alone:","page":"Obsmon","title":"How to create statistics and SQLite tables offline/stand-alone:","text":"","category":"section"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"If you are running a normal harmonie experiment with the OBSMONITOR=obstat active, the following step is not relevant.","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"Two new actions are implemented in the Harmonie script. Instead of start you can write obsmon and instead of prod you can write obsmonprod. This will use the correct definition file and only do post-processing. If you have your ODB files in another experiment you can add the variable OBSMON_EXP_ARCHIVE_ROOT to point to the archive directory in the experiment you are monitoring. This approach is used in the operational MetCoOp runs. If you set OBSMON_EXP=label the runs will be stored in $EXTRARCH/label/. This way you can use the same experiment to monitor all other experiments. The experiements do not need to belong to you as long as you have reading permissions to the experiment. ","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"1. as start:\n${HM_REV}/config-sh/Harmonie obsmon DTG=YYYYMMDDHH DTGEND=YYYYMMDDHH OBSMON_EXP_ARCHIVE_ROOT=PATH-TO-ARCHIVE-DIRECTORY-TO-MONITOR OBSMON_EXP=MY-LABEL","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"2. as prod:\n${HM_REV}/config-sh/Harmonie obsmonprod DTGEND=YYYYMMDDHH OBSMON_EXP_ARCHIVE_ROOT=PATH-TO-ARCHIVE-DIRECTORY-TO-MONITOR OBSMON_EXP=MY-LABEL","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"If you want to monitor an experiment stored on ECFS, you should specify OBSMON_EXP_ARCHIVE_ROOT with the full address (ectmp:/$USER/..... or ec:/$USER/...) e.g. ","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"OBSMON_EXP_ARCHIVE_ROOT=ectmp:/$USER/harmonie/MY-EXP OBSMON_EXP=MY-LABEL","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"You can also monitor other users experiments as long as you have read-access to the data.","category":"page"},{"location":"Verification/Obsmon/#How-to-visualize-the-SQLite-tables-using-frontend-obsmon:","page":"Obsmon","title":"How to visualize the SQLite tables using frontend obsmon:","text":"","category":"section"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"Download the code from its git repo at hirlam.org:","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"git clone git@github.com:Hirlam/obsmon.git ","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"Instructions on how to install, configure and run the code can be found in the file docs/obsmon_documentation.pdf that is shipped with the code.","category":"page"},{"location":"Verification/Obsmon/#How-to-extend-backend-obsmon-with-new-observation-types","page":"Obsmon","title":"How to extend backend obsmon with new observation types","text":"","category":"section"},{"location":"Verification/Obsmon/#Step-1:-Extract-statistics-from-ODB","page":"Obsmon","title":"Step 1: Extract statistics from ODB","text":"","category":"section"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"In the scripts you must enable monitoring of your observation type. Each observation type is monitored if active in:","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"msms/harmonie.tdf","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"The script which calls the obsmon binary, is:","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"scr/obsmon_stat","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"This script set the correct namelist based on how you define your observation below. ","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"After the information is extracted, the different SQLite bases are gathered into one big SQLite file in the script:","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"scr/obsmon_link_stat","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"The observation types which the above script is gathering is defined in obtypes in this script:","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"util/monitor/scr/monitor.inc","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"Then let us introduce the new observation in the obsmon binary. The source code is in ","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"harmonie/util/monitor","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"There are two modules controlling the extraction from ODB:","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"mod/module_obstypes.f90\nmod/module_obsmon.F90","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"The first routine defines and initializes the observation type you want to monitor. The second calls the intialization defined in the first file. The important steps are to introduce namelist variables and a meaningful definition in the initialization of the observation type.","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"The real extraction from ODB is done in","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"cmastat/odb_extract.f90","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"At the moment there are two different SQL files used, one for conventional and one for satelites. E.g. radar is handled as TEMP/AIRCRAFT.","category":"page"},{"location":"Verification/Obsmon/#Step-2:-Visualize-the-new-observation-in-shiny-(frontend-obsmon)","page":"Obsmon","title":"Step 2: Visualize the new observation in shiny (frontend obsmon)","text":"","category":"section"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"The logics of which observation type to display is defined in:","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"src/observation_definitions.R","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"In case of a new plot added, the plotting is defined in the files under:","category":"page"},{"location":"Verification/Obsmon/","page":"Obsmon","title":"Obsmon","text":"src/plots","category":"page"},{"location":"System/Scalability_and_Refactoring/#Scalability-and-refactoring","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"","category":"section"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"Preliminary docs, presentations and links","category":"page"},{"location":"System/Scalability_and_Refactoring/#Purpose","page":"Scalability and refactoring","title":"Purpose","text":"","category":"section"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"Create a reference webpage for docs, presentations and web links related with code scalability and refactoring","category":"page"},{"location":"System/Scalability_and_Refactoring/#ESCAPE-project","page":"Scalability and refactoring","title":"ESCAPE project","text":"","category":"section"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"ESCAPE stands for Energy-efficient Scalable Algorithms for Weather Prediction at Exascale.","category":"page"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"1st ESCAPE Dissemination and Training Workshop\nAtlas documentation\nPantarhei project for development of non-hydrostatic IFS related to ESCAPE and ATLAS","category":"page"},{"location":"System/Scalability_and_Refactoring/#HPC-for-Meteorology","page":"Scalability and refactoring","title":"HPC for Meteorology","text":"","category":"section"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"[http://www.ecmwf.int/en/learning/workshops-and-seminars/17th-workshop-high-performance-computing-meteorology]","category":"page"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"AROME France Optimizations Philippe Marguinaud AROME Single Precision and New IO server\necProof vs Dr Hook ecProof profiling tool","category":"page"},{"location":"System/Scalability_and_Refactoring/#OOPS-and-C","page":"Scalability and refactoring","title":"OOPS and C++","text":"","category":"section"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"C++ and OOPS training \nTechnical presentations of recent IFS changes (OOPS re-factoring), February 2016","category":"page"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"During the 2016 OOPS Seminar at ECMWF, were given some presentations of the recent changes in IFS in the OOPS re-factoring context :","category":"page"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"OOPS refactoring in IFS-Arpège Fortran - Cycles CY42-CY43, Deborah Salmon pdf\nMISO : Modifying the Ifs Source for Oops, Olivier Marsden pdf\nOOPS observation cleaning (new HOP code + TL/AD) , Alan Geer, Peter Lean, Deborah Salmond, Thibaut Montmerle, Christophe Payan pdf\nA new framework for working with observational data in IFS (new IFS/ODB interfaces), Peter Lean, Alan Geer, Deborah Salmond pdf\nOOPS Jb Changes, Mike Fisher pdf\nNew fields storage in IFS (GMV/GFL), Tomas Wilhelmsson pdf","category":"page"},{"location":"System/Scalability_and_Refactoring/#Benchmarking","page":"Scalability and refactoring","title":"Benchmarking","text":"","category":"section"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"RAPS\nVtune \nPMU Intel\nPAPI CUDA ","category":"page"},{"location":"System/Scalability_and_Refactoring/#Technical-Videoconferences-IFS/Arpege","page":"Scalability and refactoring","title":"Technical Videoconferences IFS/Arpege","text":"","category":"section"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"Minutes 20161013 pdf  \nMinutes 20160606 pdf \nMinutes 20160121 pdf\nTable 20161018 pdf","category":"page"},{"location":"System/Scalability_and_Refactoring/#Atlas-documentation","page":"Scalability and refactoring","title":"Atlas documentation","text":"","category":"section"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"[ Atlas 0.6 User guide]","category":"page"},{"location":"System/Scalability_and_Refactoring/#System-Videoconference","page":"Scalability and refactoring","title":"System Videoconference","text":"","category":"section"},{"location":"System/Scalability_and_Refactoring/","page":"Scalability and refactoring","title":"Scalability and refactoring","text":"System Status Presentation","category":"page"},{"location":"Build/Installation/#HARMONIE-System-Installation","page":"Installation","title":"HARMONIE System Installation","text":"","category":"section"},{"location":"Build/Installation/#Introduction","page":"Installation","title":"Introduction","text":"","category":"section"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"In the following the installation and compilation of system is described. Read more about how to get the system in here","category":"page"},{"location":"Build/Installation/#Rootpack-Installation","page":"Installation","title":"Rootpack Installation","text":"","category":"section"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"Gmkpack is the ALADIN utility to compile ARPEGE/IFS","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"Handle dependencies (includes/modules)\nHandle exceptions\nCompile the code\nBuild the binaries","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"A mainpack installs and builds a complete HARMONIE source (ALADIN/HIRALD/ALARO/AROME models). Generate a set of pre-compiled libraries (rootpack) and modules available for USE.  Each single user buils their own  local \"target\" pack, which synchronise local source modifications with the reference libraries.  ","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"gmapdoc","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"gmkpack vs make","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"gmkpack is intended to be installed and maintained separately. In HARMONIE it is a part of the system and used in Buildgmkpack, Buildrootpack and Build_pack","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"Available main packs on cca:/project/hirlam/harmonie/pack could look like:","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"37h1_harmonEPS_11389.01.XLF130100000009.x 37h1_harmonEPS_11842.01.XLF130100000009.x 37h1_harmonEPS_11899.01.XLF130100000009.x 37h1_main.01.XLF130100000009.x 37h1_main.02.XLF130100000009.x 38h1_alpha.01.XLF130100000009.x 38h1_alpha.02.XLF130100000009.x 38h1_beta.01.XLF130100000009.x","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"ie. CYCLE_BRANCH.VERIONS.COMPILER_VERSION.OPTION For the latest available packs please check on cca.","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"Supported platforms and compilers see util/gmkpack/arch\nIBM power 7, xlf95\nIntel (g95, gfortran, intel)\nNEC\nCray, ftn","category":"page"},{"location":"Build/Installation/#Configure-your-system","page":"Installation","title":"Configure your system","text":"","category":"section"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"We assume you have a copy of the repository under PATH_TO_HARMONIE. To start the build, create an experiment directory $HOME/hm_home/trunk and run","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"   PATH_TO_HARMONIE/config-sh/Harmonie setup -r PATH_TO_HARMONIE -h YOURHOST","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"The above command creates the following files/directories under","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"   config-sh/hm_rev                         # gives the path to the reference installation, similar to hl_rev in synoptic-Hirlam\n   config-sh/Main                           # a script to enable start Harmonie, similar to Start in synoptic-Hirlam\n   Env_system -> config-sh/config.YOURHOST  # describing your system, similar to Env_system in synoptic-Hirlam\n   Env_submit -> config-sh/submit.YOURHOST  # describing your submit commands, comparable to submission.db in synoptic-Hirlam\n   ecf/config_exp.h                          # defines your experiment, comparable to Env_expdesc in synoptic-Hirlam","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"In ecf/config_exp.h  you may identify the options sent to gmkpack","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"# Definitions about gmkpack, should fit with hm_rev\n      BUILD_ROOTPACK=${BUILD_ROOTPACK-yes}    # Build your own ROOTPACK if it doesn't exists (yes|no)\n                                              # This may take several hours!\n                                              # Make sure you have write permissions in ROOTPACK directory defined in Env_system\n      REVISION=38h1                           # Revision ( or cycle ) number, has to be set even for the trunk!\n      BRANCH=trunk\n      VERSION=01                              # Version of revision/branch to use\n      OPTION=x                                # Which gmkpack/arch/SYSTEM.HOST.OPTION file to use\n      OTHER_PROGRAMS=\"soda pgd blend odbtools bator ioassign odbsql blendsur addsurf surfex mandalay prep lfitools sfxtools\" # Other things to compile with gmkpack","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"You could run something different than defined in hm_rev, but then there would be a mismatch between your source code and the pre-compiled libraries/modules.","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"Identify your system in one of the config files in config-sh or write a new config.YOURHOST definition, make sure that some of the important optional settings are defined in this file:","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":" COMPCENTRE       # should be something else than ECMWF\n HM_DATA          # where you run your experiments and where you \n HM_LIB           # where you find the copy of the source code and the compiled libraries\n ROOTPACK         # where you refer/put your rootpack installations\n HARMONIE_CONFIG  # identifies your configuration\n AUXLIBS/EMOSLIB  # Path to your external libraries","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"Compare e.g. with config-sh/config.ECMWF.atos or config-sh/config.krypton.","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"You also have to identify your system for gmkpack in:","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"        util/gmkpack/arch/YOURMACHINE.HARMONIE_CONFIG","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"The source code for utilities not compiled with gmkpack you find under util. There should be five config files created/edited.","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"        util/gl/config/config.HARMONIE_CONFIG\n        util/gl_grib_api/config/config.HARMONIE_CONFIG\n        util/monitor/config/config.HARMONIE_CONFIG\n        util/oulan/config.HARMONIE_CONFIG\n        util/conrad/config.HARMONIE_CONFIG\n","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"The makefiles themselves should not have to be edited.","category":"page"},{"location":"Build/Installation/#Submission-rules","page":"Installation","title":"Submission rules","text":"","category":"section"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"Next you have to identify your submit file in config-sh or write a new submit.YOURHOST file. This file defines how you submit your jobs in your local batch system.  The routine get_job is called from scr/submission.db and should return the appropriate batch header including some environment variables specifying the parallel decomposition. The way the header is constructed could be different on different hosts as long as the appropriate header is returned.","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"On config-sh/submit.ECMWF.atos four list of jobs are created:","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"backg_list for jobs running as background jobs on the ecFlow server\nscalar_list single PE jobs on hpc-batch (nf queue)\nfrac_list parallel PE jobs on hpc-batch using less than half a node (nf queue)\npar_list for parallel jobs on hpc-batch  (np queue)","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"The scalar_list is the default one, meaning that any unspecified job will be run as a single PE job on HPCE. Default values are defined for each list. Special settings for special jobs can be set like:","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":" $job_list{'Prep_ini_surfex'}{'RESOURCES'} = $submit_type.'resources = ConsumableCPUs(1) ConsumableMemory(6000 MB)' ;","category":"page"},{"location":"Build/Installation/#Running-the-installation","page":"Installation","title":"Running the installation","text":"","category":"section"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"Start the compilation of the rootpack :","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"      PATH_TO_HARMONIE/config-sh/Harmonie install","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"The suite definition file used for the installation is  msms/Install_rootpack.tdf. The installation is just like running an experiment.","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"Where are things happening","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"Build_gmkpack \nBuild_rootpack\nrsync the sources from HM_REV, HM_CMODS, HM_LIB\ncleanpack\ncompiles the ODB compiler\ncompiles IFS\nlocks the pack\nBuild_pack\nbuild everything in the OTHER_PROGRAMS list. This is done inside HM_DATA/gmkpack_build\nbuild the utilities (gl/verobs/oulan)","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"When things goes wrong","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"ics_* can be run stand alone from the ROOTPACK or $HM_DATA/gmkpack_build directory\nTurn off the compilation and linking\nbuild the libraries by running the ics_* file by hand\nBUILD_ROOTPACK=no, put the corrections to the failing code in your experiment\nrerun Harmonie install","category":"page"},{"location":"Build/Installation/","page":"Installation","title":"Installation","text":"The main task for the installation is to compile the rootpack. However, the binaries created are never used by any other experiment.","category":"page"},{"location":"Build/gmkpack/#Compilation-with-gmkpack","page":"Gmkpack","title":"Compilation with gmkpack","text":"","category":"section"},{"location":"Build/gmkpack/#Introduction","page":"Gmkpack","title":"Introduction","text":"","category":"section"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"In the following we describe how to compile Harmonie with gmkapack.","category":"page"},{"location":"Build/gmkpack/#Rootpack-Installation","page":"Gmkpack","title":"Rootpack Installation","text":"","category":"section"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"Gmkpack is the ALADIN utility to compile ARPEGE/IFS","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"Handle dependencies (includes/modules)\nHandle exceptions\nCompile the code\nBuild the binaries","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"A mainpack installs and builds a complete HARMONIE source (ALADIN/ALARO/AROME models). Generate a set of pre-compiled libraries (rootpack) and modules available for USE.  Each single user buils their own  local \"target\" pack, which synchronise local source modifications with the reference libraries.  ","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"gmapdoc","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"gmkpack vs make","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"gmkpack is intended to be installed and maintained separately. In HARMONIE it is a part of the system and used in Build_gmkpack, Build_rootpack and Build_pack","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"Available main packs at ECMWF are c1a:/ms_perm/hirlam/harmonie/pack:","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"32h2_main.01.XLF91.x/32h3_main.01.XLF91.x/33h0_main.01.XLF91.x/33h1_main.01.XLF91.x/33h1_main.03.XLF91.x/34h0_trunk.01.XLF91.x/35h0_trunk.01.XLF91.x/ ie. CYCLE_BRANCH.VERIONS.COMPILER_VERSION.OPTION","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"ecgb/hpce:/home/ms/dk/nhz/harmonie_release : 32h3,33h0,33h1,trunk","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"Supported platforms and compilers: util/gmkpack/arch\nIBM power X\nIntel (g95, gfortran, intel)\nNEC\nFujitsu\nAMD?","category":"page"},{"location":"Build/gmkpack/#Configure-your-system","page":"Gmkpack","title":"Configure your system","text":"","category":"section"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"We assume you have a copy of the repository under PATH_TO_HARMONIE. To start the build, create an experiment directory $HOME/hm_home/trunk and run","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"PATH_TO_HARMONIE/config-sh/Harmonie setup -r PATH_TO_HARMONIE -h YOURHOST","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"Note the options -r and -h here are defined differently from those in the script Hirlam for synoptic-scale Hirlam system.","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"The above command creates the following files/directories under","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"   config-sh/hm_rev                         # gives the path to the reference installation, similar to hl_rev in synoptic-Hirlam\n   config-sh/Main                           # a script to enable start Harmonie, similar to Start in synoptic-Hirlam\n   Env_system -> config-sh/config.YOURHOST  # describing your system, similar to Env_system in synoptic-Hirlam\n   Env_submit -> config-sh/submit.YOURHOST  # describing your submit commands, comparable to submission.db in synoptic-Hirlam\n   ecf/config_exp.h                          # defines your experiment, comparable to Env_expdesc in synoptic-Hirlam","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"In ecf/config_exp.h  you may identify the options sent to gmkpack","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"# ROOTPACK definitions, should fit with hm_rev\nBUILD_ROOTPACK=yes                      # Only active for the Install_rootpack playfile, (yes|no)\nBUILD=yes                               # Turn on or off the compilation and binary build (yes|no)\nREVISION=35h1                           # Revision ( or cycle ) number, has to be set even for the trunk!\nBRANCH=trunk                            # Rootpack branch (usually trunk|main)\nVERSION=01                              # Version of revision/branch to use\nOPTION=x                                # Which gmkpack/arch/SYSTEM.HOST.OPTION file to use\n\nPROGRAM=aromodb                         # Main MODEL Program to compile gmkpack\nOTHER_PROGRAMS=\"pinuts blend odbtools bator ioassign mandalay\" # Other things to compile with gmkpack","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"You could run something different than defined in hm_rev, but then there would be a mismatch between your source code and the pre-compiled libraries/modules.","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"Identify your system in one of the config files in config-sh or write a new config.YOURHOST definition, make sure that some of the important optional settings are defined in this file:","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":" COMPCENTRE       # should be something else than ECMWF for non-HPCE platform\n HM_DATA          # where you run your experiments and keep your \"local\" binaries\n ROOTPACK         # where you refer/put your rootpack installations\n HARMONIE_CONFIG  # identifies your configuration\n AUXLIBS/EMOSLIB  # Path to your external libraries","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"Compare e.g. with config-sh/config.ecgb or config-sh/config.gimle.","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"You also have to identify your system for gmkpack in:","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"        util/gmkpack/arch/YOURMACHINE.HARMONIE_CONFIG","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"The source code for utilities not compiled with gmkpack you find under util. There should be three config files created/edited.","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"        util/gl/config/config.HARMONIE_CONFIG\n        util/monitor/config/config.HARMONIE_CONFIG\n        util/oulan/config.HARMONIE_CONFIG","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"The makefiles themselves should not have to be edited.","category":"page"},{"location":"Build/gmkpack/#Submission-rules","page":"Gmkpack","title":"Submission rules","text":"","category":"section"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"Next you have to identify your submit file in config-sh or write a new submit.YOURHOST file. This file defines how you submit your jobs in your local batch system.  The routine get_job is called from scr/submission.db and should return the appropriate batch header including some environment variables specifying the parallel decomposition. The way the header is constructed could be different on different hosts as long as the appropriate header is returned.","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"On ecgb config-sh/submit.ecgb three list of jobs are created:","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"backg_list for jobs running as background jobs on ecgb\nscalar_list single PE jobs on HPCE\npar_list for parallel jobs on HPCE.","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"The scalar_list is the default one, meaning that any unspecified job will be run as a single PE job on HPCE. Default values are defined for each list. Special settings for special jobs can be set like:","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":" $job_list{'Prep_ini_surfex'}{'RESOURCES'} = $submit_type.'resources = ConsumableCPUs(1) ConsumableMemory(6000 MB)' ;","category":"page"},{"location":"Build/gmkpack/#Running-the-installation","page":"Gmkpack","title":"Running the installation","text":"","category":"section"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"Start the compilation of the rootpack :","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"      PATH_TO_HARMONIE/config-sh/Harmonie install","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"The suite definition file used for the installation is  msms/Install_rootpack.tdf. The installation is just like running an experiment.","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"Where are things happening","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"Build_gmkpack \nBuild_rootpack\nrsync the sources from HM_REV, HM_CMODS, HM_LIB\ncleanpack\ncompiles the ODB compiler\ncompiles IFS\nlocks the pack\nBuild_pack\nbuild everything in the OTHER_PROGRAMS list. This is done inside HM_DATA/gmkpack_build\nbuild the utilities (gl/verobs/oulan)","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"When things goes wrong","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"ics_* can be run stand alone from the ROOTPACK. \nTurn off the compilation and linking\nbuild the libraries by running the ics_* file by hand\nBUILD_ROOTPACK=no, put the corrections to the failing code in your experiment\nrerun Harmonie install","category":"page"},{"location":"Build/gmkpack/","page":"Gmkpack","title":"Gmkpack","text":"The main task for the installation is to compile the rootpack. The binaries and libraries compiled are never used by any other experiment.","category":"page"},{"location":"Verification/Extract4verification/#Verification-preparation","page":"Extract4verification","title":"Verification preparation","text":"","category":"section"},{"location":"Verification/Extract4verification/#Introduction","page":"Extract4verification","title":"Introduction","text":"","category":"section"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"Before we can run the verification we need to extract data for each geographical point and produce files in a format that the verification program can use. In HARMONIE there are two programs, one fore extracting model data (fldextr_grib_api) and one for observations ( obsextr ). Both are part of the util/gl_grib_api. ","category":"page"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"fldextr is capable of extracting data from several sources (HARMONIE/HIRLAM/IFS and produces so called vfld-files in ASCII format. The main tasks of the program is to:","category":"page"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"Recalculates rh,td to be over water\nInterpolates to geographical points according to a synop.list and temp.list\nDoes MSLP,RH2M,TD2M calculations if the are not available in the input file\nOptional fraction of land check.\nInterpolates to pressure levels for TEMP data.","category":"page"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"obsextr extracts conventional observations from BUFR data and creates a vobs file similar to the vfld file. It:","category":"page"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"Reads SYNOP and TEMP\nLUSE_LIST controls the usage of a station list","category":"page"},{"location":"Verification/Extract4verification/#Station-lists-used-by-verification","page":"Extract4verification","title":"Station lists used by verification","text":"","category":"section"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"src/Fldextr links  synop.list to $HM_LIB/util/gl_grib_api/scr/allsynop.list  and temp.list to $HM_LIB/util/gl_grib_api/scr/alltemp.list. These station lists are based on information in WMO's ''Publication No. 9, Volume A, Observing Stations and WMO Catalogue of Radiosondes. This is regularly updated by the WMO. allsynop.list and alltemp.list are updated less frequently. There is also scope to include local stations in these lists that are not included in WMO'sPublication No. 9''. The following 7-digit station identifiers are available to HIRLAM countries:","category":"page"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"Country identifier\nNorway 1000000 - 1099999\nSweden 2000000 - 2099999\nEstonia 2600000 - 2649999\nLithuania 2650000 - 2699999\nFinland 2700000 - 2799999\nIreland 3900000 - 3900000\nIceland 4000000 - 4099999\nGreenland 4200000 - 4299999\nDenmark 6000000 - 6999999\nNetherlands 6200000 - 6299999\nSpain 8000000 - 8099999","category":"page"},{"location":"Verification/Extract4verification/#Field-extraction","page":"Extract4verification","title":"Field extraction","text":"","category":"section"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"scr/Fldextr This script goes through all forecast files and  collects all the variables (T2m, V10m, mean sea level pressure, RH2m, Q2m, total cloudiness, precipitation + profiles)  needed in basic verification.","category":"page"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"Input parameters: none.\nData: Forecast files.\nNamelists: Station lists for surface data (ewglam.list) and radiosounding data (temp.list).\nExecutables: fldextr.\nOutput: Field extraction files (vfld${EXP}${DTG}), which are placed in EXTRARCH.","category":"page"},{"location":"Verification/Extract4verification/#Extract-observations","page":"Extract4verification","title":"Extract observations","text":"","category":"section"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"scr/FetchOBS scripts takes care of the observation  extraction for verification. First, the observation BUFR-file is fetched from the MARS (ExtractVEROBSfromMARS),  then all the needed data is extracted from the BUFR-files.","category":"page"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"Input parameters: none.  \nData:  Station lists for surface data (ewglam.list) and radiosounding data (temp.list). These shoud be found from SCRDIR  \nExecutables: mars, obsextr.  \nOutput: Field extraction files (vobs*), which are placed in EXTRARCH.","category":"page"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"For the continuous monitoring on hirlam.org the most recent data are kept online at ECMWF under ecgb:/scratch/ms/dk/nhz/OBS.","category":"page"},{"location":"Verification/Extract4verification/#A-general-input-format","page":"Extract4verification","title":"A general input format","text":"","category":"section"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"The file format for verification is a simple ascii file with a header that allows an arbitrary number of different types of point data to be included in the model vfld- or observation vobs- files.","category":"page"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"The generalized input format is defined as ","category":"page"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"nstation_synop nstation_temp version_flag  # written in fortran format '(1x,3I6)' )\n# where version_flag == 4\n# If ( nstation_synop > 0 ) we read the variables in the file, their descriptors and\n# their accumulation time\n#\nnvar_synop\nDESC_1 ACC_TIME_1\n...\nDESC_nvar_synop ACC_TIME_nvar_synop\n# Station information and data N=nstation_synop times\nstid_1 lat lon hgt val(1:nvar_synop)\n...\nstid_N lat lon hgt val(1:nvar_synop)\n\n# If ( nstation_temp > 0 )\nnlev_temp\nnvar_temp\nDESC_1 ACC_TIME_1\n..\nDESC_nvar_temp ACC_TIME_nvar_temp\n# Station information and data nstation_temp times\n# and station data nlev_temp times for each station\nstid_1 lat lon hgt\npressure(1) val(1:nvar_temp)\n...\npressure(nlev_temp) val(1:nvar_temp)\nstid_2 lat lon hgt\n...","category":"page"},{"location":"Verification/Extract4verification/","page":"Extract4verification","title":"Extract4verification","text":"The accumulation time allows us to e.g. easily include different precipitation accumulation intervals.","category":"page"},{"location":"Overview/FileFormats/#File-formats-in-HARMONIE","page":"File Formats","title":"File formats in HARMONIE","text":"","category":"section"},{"location":"Overview/FileFormats/#Introduction","page":"File Formats","title":"Introduction","text":"","category":"section"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"The HARMONIE system reads and writes a number of different formats. ","category":"page"},{"location":"Overview/FileFormats/#FA-files","page":"File Formats","title":"FA files","text":"","category":"section"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"Default internal format input/output for HARMONIE for gridpoint, spectral and SURFEX data. GRIB is used as a way to pack data, but the grib record cannot be used as such.","category":"page"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"The header contains information about model domain, projection, spectral truncation, extension zone, boundary zone, vertical levels. \nOnly one date/time per file.\nFA routines are found under ifsaux/fa\nList or convert a file with gl\nOther listing tool PINUTS","category":"page"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"Read more","category":"page"},{"location":"Overview/FileFormats/#GRIB/GRIB2","page":"File Formats","title":"GRIB/GRIB2","text":"","category":"section"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"All FA files may be converted to GRIB after the forecast run. For the conversion between FA names and GRIB parameters check this table.","category":"page"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"List or convert a GRIB file with gl","category":"page"},{"location":"Overview/FileFormats/#NETCDF","page":"File Formats","title":"NETCDF","text":"","category":"section"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"In climate mode all FA files may converted to NETCDF after the forecast run. For the conversion between FA names and NETCDF parameters check util/gl/inc/nc_tab.h.","category":"page"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"For the manipulation and listing of NETCDF files we refer to standard NETCDF tools.\nNETCDF is also used as output data from some SURFEX tools.","category":"page"},{"location":"Overview/FileFormats/#BUFR-and-ODB","page":"File Formats","title":"BUFR and ODB","text":"","category":"section"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"BUFR is the archiving/exchange format for observations. Observation Database is used for efficient handling of observations on IFS. ODB used for both input data and feedback information.","category":"page"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"Read more about observations in HARMONIE here.","category":"page"},{"location":"Overview/FileFormats/#DDH-(LFA-files-)","page":"File Formats","title":"DDH (LFA files )","text":"","category":"section"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"Diagnostics by Horizontal Domains allows you to accumulate fluxes from different packages over different areas/points. ","category":"page"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"LFA files ( Autodocumented File Software )\ngmapdoc\nunder util/ddh","category":"page"},{"location":"Overview/FileFormats/#Misc","page":"File Formats","title":"Misc","text":"","category":"section"},{"location":"Overview/FileFormats/","page":"File Formats","title":"File Formats","text":"vfld/vobs files in a simple ASCII format used by the verification.\nObsmon files are stored in sqlite format.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#Generation-of-climate-and-physiography-files","page":"Climate Generation","title":"Generation of climate and physiography files","text":"","category":"section"},{"location":"ClimateGeneration/ClimateGeneration/#Introduction","page":"Climate Generation","title":"Introduction","text":"","category":"section"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"The generation of climate files includes two parts. The first part is the generation of climate files for the atmospheric model, the so called  e923 configuration. The second part is the generation of the physiography information for  SURFEX. In the following we describe how it is implemented in HARMONIE.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#Input-data-for-climate-generation","page":"Climate Generation","title":"Input data for climate generation","text":"","category":"section"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"The location of your input data for the climate generation is defined by the HM_CLDATA environment variable defined in the config-sh/config.yourhost. At ECMWF the climate data is stored on Atos here: hpc-login:/ec/res4/hpcperm/hlam/data/climate","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"Information on what data to download is available here. The input data contains physiography data, topography information and climatological values determined from a one year ARPEGE assimilation experiment with a resolution of T79. ","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"In the current version the option to use pre-generated climate files has been introduced to save time for quick experiments. To use pre-generated domains you need to set USE_REF_CLIMDIR=yes in Env_system. The regenerated domains location is defined in config_exp.h and in ECMWF are located in REF_CLIMDIR=ec:/hlam/harmonie_climdir/release-43h2.1.rc1/$DOMAIN/$ECOCLIMAP_VERSION.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#Preparation-of-SURFEX-physiography-file","page":"Climate Generation","title":"Preparation of SURFEX physiography file","text":"","category":"section"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"SURFEX needs information about the distribution of different available tiles like nature, sea, water and town. The nature tile also needs information about type of vegetation and soiltypes. The main input sources for this are found at SURFEX physiographic maps.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"The data base for SURFEX-file preparation is located under HM_CLDATA/PGD","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"ecoclimats_v2.* : Landtypes\ngtopo30.* : Topography\nsand_fao.* : Soil type distribution\nclay_fao.* : Soil type distribution","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"The generation of SURFEX physiography file (PGD.lfi) is done in scr/Prepare_pgd. The script creates the namelist OPTIONS.nam based on the DOMAIN settings in scr/Harmonie_domains.pm. Note that the SURFEX domain is only created over the C+I area. In the namelist we set which scheme that should be activated for each tile.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"     Tile\nPHYSICS Nature Sea Water Town \nAROME ISBA SEAFLX WATFLX TEB \nALARO ISBA SEAFLX WATFLX Town as rock ","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"The program PGD produces one SURFEX physiography file PGD.lfi, which is stored in CLIMDIR directory.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"To make sure we have the same topography input for the atmospheric part we call Prepare_pgd two times. One time to produce a PGD.lfi for SURFEX and a second time to produce a PGD.fa file that can be used as input for the climate generation described below. Note that for the atmosphere the topography will be spectrally filtered and the resulting topography will be imposed on SURFEX again.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#Generation-of-a-non-SURFEX-climate-file","page":"Climate Generation","title":"Generation of a non SURFEX climate file","text":"","category":"section"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"scr/Climate is a script, which prepares climate file(s) for  prefered forecast range. Climate files are produced for past, present and following month. The outline of Climate is as follows:","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"Check if climate files already exists.\nCreation of namelists. The definition of domain and truncation values is taken from src/Harmonie_domains.pm.\nPart 0: Read the PGD.fa file generated by SURFEX and write it to Neworog\nPart 1: Filter  Neworog to target grid with spectral smoothing to remove 2dx waves.\nPart 2: generation of surface, soil and vegetation variables, without annual variation.\nPart 3: creation of monthly climatological values and  modification of albedo and emissivity according to the climatology of sea-ice limit.\nPart 4: definition and modification of the vegetation and surface characteristics\nPart 5: modification of fields created by step 2 and 4 over land from high resolution datasets (for each month)\nPart 6: modification of climatological values","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"The result is climate files for the previous, current and next month. The files are named after their month like m01, m02 - m12 and stored in CLIMDIR.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/","page":"Climate Generation","title":"Climate Generation","text":"Further reference e923","category":"page"},{"location":"DataAssimilation/SingleObs/#Single-observation-impact-experiment","page":"Single Obs","title":"Single observation impact experiment","text":"","category":"section"},{"location":"DataAssimilation/SingleObs/#General","page":"Single Obs","title":"General","text":"","category":"section"},{"location":"DataAssimilation/SingleObs/","page":"Single Obs","title":"Single Obs","text":"The results of single observation impact experiment provide useful information of the observation operator and error statistics. Among others, it is a useful tool for diagnosing background error statistics. The procedure described below is the recommended one and it has been tested on HARMONIE harmonie-43h21, with some modifications. The example below with the new system is for a AROME domain covering Denmark (DOMAIN=DKCOEXP). Three TEMP observation types have been implemented in scr/Create_singe_obs as deviations to the background:","category":"page"},{"location":"DataAssimilation/SingleObs/","page":"Single Obs","title":"Single Obs","text":"A temperature increase of 1K\nA wind speed increase of 1 m/s\nA specific humidity reduction to 90% of the background.","category":"page"},{"location":"DataAssimilation/SingleObs/#Illustrative-example-of-single-observation-impact-experiment-on-ecgb/cca","page":"Single Obs","title":"Illustrative example of single observation impact experiment on ecgb/cca","text":"","category":"section"},{"location":"DataAssimilation/SingleObs/","page":"Single Obs","title":"Single Obs","text":"create hm_home/sinob directory. Then cd hm_home/sinob.\ncreate experiment by typing ~hlam/Harmonie setup -r ~hlam/harmonie_release/tags/harmonie-43h2.1.\nEdit ecf/config_exp.h  as follows:\nset ANASURF=none,\nset SINGLEOBS=yes,\nset LSMIXBC=no,\nEdit scr/include.ass  as follows:\nset USEOBSOUL=1,\nCopy a correction of the file Createsingleobs by typing \ncp ~hlam/harmone_release/git/develop/scr/Create_single_obs scr/Create_single_obs\nCopy a correction for gl\nmkdir -p util/gl/mod/ ; cp ~hlam/harmone_release/git/develop/util/gl/mod/module_rotations.f90 util/gl/mod\nLaunch the single observation impact experiment by standing in hm_home/sinob typing:\n~hlam/Harmonie start DTG=2012061003 DTGEND=2012061006\nThe resulting analysis file be found as cca:$SCRATCH/hm_home/sinob/archive/2012/06/10/06/MXMIN1999+0000. You can now diagnose the 3D-VAR analysis increments of the sinob-experiment taking the difference between the analysis  MXMIN1999+0000 (analysis) and the first guess, cca:$SCRATCH/hm_home/sinob/archive/2012/06/10/03/ICMSHHARM+0003. Plot horizontal and vertical cross-sections of temperature and other variables using your favorite software (EpyGram for example). ","category":"page"},{"location":"DataAssimilation/SingleObs/","page":"Single Obs","title":"Single Obs","text":"Note that you can change position of observation, observation error, variable to be observed etc. Investigate these options by taking a closer look at the script Createsingleobs.","category":"page"},{"location":"DataAssimilation/SingleObs/","page":"Single Obs","title":"Single Obs","text":"Read more about radiance single observation experiments here. In ec:/smx/sinob_wiki_ml you will also find OBSOUL_amsua7, a file for generating a satellati radiance amsu a channel 7 single observation impact experiment.","category":"page"},{"location":"DataAssimilation/Surface/CANARI/#Surface-Data-Assimilation-Scheme:-Canari","page":"CANARI","title":"Surface Data Assimilation Scheme: Canari","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI/#Introduction","page":"CANARI","title":"Introduction","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"(by Alena.Trojakova)","category":"page"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"CANARI stands for Code for the Analysis Necessary for ARPEGE for its Rejects and its Initialization. It is software (part of IFS/ARPEGE source code) to produce an ARPEGE/ALADIN analysis based on optimum interpolation method. The number of ARPEGE/ALADIN configuration is 701. CANARI has the two main components the quality control and an analysis. According to the type of observations the analysis can be:","category":"page"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"3D multivariate for U, V, T, Ps\n3D univariate for RH\n2D univariate for 2m/10m fields\nsoil parameters analysis is based on 2m increments","category":"page"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"CANARI can handle following 10 types of observations:","category":"page"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"SYNOP: Ps, T2m, RH2m, 10m Wind, RR, Snow depth, SST\nAIREP: P ( or Z), Wind, T\nSATOB: P, Wind, T - from geostationary satellite imagery\nDRIBU: Ps, T2m, 10m Wind, SST\nTEMP: P, Wind, T, Q\nPILOT: Wind with the corresponding Z, (sometimes 10m Wind)\nSATEM: Q, T retrieved from radiances- surface","category":"page"},{"location":"DataAssimilation/Surface/CANARI/#Applications","page":"CANARI","title":"Applications","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"Diagpack - diagnostic of mesoscale features via detailed analysis of PBL using high resolution surface observation with specific tunings:","category":"page"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"REF_S_T2  = 3.0,\nREF_S_H2  = 0.20,\nREF_A_H2  = 40000.,\nREF_A_T2  = 40000.,\n...","category":"page"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"Quality control and verification - VERAL package\nSurface analysis","category":"page"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"Detailed description of the method and technical documentation can be found in the References below. Here follows basic input/output summary and command line arguments.","category":"page"},{"location":"DataAssimilation/Surface/CANARI/#CANARI-procedure","page":"CANARI","title":"CANARI procedure","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"All optional items are controlled by various keys in namelist.","category":"page"},{"location":"DataAssimilation/Surface/CANARI/#INPUTs","page":"CANARI","title":"INPUTs","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"The first guess file\nln -s guess ICMSHANALINIT\nln -s guess ELSCFANALALBC000\nThe observation database, which requires special variables to be exported\nexport ODB_STATIC_LINKING=1\nexport TO_ODB_ECMWF=0\nexport ODB_CMA=ECMA    ... Database type (ECMA extended CCMA compressed)\nexport ODB_SRCPATH_ECMA=...\nexport ODB_DATAPATH_ECMA=...\nexport IOASSIGN=\nexport ODB_MERGEODB_DIRECT= ... optional direct ODB merge, If your ODB was not merged previously use  1\nConcerning the observation use, another file is necessary, but it is without any interest for CANARI (just part of variational analysis code is not controlled by a logical keyt !) The file can be obtained on \"tori\" via gget var.misc.rszcoef_fmt.01.\nln -s rszcoef_fmt var.misc.rszcoef_fmt.01\nThe climatological files\nln  -s  climfile_${mm}  ICMSHANALCLIM\nln  -s  climfile_${mm2} ICMSHANALCLI2\nThe namelist file\nln -s namelist fort.4 \nThe ISBA files\nfile used to derive soil moisture from 2m increment\nln -s POLYNOMES_ISBA fort.61              \nOPTIONAL assimilated increments files to smooth the fields (ICMSHANALLISSEF file is created at the and of analysis )\nln -s increment_file ICMSHANALLISSE     \nOPTIONAL The SST file - an interpolated NCEP SST analysis on ARPEGE grid and stored in FA file format) used for relaxation towards \"up-dated\" climatology\nln -s SST_file ICMSHANALSST        \nOPTIONAL The error statistic file; OI allows to know the variance of the analysis error, which can be used to improve background error next cycle => an option to use \"dynamics\" statistics instead of fixed. Output file ICMSHANALSTA2 is produced with statistics for the current run\nln -s statistics_file ICMSHANALSTA\nOPTIONAL The incremental mode files (global option only); it is possible to read 3 input file to build non-classical init; the combination is done on spectral fields only: G1=G0+A-G\nln -s G0_file ICMSHANAINIT\nln -s A_file ICMSHANALANIN\nln -s G_file ICMSHANALFGIN","category":"page"},{"location":"DataAssimilation/Surface/CANARI/#run-CANARI","page":"CANARI","title":"run CANARI","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"MASTERODB -c701 -vmeteo -maladin -eANAL -t1. -ft0 -aeul","category":"page"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"-c: configuration number (CANARI = 701)\n-v: version of the code (always \"meteo\" for ARPEGE/ALADIN)\n-m: LAM or global model (\"aladin\" or \"arpege\")\n-e: experiment name (ANAL for instance)\n-t: time-step length (no matter for CANARI, usually \"1.\", avoid 0.)\n-f: duration of the integration (t0 or h0 for CANARI)\n-a: dynamical scheme (does not matter for CANARI Eulerian = eul or semi-Lagrangian = sli (sli as usual))","category":"page"},{"location":"DataAssimilation/Surface/CANARI/#OUTPUTs","page":"CANARI","title":"OUTPUTs","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"OPTIONAL The analysis file\nICMSHANAL+0000\nOPTIONALLY updated observational database\nOPTIONAL The error statistics file\nICMSHANALSTA2\nOPTIONAL The increment file\nICMSHANALLISSEF\nThe output listing - enables checking of various parameters, e.g. number observation of given type (SYNOP,TEMP,..), number of used observation parameters (T2m, RH2m, T, geop., ...), some namelist variables, various control prints (O-G and O-A statistics, ...), grid-point and spectral norms.","category":"page"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"NODE*","category":"page"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"Sample of script is attached.","category":"page"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"As a part of the system training in Copenhagen in 2008, Roger prepared an intoduction to CANARI, which is found in HarmonieSystemTraining2008/Lecture/SurfaceAssimilation on hirlam.org","category":"page"},{"location":"DataAssimilation/Surface/CANARI/#References","page":"CANARI","title":"References","text":"","category":"section"},{"location":"DataAssimilation/Surface/CANARI/","page":"CANARI","title":"CANARI","text":"F. Taillefer, 2002: Canari - technical documentation\nF. Bouttier and P. Courtier, 1999: Data assimilation concepts and methods\nF. Bouyssel - presentation from Budapest","category":"page"},{"location":"EPS/SLAF/#SLAF-in-HarmonEPS","page":"SLAF","title":"SLAF in HarmonEPS","text":"","category":"section"},{"location":"EPS/SLAF/#Background","page":"SLAF","title":"Background","text":"","category":"section"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"SLAF stands for Scaled Lagged Average Forecasting (Ebisuzaki & Kalnay, 1991) and it is a technique used to easily generate perturbed boundary and initial conditions from a single deterministic model (HRES).","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"The general idea of SLAF is that perturbations are taking HRES  forecasts valid at the same time but with different forecast lengths and initial times :","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"IC_m = A_c + K_m * ( IFS_0  IFS_N )","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"BC_m = IFS_0 + K_m * ( IFS_0  IFS_N )","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"Where IC_m is the initial condition for member m, BC_m is the lateral boundary condition for member m, A_c is the control analysis, K_m a scaling factor, IFS_0 is the latest available IFS forecast and N is the forecast length for an earlier forecast valid at the same time.","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"This first attempt on using SLAF revealed an undesirable clustering between the different members using positive or negative perturbations. Depending on the sign of K_m the members gather on either side of the mean. If HRES has a increasing bias over the forecast length the same bias will be introduced through the perturbations. A cure of this problem is to use shorter forecast lengths and construct the perturbations by two consecutive forecasts 6 hours apart:","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"IC_m = A_c + K_m * ( IFS_N  IFS_N-6 )","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"BC_m = IFS_0 + K_m * ( IFS_N  IFS_N-6 )","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"where IFS_N is a forecast with length N and IFS_N-6 is a 6h shorter forecast, both valid at the same time as the analysis. With this construction most of the clustering is gone. THIS IS THE DEFAULT SETUP IN HarmonEPS cy40.","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"From the equation it is clear that every lag used generates two perturbations, so if we use deterministic runs from 06, 12, 18, 24 and 30 hours before we'll have 10 different perturbed members and control, then 11 members in total. ","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"The goal is to have similar spread at the boundaries of the LAMEPS than using pure downscaling but with less communication time.","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"Sotfware to use SLAF technique in HarmonEPS was introduced in HarmonEPS branch in version 38h1.1 and first tested by Jose A. Garcia-Moya.","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"The main advantage of SLAF is that only needs having stored the last runs of the deterministic model and, in daily run mode at home, we only need to have access to the last ECMWF (or any other global model) run avoiding a lot of time in communications compared with the pure downscaling technique.","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"Summarizing, to run an HarmonEPS experiment using SLAF (default in cy40) you have to:","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"Refer to HarmonEPS branch 38h1.1 (minimum). (Constant 6h lag as described above from cy40)\nIn ecf/config_exp.h  choose:\nBDSTRATEGY=simulate_operational\nENSMSEL=0-10 (or whatever you want)\nSLAFLAG=1\nSLAFK=1\nIn msms/harmonie.pm:\n'ENSBDMBR' => [ 0],\n'SLAFLAG'  => [    0,    6,     6,    12,    12,  18,     18,   24,    24,    30,    30],\n'SLAFDIFF' => [    0,    6,     6,     6,     6,    6,     6,    6,     6,     6,     6],\n'SLAFK'    => ['0.0','1.75','-1.75','1.5','-1.5','1.2','-1.2','1.0','-1.0','0.9','-0.9'] (example used for constant 6h lag)","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"Where SLAFLAG represent the lags of every member of the ensemble and SLAF are the different scales (including  the sign). Theoretically the user may set SLAFK as she/he likes but it seems to be more convenient choosing ± in order  to keep symmetry of the perturbations around control. Note that to get equally sized perturbations SLAFK is required to be tuned, please see below for details.","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"Note that no further control on the equilibrium among the perturbed variables (temperature, humidity and so on) is done, so perhaps you have to check the spin-up if you are interested in the first hour of the ensemble.","category":"page"},{"location":"EPS/SLAF/#Tuning-of-SLAF","page":"SLAF","title":"Tuning of SLAF","text":"","category":"section"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"The correct size of SLAFK can be determined by the perturbation diagnostics done in PertAna (harmonie-40h1.1) or Pertdia (harmonie-40h1.2). Here xtool is used to calculate the differences between the boundary files for the control and the individual member. The output is then collected in the HM_Date*.html files or HM_Postprocessing*.html for harmonie-40h1.1 or harmonie-40h1.2 respectively. ","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"Check perturbations for member 004 against 000\nSLAFLAG=18 SLAFK=1.4 SLAFDIFF=6\n...\nStart check boundary 048\n...\n            NAME |  PAR    LEV TYP                                    DESC          BIAS          RMSE           MIN           MAX\nS001TEMPERATURE  |  011   001 109                             Temperature   89.367E-003  286.398E-003 -807.670E-003  861.502E-003\n...\nSURFPRESSION     |  001   000 105                        Surface pressure   50.062E+000  166.547E+000 -694.022E+000  762.069E+000","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"A parser Get_pertdia.pl , working for harmonie-40h1.2, is attached and should be used like in this example from MetCoOp","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"\n./Get_pertdia.pl SURFPRESSION HM_Postprocessing_201704??00.html HM_Postprocessing_201704??12.html\nScanning HM_Postprocessing_2017040100.html \n...\nScanning HM_Postprocessing_2017040512.html \nPARAMETER:SURFPRESSION\nMBR  HH  SLAFLAG  SLAFDIFF SLAFK     BIAS      RMSE      STDV     CASES\n003  00  18       06       -1.40      0.21     59.86     59.86   11 \n003  06  18       06       -1.40     -5.67     67.37     67.13   11 \n...\n009  42  36       06        0.95     28.92    130.72    127.48   11 \n009  48  36       06        0.95     14.80    176.10    175.48   11 \n","category":"page"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"The SLAKF can then be adjusted to achieve a uniform level of STDV for all member. Note that the response may be different for different seasons and will vary between IFS versions. An example of SLAF diagnostics from MetCoOp can be seen in the figure below","category":"page"},{"location":"EPS/SLAF/#Examples","page":"SLAF","title":"Examples","text":"","category":"section"},{"location":"EPS/SLAF/","page":"SLAF","title":"SLAF","text":"Below is an example for 2016052006 for the two different approaches of SLAF described above:","category":"page"},{"location":"DataAssimilation/CHKEVO/#ECHKEVO","page":"CHKEVO","title":"ECHKEVO","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/#Introduction","page":"CHKEVO","title":"Introduction","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"This page describes how to activate CHKEVO for diagnosing forecast model spin-up of pressure. This diagnostic is available in trunk from r16488. Yann Michel (MF) kindly suggested some of the changes required. The diagnostics are generated as part of a forecast model run up to 3 h or 6 h. A known problem is that the method fails when the first lateral boundary conditions are read by the model. The suggestion is to use BDINT=3 and forecast length 3 h. FULL-POS should also be deactivated in config_exp.h.","category":"page"},{"location":"DataAssimilation/CHKEVO/#Preparations","page":"CHKEVO","title":"Preparations","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"It is assumed you already have a well defined experiment called your_exp. The following instructions are valid for a 3h diagnostic forecast.","category":"page"},{"location":"DataAssimilation/CHKEVO/#NAMCHK-namelist","page":"CHKEVO","title":"NAMCHK namelist","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"Enable CHKEVO in the namelist (in the %arome entries):\ncd $HOME/hm_home/your_exp\n~hlam/Harmonie co nam/harmonie_namelists.pm\nEdit NAMCHK:\nNAMCHK=>{\n'LECHKEVO' => '.TRUE.,',\n'LECHKTND' => '.TRUE.,',\n'LECHKPS' => '.TRUE.,',\n},","category":"page"},{"location":"DataAssimilation/CHKEVO/#ecf/config_exp.h","page":"CHKEVO","title":"ecf/config_exp.h","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"Edit your ecf/config_exp.h  as follows:\nPOSTP=\"none\"                          # Postprocessing by Fullpos (inline|offline|none).\nBDINT=3\nHH_LIST=\"00-21:3\"                     # Which cycles to run, replaces FCINT\nLL_LIST=\"3\"                           # Forecast lengths for the cycles [h], replaces LL, LLMAIN\nAlternatively for a 6 h diagnostic forecast:\nPOSTP=\"none\"                          # Postprocessing by Fullpos (inline|offline|none).\nBDINT=6\nHH_LIST=\"00-18:6\"                     # Which cycles to run, replaces FCINT\nLL_LIST=\"6\"                           # Forecast lengths for the cycles [h], replaces LL, LLMAIN","category":"page"},{"location":"DataAssimilation/CHKEVO/#Results","page":"CHKEVO","title":"Results","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"After running the forecast with CHKEVO activated the statistics of surface pressure tendencies are written to NODE.001_01 log file. This log file is included in the HM_Date_YYYYMMDDHH.html log file (written to $SCRATCH/hm_home/your_exp/archive/log on ecgate). The results can be obtained by grepping the log file as follows:","category":"page"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"grep \"^ CHKEVO : \" HM_Date_2013041118.html | tail -n +2","category":"page"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"This gives the RMS and AVG of pressure tendency for each time step. (The first line is removed as the reading of the start file produces zeros):","category":"page"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":" CHKEVO :   2.5683273661035013       0.42575646791552352     \n CHKEVO :   2.5432078820872874       0.36700119757663685     \n CHKEVO :   1.4402533781888094       0.23533175032737094     \n CHKEVO :   1.3677546254375832       0.22965677860570116     \n CHKEVO :   1.1506125378848564       0.20575065246468008     \n CHKEVO :  0.98597708942270756       0.19299583141063531\n.....","category":"page"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"The first column contains the string CHKEVO :\nSecond column contains the RMS of dps/dt averaged over the domain.\nSecond column contains the AVG of dps/dt averaged over the domain.","category":"page"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"The RMS of dps/dt alone can be extracted with:","category":"page"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"grep \"^ CHKEVO : \" HM_Date_2013041118.html | tail -n +2 | awk '{print $3}'","category":"page"},{"location":"DataAssimilation/CHKEVO/#Plotting","page":"CHKEVO","title":"Plotting","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"Some instructions here ...","category":"page"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"grep \"^ CHKEVO : \" HM_Date_2013041118.html | tail -n +2 > dps.dat\nwget https://hirlam.org/trac/raw-attachment/wiki/HarmonieSystemDocumentation/CHKEVO/plotCHKEVO.py\nchmod 755 ./plotCHKEVO.py\n./plotCHKEVO.py -h\n./plotCHKEVO.py -i ps.dat  && eog plot.png\n# OR\n./plotCHKEVO.py -i ps.dat -t 75  && eog plot.png","category":"page"},{"location":"DataAssimilation/CHKEVO/","page":"CHKEVO","title":"CHKEVO","text":"To get the unit of hPa/3h the time-step is taken into account using the -t option. If, for example, the time-step is 60 s the values in the second column will be multiplied with 60.*3. and then divided by 100.","category":"page"},{"location":"PostProcessing/Fullpos/#Postprocessing-with-FULL-POS","page":"FullPos","title":"Postprocessing with FULL-POS","text":"","category":"section"},{"location":"PostProcessing/Fullpos/#Introduction","page":"FullPos","title":"Introduction","text":"","category":"section"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"FULL-POS is a powerful postprocessing package, which is part of the  common ARPEGE/IFS cycle. FULL-POS is documented by ","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"Yessad (2011): This documentation describes the software FULL-POS doing post-processing on different kind of vertical levels. In particular, post-processable variables and organigramme are given. Some aspects of horizontal and vertical interpolators (which may be used in some other applications) are also described.\nykfpos38.pdf: FULL-POS in cycle 38\nEl Khatib (2002): Older documentation with a link to an old FULL-POS website.","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"FULL-POS is a special configuration (9xx) of the full model for setup and initialization. In other words it is a 0 hour forecast, with extra namelist settings for variables to (post)process and to write out. When generating initial or boundary files we are calling a special configuration of FULL-POS, e927.","category":"page"},{"location":"PostProcessing/Fullpos/#ecf/config_exp.h","page":"FullPos","title":"ecf/config_exp.h","text":"","category":"section"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"The use of FULL-POS is controlled by the POSTP variable inecf/config_exp.h:","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"POSTP=\"inline\"                          # Postprocessing by Fullpos (inline|offline|none).\n                                        # See Setup_postp.pl for selection of fields.\n                                        # inline: this is run inside of the forecast\n                                        # offline: this is run in parallel to the forecast in a separate task","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"inline is the default which means FULL-POS postprocessing is called from the forecast model as it runs. If you select offline the model is called independently of the running forecast model using the forecast model output files as inputs to be postprocessed. By selecting none no FULL-POS postprocessing will be carried out.","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"Output frequency by FULL-POS is controlled by PWRITUPTIMES, FPOUTINT and FREQ_RESET:","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"# Postprocessing times (space separated list)\nPWRITUPTIMES=\"03 06 09 12 15 18 21 24 30 36 42 48 54 60\"\nFPOUTINT=\"-1\"                           # Regular  interval if > 0. Not used if <= 0.\n\nFREQ_RESET=3                            # Reset frequency of max/min values in hours, controls NRAZTS","category":"page"},{"location":"PostProcessing/Fullpos/#FULL-POS","page":"FullPos","title":"FULL-POS","text":"","category":"section"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"The use of FULL-POS is controlled by the following namelists: NAMFPPHY, NAMFPDY2, NAMFPDYP, NAMFPDYH, NAMFPDYI, NAMFPDYV, NAMFPDYT and NAMFPDYS. These namelists can be used to make an accurate list of post- processed fields. It is possible to call FULL-POS when running the model. In such a configuration we can configure HARMONIE to write some parameters with a different frequency than the standard historical files produced. ","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"However, with inline postprocessing, it is possible to get, at each post-processing time step, exactly the fields you wish. In this case, you have to make other namelists file which will contain the selection of the fields you wish to get. First, you have to set in NAMCT0 the variable CNPPATH as the directory where the selection files will be. Under this directory, the name of a selection file must be xxtDDDDHHMM, where DDDDHHMM specifies the date/time of the post-processing time step.","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"The Postpp script executes the offline FULL-POS postprocessing in HARMONIE system. ","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"The management of FULL-POS and the creation of selection files is made easier for the user by scr/Select_postp.pl. ","category":"page"},{"location":"PostProcessing/Fullpos/#Parameter-selection","page":"FullPos","title":"Parameter selection","text":"","category":"section"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"Some of the more relevant entries in this script:","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"CFP2DF: names of the 2D dynamics fields to be post-processed. Names have at maximum 16 characters. By default CFP2DF contains blanks (no 2D dynamical field to post-process).\nCFP3DF: names of the 3D dynamics fields to be post-processed. Names have at maximum 12 characters. By default CFP3DF contains blanks (no 3D dynamical field to post-process).\nCFPPHY: names of the physical fields to be post-processed. Names have at maximum 16 characters. By default CFPPHY contains blanks (no physical field to post-process). \nCFPXFU:  names of the instantaneous fluxes fields to be post-processed. Names have at maximum 16 characters. By default CFPXFU contains blanks (no instantaneous fluxes field to post-process).\nCFPCFU: names of the cumulated fluxes fields to be post-processed. Names have at maximum 16 characters. By default CFPCFU contains blanks (no cumulated fluxes field to post-process).","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"You can define the levels you wish to output using the following variable:","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"NRFP3S: model levels to postprocess\nRFP3H: height above ground levels to postprocess\nRFP3P: pressure levels to postprocess\nRFP3PV: PV levels to postprocess\nRFP3I: temperature levels to postprocess","category":"page"},{"location":"PostProcessing/Fullpos/#Add-new-output","page":"FullPos","title":"Add new output","text":"","category":"section"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"This section provides a simple example on how to add a new parameter/vertical level for postprocessing in scr/Select_postp.pl.","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"To add new \"height above ground\" output at 150m to the FULL-POS output, two changes are required:","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"Add the new height, 150., to the RFP3H array\nAdd level array number to the @namfpdyh_lev level selection","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"cd $HOME/hm_home/levexp\n$PATH_TO_HARMONIE/config-sh/Harmonie co scr/Select_postp.pl\ncp scr/Select_postp.pl scr/Select_postp.pl.ori\n### edit scr/Select_postp.pl\ndiff scr/Select_postp.pl scr/Select_postp.pl.ori\n83c83\n<  RFP3H => ['20.','50.','100.','150.','250.','500.','750.','1000.','1250.','1500.','2000.','2500.','3000.'],\n---\n>  RFP3H => ['20.','50.','100.','250.','500.','750.','1000.','1250.','1500.','2000.','2500.','3000.'],\n132c132\n<  @namfpdyh_lev = (1,2,3,4,5,6,7,8,9,10,11,12,13) ;\n---\n>  @namfpdyh_lev = (1,2,3,4,5,6,7,8,9,10,11,12) ;","category":"page"},{"location":"PostProcessing/Fullpos/#Expert-users","page":"FullPos","title":"Expert users","text":"","category":"section"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"In the FULL-POS namelist NAMFPC (variables explained in src/arp/module/yomfpc.F90), the variables are placed into different categories:","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"LFPCAPEX: if true XFU fields used for CAPE and CIN computation (with NFPCAPE).\nLFPMOIS: month allowed for climatology usage:\n.F. => month of the model (forecast).\n.T. => month of the file.\nNFPCLI:  usage level for climatology:\n0: no climatology\n1: orography and land-sea mask of output only\n2: all available climatological fields of the current month\n3: shifting mean from the climatological fields of the current month to the ones of the closest month\nNFPCAPE: kind of computation for CAPE and CIN:\n1 => from bottom model layer\n2 => from the most unstable layer\n3 => from mto standard height (2 meters) as recomputed values\n4 => from mto standard height (2 meters) out of fluxes (for analysis)\nCFPFMT:  format of the output files, can take the following values:\n’MODEL’ for output in spherical harmonics.\n’GAUSS’ for output in grid-point space on Gaussian grid (covering the global sphere).\n’LELAM’ for output on a grid of kind ALADIN (spectral or grid-point coefficients).\n’LALON’ for a grid of kind \"latitudes * longitudes\".\nDefault is ’GAUSS’ in ARPEGE/IFS, ’LELAM’ in ALADIN.\nCFPDOM: names of the subdomains. Names have at maximum 7 characters.\nIf CFPFMT=’GAUSS’ or ’LELAM’ only one output domain is allowed.\nIf CFPFMT=’LALON’ the maximum of output subdomains allowed is 10.\nBy default, one output domain is requested, CFPDOM(1)=’000’ and CFPDOM(i)=’’ for i>1.\nL_READ_MODEL_DATE:  if: .TRUE. read date from the model","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"The default FA-names for parameters in different categories can be found from src/arp/setup/suafn1.F90 L687.","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"It's worth mentioning some of the variables postprocessed by FULL-POS","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"True vertical velocity w [VW].  (for NH ALADIN only).\nPotential vorticity P V [PV].\nPressure coordinate vertical velocity ω [VV].\nEta coordinate vertical velocity η [ETAD].\nAbsolute vorticity ζ + f [ABS].\nRelative vorticity ζ [VOR].\nDivergence D [DIV].\nSatellite equivalents\nMSAT7 MVIRI channels 1 and 2 ([MSAT7C1] and [MSAT7C2]).\nMSAT8 MVIRI channels 1 to 8 ([MSAT8C1] to [MSAT8C8]).\nMSAT9 MVIRI channels 1 to 8 ([MSAT9C1] to [MSAT9C8]).\nGOES11 IMAGER channels 1 to 4 ([GOES11C1] to [GOES11C4]).\nGOES12 IMAGER channels 1 to 4 ([GOES12C1] to [GOES12C4]).\nMTSAT1 IMAGER channels 1 to 4 ([MTSAT1C1] to [MTSAT1C4]).","category":"page"},{"location":"PostProcessing/Fullpos/#Problems","page":"FullPos","title":"Problems","text":"","category":"section"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"Problems may be encountered with FULL-POS when running on large domains. Here are some things to look out for:","category":"page"},{"location":"PostProcessing/Fullpos/","page":"FullPos","title":"FullPos","text":"Increase the MBX_SIZE if you run out of MPI buffer space. \nIncrease number of cores if you run out of memory.\nMake sure NFPROMA and NFPROMA_DEP are small and equal to NPROMA.\nSet NSTRIN=NSTROUT=NPROC in nampar0 if one of the above mentioned doesn't help.","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC_vars/#Variable-names-for-MUSC-output","page":"MUSC vars","title":"Variable names for MUSC output","text":"","category":"section"},{"location":"ForecastModel/SingleColumnModel/MUSC_vars/","page":"MUSC vars","title":"MUSC vars","text":"List of parameters copied from variable_list.csv ","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC_vars/","page":"MUSC vars","title":"MUSC vars","text":"short name variable name long name unit\nlwdw PFRTHDS long wave downward radiation at surface W/m2\nlwup PFRTH long wave upward radiation at surface W/m2\nswdw PFRSODS short wave downward radiation at surface W/m2\nswup PFRSO short wave upward radiation at surface W/m2\nshf PFCS sensible heat flux W/m2\nlhf PFCLN latent heat flux W/m2\nevap ZLH_flux evaporation+sublimation flux mm/day\nevap2 PFEVL evaporation+sublimation flux mm/day\nustar ZUSTAR friction velocity m/s\nrain PREC_TOT precipitation (liq+sol) rate mm/day\npsurf PAPRS surface Pressure Pa\nhpbl PCLPH boundary layer height m\nhpbl2 KCLPH boundary layer height m\ntsurf PTS surface temperature K\nt2m PTCLS 2 m temperature K\nq2m PQCLS 2 m specific humidity Kg/Kg\nrh2m PRHCLS 2 m relative humidity [0-100]\nu10m PUCLS 1 0m u-component m/s\nv10m PVCLS 10m v-component m/s\nt3m PT_03 temperature at 3.30 meter above the surface K\nq3m PQ_03 specific humidity at 3.30 meter Kg/Kg\nrh3m PRH_03 relative humidity at 3.30 meter [0-100]\nu3m PU_03 u-component at 3.30 meter m/s\nv3m PV_03 v-component  at 3.30 meter m/s\netc   \nt42m PT_42 temperature at 41.90 meter above the surface K\nq42m PQ_42 specific humidity at 41.90 meter Kg/Kg\nrh42m PRH_42 relative humidity at 41.90 meter [0-100]\nu42m PU_42 u-component at 41.90 meter m/s\nv42m PV_42 v-component  at 41.90 meter m/s\ncc PCLCT total cloud cover fraction 0 1\ntsurf PTS Surface temperature K\nalb PALBH Albedo [0-1]\nalb_surf TALB_ISBA surface albedo -\nz0m PGZ0 Momentum roughness length m\nz0h PGZ0H Heat roughness length m\nemis PEMIS surface emissivity [0-1]\nemis EMIS surface emissivity [0-1]\nzf PAPHIF Altitude of layer mid-points at t=0 (full-level) m\npf PAPRSF Pressure of layer mid-points at t=0 (full-level) Pa\nt PT temperature K\nth THETA potential temperature K\nq PQ specific humidity kg/kg\nu PU zonal wind component m/s\nv PV meridional wind component m/s\nugeo ZFUGEO u-component geostrophic wind m/s\nvgeo ZFVGEO v-component geostrophic wind m/s\ndudt_ls ZFU u-component advection m/s/s\ndvdt_ls ZFV v-component advection m/s/s\ndtdt_ls ZFT temperature advection K/s\ndqdt_ls ZFQ moisture advection Kg/Kg/s\nw ZW vertical movement m/s\nzhh PAPHI height of half level m\nphh PAPRS pressure of half level Pa\nkm ZKM Eddy diffusivity momentum m2/s\nkh ZKH Eddy diffusivity momentum m2/s\nmf ZMF_shal massflux Kg/m2/s\ndT_dt_rad ZDTRAD temperature tendency from radiation K/d\nTKE PECT turbulent kinetic energy m^2s^2\nshear ZPRDY shear production m^2s^3\nbuoy ZPRTH buoyancy production m^2s^3\ntrans ZDIFF total transport m^2s^3\ndissi ZDISS dissipation m^2s^3","category":"page"},{"location":"ForecastModel/SingleColumnModel/MUSC_vars/","page":"MUSC vars","title":"MUSC vars","text":"","category":"page"},{"location":"ExperimentConfiguration/Namelists/#Controlling-the-namelists-in-HARMONIE","page":"Namelist","title":"Controlling the namelists in HARMONIE","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/#Introduction","page":"Namelist","title":"Introduction","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"IFS is largely driven by namelists and has thousands of options. For each configuration a number of namelists controlling different parts are read. To make the maintenance of the namelists manageable and to assure consistency in terms on e.g. name conventions for fields, packing accuracy, physics settings and parallel options all namelists are generated as they are needed during the run. All the basic settings are defined in a perl dictionary nam/harmonie_namelists.pm for IFS and nam/surfex_namelists.pm and nam/surfex_selected_output.pm for SURFEX. surfex reference namelist settings. In future versions the commented namelists will be included and maintained as part of the code.   The IFS dictionary is structured in several sections:","category":"page"},{"location":"ExperimentConfiguration/Namelists/#harmone_namelists.pm","page":"Namelist","title":"harmone_namelists.pm","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"Global settings\nTechnical settings\nHost specific settings\nMPP options\nFile settings\nDYNAMICS SETTINGS\nMain dynamics switches\nNon-hydrostatic settings\nVertical finite element\nDFI\nMain physics options. NB! These may contain switches for dynamics as well\nALADIN\nAROME\nEDMFM switches, to be applied after AROME\nAlaro\nOld surface\nSURFEX\nDDH\nE927 Interpolation settings\nMain fullpos settings\nE927\nE927 nh\nSURFEX initial file generation\nAladin e927\nALARO e927\nArome e927\nGeneral postprocessing switches \nDefault fullpos settings\nNH postprocessing\nSwitches for postprocessing with surfex\nSpecial cases for arome\nAssimilation\nCanari\nArome canari\nVarbc_rad\nVarbc_coldstart\nScreening\nArome screening\nAlaro screening\nMinimization\nAlaro minimization\n4DVAR \n4DVAR minimization\n4DVAR screening\n4DVAR trajectory\nClimate generation\nClimate generations (e923)\nMisc\nGeneral namelist settings for Tangent-Linear and Adjoint tests \nExtra Adjoint test options \nOulan\nBator","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"The final namelists are build through the rules given in scr/Get_namelist and are generated by nam/gen_namelists.pl. Note that in several cases environment variables are still parsed in the scripts.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#surfex_namelists.pm","page":"Namelist","title":"surfex_namelists.pm","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"All possible SURFEX namelist setting are documented at the SURFEX web site. Use the search text area in the upper right corner to search for a specific namelist or namelist option. Please, keep in mind that the SURFEX web site documents the latest SURFEX version, i.e. SURFEXv8, while in cy40h SURFEXv7.3 is used. Therefore, some of the settings may be different or not available.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#PGD","page":"Namelist","title":"PGD","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"PGD represents in general the preparation of physiography data. The default PGD settings are listed here. Some modifications can be done for specific model configurations and will be specified as e.g. alaro_pgd or arome_pgd.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"# 2 layer ISBA scheme\n%isba_2L=(\n NAM_ISBA=>{\n  CISBA          => '\"2-L\",',\n  NGROUND_LAYER  => '2,',\n },\n);\n\n# 3 layer ISBA scheme\n%isba_3L=(\n NAM_ISBA=>{\n  CISBA          => '\"3-L\",',\n  NGROUND_LAYER  => '3,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"The section used here is decided by setting CISBA=\"3-L\" (default) in config_exp.h.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"%isba_pgd=(\n NAM_ISBA=>{\n   YCLAY         => 'YCLAY,',\n   YCLAYFILETYPE => '\"DIRECT\",' ,\n   YSAND         => 'YSAND,',\n   YSANDFILETYPE => '\"DIRECT\",'\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"The clay/sand used is decided by setting SOIL_TEXTURE_VERSION=FAO (default) in config_exp.h. FAO corresponds to 10 km resolution clay/sand according to SURFEX Soil texture description. The 1 km HWSD_v2 is not used by default since is shows strange vales over Scandinavia.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"%pgd=(\n NAM_IO_OFFLINE=>{\n  CSURF_FILETYPE => 'CSURF_FILETYPE,',\n   CPGDFILE      => 'CPGDFILE,',\n },\n NAM_PGD_GRID=>{\n   CGRID         => '\"CONF PROJ\",',\n },\n NAM_CONF_PROJ=>{\n   XLAT0         => $ENV{LAT0},\n   XLON0         => $ENV{LON0},\n   XRPK          => $ENV{SINLAT0},\n   XBETA         => 0.0,\n },\n NAM_CONF_PROJ_GRID=>{\n   XLATCEN       => $ENV{LATC},\n   XLONCEN       => $ENV{LONC},\n   NIMAX         => 'BNIMAX,',\n   NJMAX         => 'BNJMAX,',\n   XDX           => 'BXDX,', \n   XDY           => 'BXDY,',\n },\n NAM_COVER=> {\n   YCOVER        => 'YCOVER,',\n   YCOVERFILETYPE=> '\"DIRECT\",',\n },\n NAM_ZS=>{\n   YZS           => 'YTOPO,',\n   YZSFILETYPE   => '\"DIRECT\",',\n },\n NAM_SEABATHY=>{\n   XUNIF_SEABATHY=>'0.,', \n },\n NAM_PGD_SCHEMES=>{\n    CNATURE => '\"ISBA \",',\n    CSEA    => '\"SEAFLX\",',\n    CWATER  => '\"WATFLX\",',\n    CTOWN   => '\"TEB \",',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"Most options here are specified in different scripts and are normally not supposed to be modified. Exceptions are e.g.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"ECOCLIMAP version used (YCOVER) is decided by setting ECOCLIMAP_VERSION=2.2 (default) in config_exp.h. Available ECOCLIMAP versions provided via the SURFEX team are documented in SURFEX Land use description. Some of these can be available on your system, see your $HM_CLDATA setting.\nOrography version used (YTOPO) is decided by setting TOPO_SOURCE=gmted2010 (default) in config_exp.h.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#PREP","page":"Namelist","title":"PREP","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"PREP represents in general initialisation of prognostic variables. The default PREP settings are listed here. Some modifications can be done for specific model configurations and will be specified as e.g. alaro_prep or arome_prep.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"%sice_prep=(\n NAM_SEAFLUXn=>{\n    LHANDLE_SIC            =>'.TRUE.,',\n    LSIC_FROM_FILE         =>'.TRUE.,',\n    CSEA_ICE               =>'\"SICE\",',\n    NICE_LAYER             =>'4,',\n },\n NAM_PREP_SEAFLUX=>{\n    CFILE_SIC           =>'\"CFILE_SIC\",',\n    CTYPE_SIC           =>'\"GRIB  \"',\n },\n NAM_SIMPLE_ICE=>{\n    XICE_THICKNESS      =>'.75,',\n    LICE_HAS_SNOW       =>'.FALSE.,',\n    CICE_SNOW           =>'\"S-D\",',\n    NICE_SNOW_NLAYERS   =>'4,',\n    XICE_SNOW_HEIGHT    =>'.3,',\n    LSIC_DRIVEN_THICKNESS=>'.FALSE.',\n    XSIC_DRIVEN_MAX_THICKNESS=>'1.',\n },\n NAM_PREP_SIMPLE_ICE=>{\n    LINIT_FROM_SST => '.TRUE.,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"CSEA_ICE : selects preferred sea ice scheme.\nNONE – use default SURFEX ICEFLUX diagnostic scheme \nSICE – use SICE\nLHANDLE_SIC : activates ice fraction handling\nLSIC_FROM_FILE : ice fraction data from an external source\n.TRUE. – use the externally provided ice fraction data\n.FALSE. – assume that ice fraction data is encoded within SST field\nNICE_LAYER : number of ice layers (2 < NICE_LAYER < 100)\nCFILE_SIC : external data file that stores ice fraction data\nCTYPE_SIC : type of external data source\nXICE_THICKNESS : uniform thickness of the ice field\nLICE_HAS_SNOW : snow upon the ice\n.TRUE. – preform snow-enabled run\n.FALSE. – use bare ice configuration\nCICE_SNOW : snow scheme that used to parametrize snow upon the ice\nS-D – test heat diffusion scheme\nNICE_SNOW_NLAYERS : number of snow layers for S-D snow scheme\nXICE_SNOW_HEIGHT : thickness of snow pack for S-D snow scheme\n3-L – use 3-L explicit snow scheme \nLSIC_DRIVEN_THICKNESS : use ice fraction to estimate ice thickness. Untested, should be set to .FALSE.\nXSIC_DRIVEN_MAX_THICKNESS : unused if LSIC_DRIVEN_THICKNESS == .FALSE.\nLINIT_FROM_SST : use composite SST/SIST field to initialize sea ice temperatures","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"This part represents settings connected to the Simple Sea-ice Scheme (SICE) by Yurii Batrak. Please note that SICE is not yet an official contribution to SURFEX and therefore you will not find any documentation of SICE via the SURFEX web site. ","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"%isba_prep=(\n NAM_PREP_ISBA=>{\n   LISBA_CANOPY  => '.TRUE.,',\n   LEXTRAP_TG    => '.TRUE.,',\n   LEXTRAP_WG    => '.TRUE.,',\n   LEXTRAP_WGI   => '.TRUE.,',\n   LEXTRAP_SN    => '.TRUE.,',\n   NDIM_EXTRAP   => '20,',\n },\n NAM_PREP_ISBA_SNOW=>{\n   LSWEMAX       => '.TRUE.,',\n },\n NAM_DIAG_ISBAn=>{\n   LPATCH_BUDGET=>'.TRUE.,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"LISBA_CANOPY : activates surface boundary multi layer scheme over vegetation\nLEXTRAP_XX : extrapolate XX points where LSM < 0.5 (buffer only)\nNDIM_EXTRAP : Size of search domain for extrapolation (not in official SURFEX)\nLSWEMAX : logical switch to set an upper limit on initial snow water equivalent (set by XSWEMAX (=500 kg/m2 default) ).","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"%fullpos_prep=(\n NAM_FILE_NAMES=>{\n   HPGDFILE      =>'\"PGDFILE\",',\n   CINIFILE      =>'\"SURFXINI\",',\n },\n);\n\n%offline_prep=(\n NAM_IO_OFFLINE=>{\n   CSURF_FILETYPE       => '\"LFI   \",',\n   CPREPFILE            => '\"SURFXINI\",',\n   CPGDFILE             => '\"PGD\",',\n },\n NAM_PREP_SURF_ATM=>{\n   CFILEPGD      =>'\"PGD_host\",',\n   CFILEPGDTYPE  =>'\"LFI\",',\n   CFILE      =>'\"INFILE\",',\n   CFILETYPE  =>'CFILETYPE,',\n   NYEAR      =>'NYEAR,',\n   NMONTH     =>'NMONTH,',\n   NDAY       =>'NDAY,',\n   XTIME      =>'XTIME,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"Most options here are specified in different scripts and are normally not supposed to be modified.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"%arome_prep=(\n NAM_PREP_SEAFLUX=>{\n   LSEA_SBL     => '.FALSE.,',\n },\n NAM_SEAFLUXn=>{\n    CSEA_ICE               =>'\"NONE\",',\n    LHANDLE_SIC            =>'.FALSE.,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"Please note these special settings for the AROME model configuration.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#FORECAST","page":"Namelist","title":"FORECAST","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"FORECAST represents in general the namelist settings used during Forecast. The default FORECAST settings are listed here. Some modifications can be done for specific model configurations and will be specified as e.g. alaro_forecast or arome_forecast.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"%sice_forecast=(\n NAM_SEAFLUXn=>{\n    LHANDLE_SIC            =>'.TRUE.,',\n    LSIC_FROM_FILE         =>'.TRUE.,',\n    CSEA_ICE               =>'\"SICE\",',\n    NICE_LAYER             =>'4,',\n },\n NAM_SIMPLE_ICE=>{\n    XICE_THICKNESS            => '.75,',\n    LICE_HAS_SNOW             => '.FALSE.,',\n    CICE_SNOW                 => '\"S-D\",',\n    NICE_SNOW_NLAYERS         => '4,',\n    XICE_SNOW_HEIGHT          => '.3,',\n    LSIC_DRIVEN_THICKNESS     => '.FALSE.',\n    XSIC_DRIVEN_MAX_THICKNESS => '1.',\n },\n NAM_DIAG_SURFn=>{\n   LCOEF             => '.TRUE.,',\n   LSURF_VARS        => '.TRUE.,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"This part represents settings connected to the Simple Sea-ice Scheme (SICE) by Yurii Batrak. Please note that SICE is not yet an official contribution to SURFEX and therefore you will not find any documentation of SICE via the SURFEX web site.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"%isba_forecast=(\n NAM_ISBAn=>{\n   CROUGH        => '\"NONE\",',\n },\n NAM_DIAG_ISBAn=>{\n   LPGD     => '.TRUE.,',\n   LSURF_MISC_BUDGET=> '.TRUE.,',\n },\n);\n\n%forecast=(\n NAM_IO_OFFLINE=>{\n  'CSURF_FILETYPE'      => '\"LFI    \",',\n  'CFORCING_FILETYPE'   => '\"ASCII\",',\n  'CTIMESERIES_FILETYPE'=> '\"LFI\",',\n  'XTSTEP_SURF'         => $ENV{TSTEP}.\",\",\n  'XTSTEP_OUTPUT'       => '3600.,',\n  'LRESTART'            => '.TRUE.,',\n  'CPREPFILE'           => '\"PREP\",',\n  'CPGDFILE'            => '\"PGD\",',\n },\n NAM_SURF_ATM=>{\n   XRIMAX=>'0.0,',\n },\n NAM_DIAG_SURFn=>{\n   LSURF_BUDGET      => '.TRUE.,',\n   N2M      =>'2,',\n },\n NAM_DIAG_SURF_ATMn=>{\n   LT2MMW            => '.TRUE.,',\n },\n NAM_DIAG_ISBAn=>{\n   LPATCH_BUDGET  => '.TRUE.,',\n },\n NAM_SSOn=>{\n   CROUGH   => \"'\".$ENV{CROUGH}.\"'\",\n   XFRACZ0  => '15.,',\n },\n NAM_SEAFLUXn=>{\n  CSEA_FLUX => '\"ECUME\",',\n  LPWG      => '.FALSE.,',\n  LPRECIP   => '.FALSE.,',\n  LPWEBB    => '.FALSE.,',\n  CSEA_ICE    =>'\"NONE\",',\n  LHANDLE_SIC =>'.FALSE.,',\n  LPERTFLUX   => 'LPERTSURF,',\n },\n NAM_ISBAn=>{\n  LCANOPY_DRAG => '.TRUE.,',\n  XCDRAG       => '0.01,',\n  LPERTSURF    => 'LPERTSURF,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"XRIMAX: limitation of Richardson number in drag computation (=0 default).\nN2M : flag to compute surface boundary layer characteristics (=0 default). N2M=2 computes temperature at 2 m, specific humidity at 2 m, relative humidity, zonal and meridian wind at 10 m, and Richardson number. 2m and 10m quantities are calculated interpolating between atmospheric forcing variables and surface temperature and humidity. Please note that if the surface boundary multi layer scheme is activated over any tile (as with LISBA_CANOPY=T over land) it overrides the diagnostic N2M method. \nLT2MMW : Alternative weighting of grid average T2M giving more weight to the land tile (=FALSE default).\nCROUGH: type of orographic roughness length. CROUGH is decided by setting CROUGH=\"NONE\" in config_exp.h which means that no orographic treatment is applied.\nXFRACZ0 : Z0=Min(Z0, Href/XFRACZ0). Not applied here since CROUGH=\"NONE\".\nLPERTFLUX: multiplicative perturbation of Ecume fluxes for ensemble forecasting. In HARMONIE this is set in a number of scripts under scr.\nLPERTSURF: if .True. modification of surface fluxes for ensemble forecasting. In HARMONIE this is set in a number of scripts under scr.\nLCANOPY_DRAG: drag activated in SBL scheme within the canopy.\nXCDRAG: drag coefficient in canopy (=0.15 default in SURFEX).","category":"page"},{"location":"ExperimentConfiguration/Namelists/#ASSIMILATION","page":"Namelist","title":"ASSIMILATION","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"ASSIMILATION represents in general the surface assimilation namelist settings used for CANARI and SURFEX. The default ASSIMILATION settings are listed here. The SURFEX assimilation method used (OI or EKF) is decided by setting ANASURF=CANARI_OI_MAIN in config_exp.h. OI is default and EKF is still experimental.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"%sice_assim=(\n NAM_SEAFLUXn=>{\n    LHANDLE_SIC            =>'.TRUE.,',\n    LSIC_FROM_FILE         =>'.TRUE.,',\n    CSEA_ICE               =>'\"SICE\",',\n    NICE_LAYER             =>'4,',\n },\n NAM_SIMPLE_ICE=>{\n   LICE_HAS_SNOW       =>'.FALSE.,',\n   XICE_THICKNESS      =>'.75,',\n   CICE_SNOW           =>'\"S-D\",',\n   NICE_SNOW_NLAYERS   =>'4,',\n   XICE_SNOW_HEIGHT    =>'.3,',\n   LSIC_DRIVEN_THICKNESS      =>'.FALSE.',\n   XSIC_DRIVEN_MAX_THICKNESS  =>'1.',\n \n},\n NAM_PREP_SEAFLUX=>{\n    CFILE_SIC          => 'CFILE_SIC,',\n    CTYPE_SIC          => '\"GRIB  \"',\n    LSEA_SBL           => '.FALSE.',\n },\n NAM_PREP_SIMPLE_ICE=>{\n    LINIT_FROM_SST             =>'.TRUE.,',\n    LPREP_ONLY_NEW_ICE         =>'.TRUE.,',\n    LEXTRAPOLATE_FROM_FORECAST =>'.TRUE.,',\n    CFORECAST_GRIB             =>'CFORECAST_GRIB',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"LPREP_ONLY_NEW_ICE : perform prep in current grid cell only if ice state is undefined\nLEXTRAPOLATE_FROM_FORECAST : use previous forecast to fill areas with new ice. If LINIT_FROM_SST == .TRUE. .AND. LEXTRAPOLATE_FROM_FORECAST == .TRUE. option LINIT_FROM_SST is forced to .FALSE.\nCFORECAST_GRIB : previous forecast in GRIB format","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"This part represents settings connected to the Simple Sea-ice Scheme (SICE) by Yurii Batrak. Please note that SICE is not yet an official contribution to SURFEX and therefore you will not find any documentation of SICE via the SURFEX web site. ","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"%assim_surfex=(\n NAM_NACVEG=>{\n   NECHGU      => ''.$ENV{FCINT}.',',\n   RCLIMCA     => '0.,',\n   RCLISST     => '0.05,',\n   SIGH2MO     => '0.10,',\n   SIGT2MO     => '1.0,',\n   LOBS2M      => '.TRUE.,',\n   LOBSWG      => '.FALSE.,',\n },\n NAM_IO_OFFLINE=>{\n   CSURF_FILETYPE       => 'CSURF_FILETYPE,',\n   CTIMESERIES_FILETYPE => '\"LFI \",',\n   CFORCING_FILETYPE    => '\"ASCII\",',\n   LRESTART             => '.TRUE.,',\n   XTSTEP_SURF          => '3600.,',\n   XTSTEP_OUTPUT        => '3600.,',\n },\n NAM_ASSIM=>{\n   LASSIM              => '.TRUE.,',\n   LEXTRAP_WATER       => '.TRUE.,',\n   LEXTRAP_SEA         => '.FALSE.,',\n   LEXTRAP_NATURE      => '.FALSE.',\n   LREAD_SST_FROM_FILE => '.TRUE.,',\n   LWATERTG2           => '.TRUE.,',\n   LAESNM              => 'LAESNM,',\n   LECSST              => 'LECSST,',\n   LAROME              => 'LAROME,',\n   NPRINTLEV           => '1,',\n },\n);\n\n%oi_main=(\n NAM_ASSIM=>{\n   CASSIM_ISBA         => '\"OI\",',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#surfex_selected_output.pm","page":"Namelist","title":"surfex_selected_output.pm","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"The output from SURFEX to the .sfx fa-files (e.g. ICMSHHARM+0002.sfx) is in general decided by SURFEX NAM_DIAG namelist settings. These settings activate or deactivate groups of variables in output files. When one or more such groups are activated it is possible to limit the output to a specific list of variables by the use of the LSELECT/CSELECT options in SURFEX. This way of specifying output from SURFEX is the default way in cy40h. The setting is SURFEX_LSELECT=\"yes\" in config_exp.h. When SURFEX_LSELECT=\"yes\" the namelist nam/surfex_selected_output.pm is used to specify the output variables. The style of surfex_selected_output.pm is","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"%surfex_output=(\n NAM_WRITE_DIAG_SURFn=>{\n   NSTEP_DUMP_STATE => 'NSTEP_DUMP_STATE,',\n   LSELECT     => '.TRUE.,',\n   CSELECT     => '\n\"Z0\",\n\"RNC\",\n\"HC\",\n\n\n\"T2M_P\",\n\"T2MMIN_P\",\n\"T2MMAX_P\",'\n },);","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"Please note the \" ' \" character at the end of the last variable line \"T2MMAX_P\",'. Don't miss it, it is very important! The naming convention for variables in this file follows the SURFEX naming convention which is not exactly how they appear in fa-files. A fa-file output example from a .sfx file may look:","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"SFX.SST         > 001:011-  000-102@20160301_15:00+002h00m tri:000 000 SST\nSFX.TS_WATER    > 001:011-  770-105@20160301_15:00+002h00m tri:000 000 TS_WATER\nX001TG1         >                   20160301_15:00+002h00m         000\nX001TG2         >                   20160301_15:00+002h00m         000\nX001WG1         >                   20160301_15:00+002h00m         000\nX001WG2         >                   20160301_15:00+002h00m         000\nSFX.TROAD1      >                   20160301_15:00+002h00m         000\nSFX.WS_ROAD     > 001:024-  950-105@20160301_15:00+002h00m tri:000 000 WS_ROAD","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"You get the corresponding SURFEX names by removing \"SFX.\" or \"X001\" at the beginning of these fa-names.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"The .sfx files are used as first guess for next forecast. However, the first guess needs ALL SURFEX variables and not only a subset as defined by LSELECT/CSELECT. Therefore, a full .sfx file (e.g. ICMSHFULL+0003.sfx) is created for each assimilation cycle hour in addition to the corresponding limited file ICMSHHARM+0003.sfx. The output frequency of full files is defined by SURFEX_DUMP_STATE_STEPS=\"\" in config_exp.h.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"The content of the FULL-files can be used to identify additional variables to add to surfex_selected_output.pm.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#Change-your-namelists","page":"Namelist","title":"Change your namelists","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"There are several ways of changing namelists generated from the dictionary.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"Copy the harmonie_namelist.pm file to your local experiment directory and change the right section like for any source or script modification.\nIf you feel uncertain where to change in the dictionary you can copy the actual namelist used in your run. Every namelist used is listed in the logfile so copy it from there and put it under the nam directory in your local experiment. Make sure you give it a unique name. You must then also change the script(s) using this namelist like in the Forecast script scr/Forecast.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"#  Get namelist name\n#NAMELIST=$WRK/$WDIR/namelist_forecast\n#Get_namelist forecast $NAMELIST\nNAMELIST=$HM_LIB/nam/namelist_forecast_with_a_unique_name\n","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"For namelists not present in the dictionary you just copy them to you local nam directory.","category":"page"},{"location":"ExperimentConfiguration/Namelists/","page":"Namelist","title":"Namelist","text":"There is also a description on how to generate new namelist dictionaries here.","category":"page"},{"location":"ForecastModel/NearRealTimeAerosols/#Near-Real-Time-Aerosols","page":"Near Real Time Aerosols","title":"Near Real Time Aerosols","text":"","category":"section"},{"location":"ForecastModel/NearRealTimeAerosols/","page":"Near Real Time Aerosols","title":"Near Real Time Aerosols","text":"The model can be configure to use near real time aerosols from CAMS. Just by setting BDAERO=cams in ecf/config_exp.h","category":"page"},{"location":"ForecastModel/NearRealTimeAerosols/","page":"Near Real Time Aerosols","title":"Near Real Time Aerosols","text":"Namelist NAMNRTAER contains switches.\nBoundary files containing aerosol mass mixing ratio fields from CAMS are retrieved.\nAerosol fields defined in YAERO_NL in namelist NAMGFL.","category":"page"},{"location":"ForecastModel/NearRealTimeAerosols/#NAMNDRTAER-namelist","page":"Near Real Time Aerosols","title":"NAMNDRTAER namelist","text":"","category":"section"},{"location":"ForecastModel/NearRealTimeAerosols/","page":"Near Real Time Aerosols","title":"Near Real Time Aerosols","text":"The switches and some parameters can be set in NAMNRTAER (in nam/harmonie_namelists.pm)","category":"page"},{"location":"ForecastModel/NearRealTimeAerosols/","page":"Near Real Time Aerosols","title":"Near Real Time Aerosols","text":"LCAMS_NRT: switch on the use of CAMS aerosols in HARMONIE-AROME, the Mass mixing ratio fields must be present in the first guess and the boundary conditions. The number and name of those fields are specified in the namelist NAMGFL.\nLAERDROP: Permits to use Cloud Droplet Number Concentration (CDNC) from n.r.t. aerosols to calculate the effective radius in the radiation scheme.\nSSMIN: Supersaturation at sfc level. (default 0.05%). The supersaturation activates the condensation nuclei (CN) to obtain CCN.\nSSMAX: Supersaturation over 100 m height (default 0.08%).\nCCNMIN: Minimum number concentration of Cloud Condensation Nuclei (CCN) inside the cloud: It is considered 10E6 (10 cm-3). Other values can be considered, but probably not over 50cm-3.\nCLDROPMIN: Minimum CDNC inside the cloud. It is practically the same as CCNMIN. Other values can be considered, but probably not over 50cm-3.\nLMOCA_NRT: In case of getting the aerosol fields from MOCAGE (still not in use).\nLAEIFN: To activate Ice nuclei (mainly dust and hydrophobic organic matter and Black carbon).\nLAERDRDEP: Activates the aerosol deposition. (FALSE by default).\nLAECCN2CLDR: By default LAECCN2CLDR=FALSE, that is CDNC=CCN.","category":"page"},{"location":"System/UpdateNamelists/#Update-the-namelist-hashes","page":"Update Namelist","title":"Update the namelist hashes","text":"","category":"section"},{"location":"System/UpdateNamelists/#Introduction","page":"Update Namelist","title":"Introduction","text":"","category":"section"},{"location":"System/UpdateNamelists/","page":"Update Namelist","title":"Update Namelist","text":"Each namelists is build from a perl dictionary of different settings, nam/harmonie_namelists.pm as the deviation from the default setup. One section takes care of the general file settings, one of the mpp options and the large ones of different configurations. The script nam/gen_namelists.pl allows us to build new namelists adding the settings on top of each other. In the following we describe how to add new namelists and include them in the suite.  ","category":"page"},{"location":"System/UpdateNamelists/#Create-a-new-hash-module","page":"Update Namelist","title":"Create a new hash module","text":"","category":"section"},{"location":"System/UpdateNamelists/","page":"Update Namelist","title":"Update Namelist","text":"Let us assume we have some new 4DVAR namelists we would like to merge.  Create a directory, 4dvar, and put your new namelists in here. Run the script nam/Create_hashes.pl","category":"page"},{"location":"System/UpdateNamelists/","page":"Update Namelist","title":"Update Namelist","text":"./Create_hashes.pl 4dvar\nCreate namelist hash for 4dvar \nScan 4dvar/namscreen_dat_4d \nScan 4dvar/namtraj_1_4d \nScan 4dvar/namvar_dat_4d \nCreate namelist hash 4dvar.pm \nCreate updated empty namelist hash empty_4dvar.pm for 4dvar","category":"page"},{"location":"System/UpdateNamelists/","page":"Update Namelist","title":"Update Namelist","text":"We have now created a perl module for the new namelists. One with empty namelist entries, 4dvar_empty.pm, and one with all namelists in the right format, 4dvar.pm. To get one of your namelists back ( sorted ) you can write:","category":"page"},{"location":"System/UpdateNamelists/","page":"Update Namelist","title":"Update Namelist","text":"./gen_namlist.pl -n 4dvar_empty.pm -n 4dvar.pm namscreen_dat_4d","category":"page"},{"location":"System/UpdateNamelists/","page":"Update Namelist","title":"Update Namelist","text":"To get the module integrated in the system the module has to be merged with the conventions in harmonie_namelists.pm, but as a start the full namelists can be used. Copy the new empty*.pm to empty.pm to get the updated list of empty namelists.","category":"page"},{"location":"System/UpdateNamelists/#Create-the-new-namelist","page":"Update Namelist","title":"Create the new namelist","text":"","category":"section"},{"location":"System/UpdateNamelists/","page":"Update Namelist","title":"Update Namelist","text":"Add the new namelists to the script scr/Get_namelist. In this case we would add a new case for 4dvar","category":"page"},{"location":"System/UpdateNamelists/","page":"Update Namelist","title":"Update Namelist","text":"4dvartraj) \n   NAMELIST_CONFIG=\"$DEFAULT minimization dynamics ${DYNAMICS} ${PHYSICS} ${PHYSICS}_minimization ${SURFACE} ${EXTRA_FORECAST_OPTIONS} varbc minim4d\"\n    ;;","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/#Use-of-Observation","page":"Observations","title":"Use of Observation","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/#Background-Information","page":"Observations","title":"Background Information","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"ODBusage2.pdf Anne Fouilloux's (ECMWF) review presentation about ODB\nhttp://apps.ecmwf.int/odbgov ECMWF's ODB governance pages - useful for looking up ODB and BUFR definintions\nhttp://www.ecmwf.int/research/ifsdocs/CY28r1/pdf_files/odb.pdf The ODB bible - Sami Saarinen's ODB user guide (2004)\nhttp://www.rclace.eu/File/DataAssimilation/2007/laceobspp.pdf Sandor's document about observation dataflow in ALADIN ","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/#Observation-type","page":"Observations","title":"Observation type","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"The observation types used by Harmonie (upper-air) data assimilation are defined in scr/include.ass.","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/#SYNOP","page":"Observations","title":"SYNOP","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"By default all SYNOP observation types (including SHIP) are used. ","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"export SYNOP_OBS=1             # All synop","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"To blacklist SYNOP observations add blacklisted \"ODB observation type/ASCII type/ODB code type/ODB variable number/station identifier/date to blacklist from\" to nam/LISTE_NOIRE_DIAP. For example to blacklist 10m winds from Valentia Automatic SYNOP (03953) from the 10th of November 2012 enter the following line to LISTE_NOIRE_DIAP:","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":" 1 SYNOP       14  41 03953    10112012","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"(Note: please don't add Valentia to your blacklist - the observations from there are pretty good!)","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"For further information on ODB observation types, code types, variable numbers etc see the ECMWF ODB governance page here: http://apps.ecmwf.int/odbgov/obstype/","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/#SHIP","page":"Observations","title":"SHIP","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"See information provided above on SYNOP observations.","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/#BUOY","page":"Observations","title":"BUOY","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"By default all BUOY observation types are used. ","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"export BUOY_OBS=1              # Buoy","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"To blacklist BUOY observations add blacklisted \"ODB observation type/ASCII type/ODB code type/ODB variable number/station identifier/date to blacklist from\" to nam/LISTE_NOIRE_DIAP. For example to blacklist surface temperatures from BUOY M5 (62094) from the 10th of November 2012 enter the following line to LISTE_NOIRE_DIAP:","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":" 4 BUOY        165  11 62094    10112012","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"(Note: please don't add M4 to your blacklist - the observations from there are pretty good too!)","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"For further information on ODB observation types, code types, variable numbers etc see the ECMWF ODB governance page here: http://apps.ecmwf.int/odbgov/obstype/","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/#AIRCRAFT","page":"Observations","title":"AIRCRAFT","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"By default all AIRCRAFT observation types (including AMDAR, AIREP, ACARS) are used. ","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"export AIRCRAFT_OBS=1          # AMDAR, AIREP, ACARS","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"Below are lines added by Xiaohua to the  DMI dka37 LISTE_NOIRE_DIAP file to exclude problematic aircraft observations:","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"2 AMDAR 144 2 EU0028 08292013\n2 AMDAR 144 2 EU0092 01042013\n2 AMDAR 144 2 EU0079 01052013\n2 AMDAR 144 2 EU0097 01052013\n2 AMDAR 144 2 EU0107 01052013\n2 AMDAR 144 2 EU0033 01062013\n2 AMDAR 144 2 EU0118 01062013\n2 AMDAR 144 2 EU0112 07052013\n2 AMDAR 144 2 EU1110 08122013\n2 AMDAR 144 2 EU0074 08122013","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/#TEMP","page":"Observations","title":"TEMP","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"By default all TEMP observation types are used. ","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"export TEMP_OBS=1              # TEMP, TEMPSHIP","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/#PILOT","page":"Observations","title":"PILOT","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"By default all PILOT observation types are used. ","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"export PILOT_OBS=1             # Pilot, Europrofiler","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/#AMSUA","page":"Observations","title":"AMSUA","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"By default all AMSUA observation types are not used. ","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"export AMSUA_OBS=0             # AMSU-A","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"To use locally received AMSUA data provided by EUMETCast set ATOVS_SOURCE to local in scr/include.ass:","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"export ATOVS_SOURCE=mars       # local: EUMETCast; \n                               # mars: data from MARS\n                               # hirlam: hirlam radiance template ","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/#AMV-(aka-SATOB,-GEOWIND)","page":"Observations","title":"AMV (aka SATOB, GEOWIND)","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"For AMVs there is a HOW-TO page.","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/#Other-observation-types-...","page":"Observations","title":"Other observation types ...","text":"","category":"section"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"More documentation to follow ...","category":"page"},{"location":"ExperimentConfiguration/UseofObservation/","page":"Observations","title":"Observations","text":"export AMSUB_OBS=0             # AMSU-B, MHS\nexport IASI_OBS=0              # IASI  \nexport PAOB_OBS=0              # PAOB not defined everywhere\nexport SCATT_OBS=0             # Scatterometer data not defined everywhere\nexport LIMB_OBS=0              # LIMB observations, GPS Radio Occultations\nexport RADAR_OBS=0             # Radar ","category":"page"},{"location":"Observations/Atovs/#ATOVS-radiances-(pre-)-processing","page":"ATOVS","title":"ATOVS radiances (pre-) processing","text":"","category":"section"},{"location":"Observations/Atovs/#Introduction","page":"ATOVS","title":"Introduction","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"The IFS/ARPEGE/AROME data assimilation code uses level 1-c radiances. ATOVS radiances are available through local HRPT (High Rate Picture Transmission) antenna and the EUMETSAT EARS (European Advanced Retransmission Service) EUMETCast broadcasting system. Data received through local antenna need to be pre-processed with the ATOVS and AVHRR Pre-processing Package (AAPP). Radiances are also available trough the GTS, but with longer timelines. ","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"This short description explains how to prepare ATOVS radiances for (operational) data assimilation. Like all radiances, ATOVS data bias is corrected using Variational technique. VarBC coefficients should be updated for each limited area model. The variational bias correction is activated through namelist switches (see below).","category":"page"},{"location":"Observations/Atovs/#ATOVS-radiances","page":"ATOVS","title":"ATOVS radiances","text":"","category":"section"},{"location":"Observations/Atovs/#scr/include.ass","page":"ATOVS","title":"scr/include.ass","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"src/include.ass should be edited to \"switch on\" the use of AMSUA (AMSU-A), AMSUB (AMSU-B/MHS):","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"export AMSUA_OBS=1             # AMSU-A\nexport AMSUB_OBS=1             # AMSU-B, MHS\nexport ATOVS_SOURCE=mars       # local: EUMETCast;\n                               # mars: data from MARS\n                               # hirlam: hirlam radiance template","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"The default handling of ATOVS (AMSU-A and AMSU-B/MHS) is so that we store/load them in separate ODB bases. The definition of the bases in Harmonie is done in scr/include.ass.","category":"page"},{"location":"Observations/Atovs/#Loading-the-ATOVS-radiances","page":"ATOVS","title":"Loading the ATOVS radiances","text":"","category":"section"},{"location":"Observations/Atovs/#Data-extracted-from-MARS","page":"ATOVS","title":"Data extracted from MARS","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Data extracted from MARS (the default setting in the Harmonie) is loaded the following way:\nAfter extraction, all observations, including the radiances, are shuffled per observation type;\nIn Bator ATOVS data are loaded from the oulan working directory the following way:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"        # AMSU-A observations\n      elif [ \"$base\" = amsua] ; then\n        # AMSU-A\n        if [ \"$AMSUA_OBS\" -eq 1]; then\n          echo \"amsua    BUFR     amsua            ${YMD} ${HH}\">>refdata\n          ln -sf \"$WRK\"/oulan/amsua ./BUFR.amsua\n          cp \"${HM_LIB}\"/nam/param_bator.cfg.amsua.\"${ATOVS_SOURCE}\" ./param.cfg\n        fi\n      elif [ \"$base\" = amsub] ; then\n        # AMSU-B\n        if [ \"$AMSUB_OBS\" -eq 1]; then\n          echo \"amsub    BUFR     amsub            ${YMD} ${HH}\">>refdata\n          ln -sf \"$WRK\"/oulan/amsub ./BUFR.amsub\n          cp \"${HM_LIB}\"/nam/param_bator.cfg.amsub.\"${ATOVS_SOURCE}\" ./param.cfg\n        fi","category":"page"},{"location":"Observations/Atovs/#Locally-received-data","page":"ATOVS","title":"Locally received data","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Locally received (usually through the EARS system) data are loaded the from a source or the \"observations\" directory. Note, it is recommanded to have both AMSU-B and MHS data in one file. Otherwise, it's recommanded to create a single file with them. One can use the command \"cat\" to do this.\nThe example below shows a case when AMSU-B and MHS are stored in different BUFR files:\n       # AMSU-A observations\n    elif [ \"$base\" = amsua] ; then\n      # AMSU-A\n      if [ \"$AMSUA_OBS\" -eq 1]; then\n        echo \"amsua    BUFR     amsua            ${YMD} ${HH}\">>refdata\n        #ln -sf \"$WRK\"/oulan/amsua ./BUFR.amsua\n        ln -sf /scratch/ms/no/sbt/hm_home/RR_CONV/observations/amsua${YMD}${HH} ./BUFR.amsua\n        cp \"${HM_LIB}\"/nam/param_bator.cfg.amsua.\"${ATOVS_SOURCE}\" ./param.cfg\n      fi\n    elif [ \"$base\" = amsub] ; then\n      # AMSU-B\n      if [ \"$AMSUB_OBS\" -eq 1]; then\n        cat /scratch/ms/no/sbt/hm_home/RR_CONV/observations/amsub${YMD}${HH}  /scratch/ms/no/sbt/hm_home/RR_CONV/observations/mhs${YMD}${HH} > \"$WRK\"/oulan/amsub\n        echo \"amsub    BUFR     amsub            ${YMD} ${HH}\">>refdata\n        ln -sf \"$WRK\"/oulan/amsub ./BUFR.amsub\n        cp \"${HM_LIB}\"/nam/param_bator.cfg.amsub.\"${ATOVS_SOURCE}\" ./param.cfg\n      fi","category":"page"},{"location":"Observations/Atovs/#param.cfg","page":"ATOVS","title":"param.cfg","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"The BUFR template used by ATOVS (AMSU-A, AMSU-B/MHS) data should be defined in the param_bator.cfg.${atovs}.${ATOVS_SOURCE} file used by Bator. Where $atovs (amsua or amsua) and ATOVS_SOURCE as defined above according to the source of the data source. param.cfg files for Bator are in the nam namelist directory. The ATOVS param.cfg template should be something like this: For locally processed AMSU-A:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"BEGIN amsua\n1 1 0 14\ncodage     1  310009\ncontrol    1      15  nb de canaux\nvalues     6  001034  Originating sub-centre\nvalues     7  001007  Satellite identifier\nvalues    11  005041  Scan line number\nvalues    12  005043  Field of view number\nvalues    22  005001  Latitude\nvalues    23  006001  Longitude\nvalues    16  004001  Year\nvalues    25  007024  Satellite zenith angle\nvalues    24  007001  Height of station\nvalues    26  005021  Bearing or azimuth\nvalues    27  007025  Solar zenith angle\nvalues    28  005022  Solar azimuth\nvalues    43  012063  Brightness Temperature\nvalues    38  002150  Tovs Channel number\nEND amsua","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"For AMSU-A from MARS:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"BEGIN amsua\n1 1 0 14\ncodage     1  310008\ncontrol    1      15  nb de canaux\nvalues     6  001034  Originating sub-centre\nvalues     7  001007  Satellite identifier\nvalues    11  005041  Scan line number\nvalues    12  005043  Field of view number\nvalues    22  005001  Latitude\nvalues    23  006001  Longitude\nvalues    16  004001  Year\nvalues    25  007024  Satellite zenith angle\nvalues    24  007001  Height of station\nvalues    26  005021  Bearing or azimuth\nvalues    27  007025  Solar zenith angle\nvalues    28  005022  Solar azimuth\nvalues    43  012063  Brightness Temperature\nvalues    38  002150  Tovs Channel number\nEND amsua","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"For locally processed AMSU-B:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"BEGIN amsub\n1 1 0 15\ncodage     1  310010\ncontrol    1       5  nb de canaux\nvalues     6  001034  Originating sub-centre\nvalues     7  001007  Satellite identifier\nvalues    11  005041  Scan line number\nvalues    12  005043  Field of view number\nvalues    22  005001  Latitude\nvalues    23  006001  Longitude\nvalues    16  004001  Year\nvalues     8  002048  Satellite sensor type\nvalues    24  007001  Height of station\nvalues    25  007024  Satellite zenith angle\nvalues    26  005021  Bearing or azimuth\nvalues    27  007025  Solar zenith angle\nvalues    28  005022  Solar azimuth\nvalues    38  002150  Tovs Channel number\nvalues    43  012063  Brightness Temperature\nEND amsub","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"For AMSU-B from MARS:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"BEGIN amsub\n1 1 0 15\ncodage     1  310008\ncontrol    1       5  nb de canaux\nvalues     6  001034  Originating sub-centre\nvalues     7  001007  Satellite identifier\nvalues    11  005041  Scan line number\nvalues    12  005043  Field of view number\nvalues    22  005001  Latitude\nvalues    23  006001  Longitude\nvalues    16  004001  Year\nvalues     8  002048  Satellite sensor type\nvalues    24  007001  Height of station\nvalues    25  007024  Satellite zenith angle\nvalues    26  005021  Bearing or azimuth\nvalues    27  007025  Solar zenith angle\nvalues    28  005022  Solar azimuth\nvalues    38  002150  Tovs Channel number\nvalues    43  012063  Brightness Temperature\nEND amsub","category":"page"},{"location":"Observations/Atovs/#BATOR-namelist-and-data-extraction","page":"ATOVS","title":"BATOR namelist and data extraction","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Depending on the satellite and channel you may have to add entries to the NADIRS namelist in the Bator script like the following:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":" &NADIRS\n   LMFBUFR=.FALSE.,\n   TS_AMSUA(206)%t_select%ChannelsList(:) = -1,\n   TS_AMSUA(206)%t_select%TabFov(:) = -1,\n   TS_AMSUA(206)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUA(207)%t_select%TabFov(:) =  -1,\n   TS_AMSUA(207)%t_select%ChannelsList(:) = -1,\n   TS_AMSUA(207)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUA(209)%t_select%ChannelsList(:) = -1,\n   TS_AMSUA(209)%t_select%TabFov(:) = -1,\n   TS_AMSUA(209)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUA(223)%t_select%ChannelsList(:) = -1,\n   TS_AMSUA(223)%t_select%TabFov(:) = -1,\n   TS_AMSUA(223)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUA(4)%t_select%ChannelsList(:) = -1,\n   TS_AMSUA(4)%t_select%TabFov(:) = -1,\n   TS_AMSUA(4)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUA(3)%t_select%ChannelsList(:) = -1,\n   TS_AMSUA(3)%t_select%TabFov(:) = -1,\n   TS_AMSUA(3)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUA(3)%t_select%SclJump = 0,\n   TS_AMSUA(784)%t_select%TabFov(:) = -1,\n   TS_AMSUA(784)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUB(206)%t_select%TabFov(:) = -1,\n   TS_AMSUB(206)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUB(206)%t_select%SclJump = 0,\n   TS_AMSUB(206)%t_satsens%ModSensor = -1,\n   TS_AMSUB(207)%t_select%TabFov(:) = -1,\n   TS_AMSUB(207)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUB(207)%t_select%SclJump = 0,\n   TS_AMSUB(207)%t_select%ChannelsList(:) = -1,\n   TS_AMSUB(208)%t_select%TabFov(:) = -1,\n   TS_AMSUB(208)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUB(208)%t_select%SclJump = 0,\n   TS_AMSUB(208)%t_select%ChannelsList(:) = -1,\n   TS_AMSUB(209)%t_select%TabFov(:) = -1,\n   TS_AMSUB(209)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUB(209)%t_select%SclJump = 0,\n   TS_AMSUB(209)%t_satsens%ModSensor = 4,\n   TS_AMSUB(209)%t_select%ChannelsList(:) = -1,\n   TS_AMSUB(4)%t_select%TabFov(:) = -1,\n   TS_AMSUB(4)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUB(4)%t_select%SclJump = 0,\n   TS_AMSUB(4)%t_select%ChannelsList(:) = -1,\n   TS_AMSUB(3)%t_select%TabFov(:) = -1,\n   TS_AMSUB(3)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUB(3)%t_select%SclJump = 0,\n   TS_AMSUB(3)%t_satsens%ModSensor = 15,\n   TS_AMSUB(223)%t_select%TabFov(:) = -1,\n   TS_AMSUB(223)%t_select%TabFovInterlace(:) = -1,\n   TS_AMSUB(223)%t_select%SclJump = 0,\n   TS_AMSUB(223)%t_select%ChannelsList(:) = -1,","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"All the \"-1\" mean that we extract all available data we found. No restriction on the channels nor fields of view (FOV), nor on scanning lines (through TS_AMSUB(???)%t_select%SclJump = 0,). The default choices are defined in the routine called src/odb/pandor/module/bator_init_mod.F90. Note that although the NOAA-18 was embarked with MHS instrument, somehow our system treat the NOAA-18 MHS as AMSU-B (implementation decision) TS_AMSUB(209)%t_satsens%ModSensor = 4, is changing the instrument characteristics in our system, while for Metop-A MHS is defined through TS_AMSUB(3)%t_satsens%ModSensor = 15,.","category":"page"},{"location":"Observations/Atovs/#Variational-bias-correction-and-screening","page":"ATOVS","title":"Variational bias correction and screening","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"The variational technique is used to correct the bias for radiance data in Harmonie. Default choice of predictors used to correct bias for different channels of different instruments is defined in src/arpifs/module/varbc_rad.F90 as the example taken for AMSU-B below:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"DO ic = 2, 5\n  SELECT case (ic)\n  case (2)\n    yconfig(msensor_AMSUB, ic)%nparam = 4\n    yconfig(msensor_AMSUB, ic)%npredcs(1:4) = (/0,8,9,10/)\n  case (3:5)\n    yconfig(msensor_AMSUB, ic)%nparam = 8\n    yconfig(msensor_AMSUB, ic)%npredcs(1:8) = (/0,1,2,5,6,8,9,10/)\n  END SELECT\nENDDO","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"The choice for predictors for VarBC in Harmonie is given through namelist of the screening and minimisation in nam/harmonie_namelist.pm as follows. The update concerns the choice of predictors for each channel of the used instruments and also the nbg (nbg_AMSUA, nbg_AMSUB, nbg_MHS, ...) definition for each instruments. Note that the NBG defines the \"speed of the  adaptivity\" of the varBC. High value have slowing, while low value have speeding effect:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":" NAMVARBC=>{\n },\n NAMVARBC_RAD=>{\n 'LBC_RAD' => '.TRUE.,',\n 'yconfig(3,5)%nparam' => '10',\n 'yconfig(3,5)%npredcs(1:10)' => '0,1,2,8,9,10,15,16,17,18',\n 'yconfig(3,6)%nparam' => '5',\n 'yconfig(3,6)%npredcs(1:5)' => '0,1,8,9,10',\n 'yconfig(3,7)%nparam' => '5',\n 'yconfig(3,7)%npredcs(1:5)' => '0,1,8,9,10',\n 'yconfig(3,8)%nparam' => '5',\n 'yconfig(3,8)%npredcs(1:5)' => '0,1,8,9,10',\n 'yconfig(3,9)%nparam' => '5',\n 'yconfig(3,9)%npredcs(1:5)' => '0,1,8,9,10',\n 'yconfig(3,10)%nparam' => '5',\n 'yconfig(3,10)%npredcs(1:5)' => '0,1,8,9,10',\n 'yconfig(4,2)%nparam' => '4',\n 'yconfig(4,2)%npredcs(1:4)' => '0,8,9,10',\n 'yconfig(4,3)%nparam' => '5',\n 'yconfig(4,3)%npredcs(1:5)' => '0,1,8,9,10',\n 'yconfig(4,4)%nparam' => '5',\n 'yconfig(4,4)%npredcs(1:5)' => '0,1,8,9,10',\n 'yconfig(4,5)%nparam' => '5',\n 'yconfig(4,5)%npredcs(1:5)' => '0,1,8,9,10',\n 'yconfig(15,2)%nparam' => '4',\n 'yconfig(15,2)%npredcs(1:4)' => '0,8,9,10',\n 'yconfig(15,3)%nparam' => '5',\n 'yconfig(15,3)%npredcs(1:5)' => '0,1,8,9,10',\n 'yconfig(15,4)%nparam' => '5',\n 'yconfig(15,4)%npredcs(1:5)' => '0,1,8,9,10',\n 'yconfig(15,5)%nparam' => '5',\n 'yconfig(15,5)%npredcs(1:5)' => '0,1,8,9,10',\n 'nbg_AMSUA' => '2000',\n 'nbg_AMSUB' => '2000',\n 'nbg_MHS' => '2000'","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Satellite identifiers are available here: [https://software.ecmwf.int/wiki/display/ECC/WMO%3D27+code-flag+table]","category":"page"},{"location":"Observations/Atovs/#Source-code","page":"ATOVS","title":"Source code","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"The reading of BUFR ATOVS (AMSU-A and AMSU-B/MHS) is taken care of by the subroutines in src/odb/pandor/module/bator_decodbufr_mod.F90. These subroutines read the above parameters defined in the param.cfg file (see above):","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Default values for VarBC are defined in src/arpifs/module/varbc_rad.F90.","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"The defined varBC predictors are computed in src/arpifs/module/varbc_pred.F90.","category":"page"},{"location":"Observations/Atovs/#Blacklisting","page":"ATOVS","title":"Blacklisting","text":"","category":"section"},{"location":"Observations/Atovs/#Preparation-for-radiance-monitoring","page":"ATOVS","title":"Preparation for radiance monitoring","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Radiances sensed by polar orbiting satellites like NOAA and Meteop series are accessible differently at different assimilation time for different LAM model. This means that we use these data differently in different Harmonie LAM models. To make the assimilation optimal for our domain, we need to monitor the accessibility of data from different instruments inside our model domain. There are two ways of monitoring the radiance data assimilation:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"we do passive assimilation of the radiances using the assimilation and forecast systems;\nwhen having forecasts with the system, which we would like to use when assimilating the radiance data, then we can do independent assimilations of radiances with the first guess from the existing mentioned results.","category":"page"},{"location":"Observations/Atovs/#Monitoring-with-passive-data-assimilation","page":"ATOVS","title":"Monitoring with passive data assimilation","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"There two ways of monitoring the availability and assimilation of polar orbiting-based radiances. In any case we assimilate the radiances in passive way by changing some settings in the src/blacklist/mf_blacklist.b as follows at the end of blacklisting procedure for each instrument. Please read carefully this paragraph until the end before starting your passive experiment. Below is the example for AMSU-A radiances:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"\n ...\n! begin passive\n        fail(EXPERIMENTAL);\n! end passive\n\n    endif;   ! SENSOR = AMSUA","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"This way we guarantee that the blacklisting rules for active radiances are applied, which is very important when we change to active assimilation.","category":"page"},{"location":"Observations/Atovs/#Monitoring-using-the-Obsmon-tool","page":"ATOVS","title":"Monitoring using the Obsmon tool","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"To have the all statistics we need when using the Obsmon tool, in ecf/config_exp.h set the following:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"# *** Observation monitoring ***\nOBSMONITOR=obstat               # Create Observation statistics plots","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Note that running the Harmonie system with this option may slow down you monitoring process. Instead, you can run the Obmon separately. More about the Obsmon monitoring tool can be found here.","category":"page"},{"location":"Observations/Atovs/#Cold-start","page":"ATOVS","title":"Cold start","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"This way of monitoring is defined the following way in Harmonie through the scr/include.ass:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"# Start with empty VARBC coefficients\nexport VARBC_COLD_START=yes # yes|no","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"In this way the estimation starts with zero coefficients and we need minimum 20 days to have the bias for different channels well converging to their nominal values. This is the common practice in Hirlam community, but some studies in LACE showed that warmstart can be advantageous.","category":"page"},{"location":"Observations/Atovs/#Warm-start","page":"ATOVS","title":"Warm start","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"This way we start with precomputed varBC coefficients from other LAM or from a global model. See the section on \"activating existing varBC coefficients\" below. We set our choice in scr/include.ass the following way:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"# Start with empty VARBC coefficients\nexport VARBC_COLD_START=no # yes|no","category":"page"},{"location":"Observations/Atovs/#Monitoring-with-independent-analyses-and-without-Obsmon","page":"ATOVS","title":"Monitoring with independent analyses and without Obsmon","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"This solution needs additional playfile, which is not yet implemented in the Harmonie system. The playfile avoid the extraction of the LBCs, execution of the forecast and the post-processing parts of the Harmonie system. On top of the screening and minimization, it takes into account of the archiving part of the odb data.","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Once you have the ODB database after a month of experiment with analyses only, you can extract the the first-gess departures (fg_depar) and the analysis increments (an_depar) using the scripts in this tarball. When the extraction is finished, you can compute the statistics using the this tool. Use the script called ExtractOdb_?? to extract the departure information. To compute the statistics, use the script called checkbias_??. To plot the final results you can use the R-based scripts in this tarbal file.","category":"page"},{"location":"Observations/Atovs/#Analysing-the-results","page":"ATOVS","title":"Analysing the results","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"**Using obsmon **","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"To check the efficiency of the assimilation of each channel extracted by Bator use the obcmon abd make your choice similar to the example below. Make sure that you have chosen your experiment and set up the period for the whole period of your monitoring.","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"note: Note\nmissing image Obsmon_departures_check.png","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Then plotting the results, you should have something similar to the example below:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"note: Note\nmissing image Obsmon_departues_check2.png","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Take note about the channels that behave badly (having large bias an_depar over Land or Sea) at different assimilation time. Note that for AMSU-B and MHS, the observation error is relatively larger than that of the AMSU-A. So, you can allow larger bias than for AMSU-A. See the examples bellow.","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"**Not using obsmon **","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"The good point of choosing this tool is that the results are more compacted in only few eps-formatted files, which makes the analysis easier than with Obsmon. The analysis procedure is the same.","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"When the assimilation of the ATOVS radiances at all assimilation times was checked, then use the blacklisting rules described below to make your final choice for your LAM model with ATOVS.","category":"page"},{"location":"Observations/Atovs/#Blacklisting-rules","page":"ATOVS","title":"Blacklisting rules","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Once we have the list of the bad channels for each of the assimilation time you'd like to apply your ATOVS assimilation, the follow the rules described below. Let take the following decision for example. We need to blacklist the channels 5,6,7, 8 from NOAA-18 AMSU-A 12 UTC. How to register the bad channels in the file called LISTELOC{HH} is described in this presentation.","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"According to our example, we create the file called LISTE_LOC_12 with the following content:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"N  7 210      209       3 TOVS4       5       6       7       8\nN  7 210      209       3 TOVS5      11      12      13      14      15","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"The first line means: N      - blacklist 7      - satem observation 210    - level 1c 209    - NOAA-18 3      - AMSU-A TOVS4  - blacklist 4 channels, which are 5, 6,7 and 8","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Put the LISTE_LOC_$HH files under the nam directory.","category":"page"},{"location":"Observations/Atovs/#Set-up-the-VarBC-coefficients-for-your-experiment-or-operational-data-assimilation","page":"ATOVS","title":"Set up the VarBC coefficients for your experiment or operational data assimilation","text":"","category":"section"},{"location":"Observations/Atovs/#For-experiments","page":"ATOVS","title":"For experiments","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Here we have already the VarBC coefficients, so we do warm start (see setup above). her is the procedure:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Fetch the VARBC.cycle files from the latest odb_stuff.tar for each assimilation time. In case of 3h cycling, we fetch the VARBC.cycle from the latest 8 cycles. Rename them the following way: VARBC.cycle.$DOMAIN.$EMONTH.$HH, where EMONTH can be SUMMER or WINTER. Like for example: VARBC.cycle.AROME_Arctic.SUMMER.12.\nPut these files under const/bias_corr/","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"These files are arranged this Fetchassimdata the following way: For all Hamonie cycles up to 40:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"     # Fetch data\n       if [ ! -s ${DLOCVARBC}/VARBC.cycle] ; then\n\n         echo \"Fetch the data from $HM_LIB/const/bias_corr\"\n         # Set the proper VARBC period for coefficients\n         case $MM in\n           10|11|12|01|02|03)\n             EMONTH=WINTER\n           ;;\n           04|05|06|07|08|09)\n             EMONTH=SUMMER\n           ;;\n           *)\n             echo \"This should never happen. MM is $MM\"\n             exit 1\n         esac\n\n         cp $HM_LIB/const/bias_corr/VARBC.cycle.$DOMAIN.$EMONTH.$HH ${DLOCVARBC}/VARBC.cycle || \\\n         { echo \"Could not find cold start VARBC data VARBC.cycle.$EMONTH.$HH\" ; exit 1 ; }\n              ls -lrt ${DLOCVARBC}\n       fi","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"From cycle 43:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"     # Fetch data\n       if [ ! -s ${DLOCVARBC}/VARBC.cycle] ; then\n\n         echo \"Fetch the data from $HM_LIB/const/bias_corr\"\n         # Set the proper VARBC period for coefficients\n         case $MM in\n           10|11|12|01|02|03)\n             EMONTH=WINTER\n           ;;\n           04|05|06|07|08|09)\n             EMONTH=SUMMER\n           ;;\n           *)\n             echo \"This should never happen. MM is $MM\"\n             exit 1\n         esac\n\n         cp $HM_LIB/const/bias_corr/${DOMAIN}/VARBC.cycle.$HH ${DLOCVARBC}/VARBC.cycle || \\\n         { echo \"Could not find cold start VARBC data VARBC.cycle.$EMONTH.$HH\" ; exit 1 ; }\n              ls -lrt ${DLOCVARBC}\n       fi","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"With a tiny difference that all the VarBC files are now stored under a ${DOMAIN} directory. This allows our system to be up-to-date and ready for all known model domains. Please send your VarBC files to the system administrators.","category":"page"},{"location":"Observations/Atovs/#For-operational-implementation","page":"ATOVS","title":"For operational implementation","text":"","category":"section"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"The setup is much easier. Name the VARBC.cycle files the following way VARBC.cycle.${HH} and put them in $ARCHIVE_ROOT/VARBC_latest, which you need to create.","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"To check that you have done things right:","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"Doing experiment: Check that you have all LISTE_LOC_$HH files under the nam directory, and the VARBC.cycle.$DOMAIN.$EMONTH.$HH files under const/bias_corr directory.\nOperational implementation: Check that you have all LISTE_LOC_$HH files under the nam directory, and the VARBC.cycle.$HH under the $ARCHIVE_ROOT/VARBC_latest directory.","category":"page"},{"location":"Observations/Atovs/","page":"ATOVS","title":"ATOVS","text":"If you passed the test, then you are ready with ATOVS implementation. Congratulation!","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#Model-Domain","page":"Domain","title":"Model Domain","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/#Introduction","page":"Domain","title":"Introduction","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"There are four projections available in HARMONIE, polar stereographic, lambert, mercator and rotated mercator. The model itself chooses the best (least distortion) projection among the first three given your domain specifications. The rotated mercator projection is selected through the variable LROTMER. Note that the polar stereographic project is defined at 90^o N(S) whereas in GRIB1 it is defined at 60^o N(S).","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"(Image: projections)","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"Polar stereographic, Lambert and Mercator projection.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"(Image: Rotated Mercator)","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"Rotated mercator projection","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#Model-domain-settings","page":"Domain","title":"Model domain settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"For each domain we set variables related to the geometry and the resolution like:","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"HARMONIE model domains are defined in settings in scr/Harmonie_domains.pm. The following variables related to the geometry and the resolution are required:","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"TSTEP is model timestep in seconds\nNLON is number of points in x-direction.\nNLAT is number of points in y-direction.\nLONC is the longitude of domain centre in degrees.\nLATC is the latitude of domain center in degrees.\nLON0 is the reference longitude of the projection in degrees.\nLAT0 is the reference latitude of the projection in degrees. If LAT0 is set to 90, the projection is polar stereographic. If LAT0 < 90, the projection is lambert unless LMRT=.TRUE.  \nGSIZE is grid size in meters in both x- and y-direction.\nEZONE is number of points over extension zone in both x- and y-direction. Default value 11. \nLMRT switch for rotated Mercator projection. If LMRT=.TRUE. LAT0 should be zero.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"NLON and NLAT should satisfy the equation 5^b * 3^d * 2^e, where a-e are integers geq 0.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"The default area is the Denmark domain (DKCOECP). The following values for C+I zone and truncation are calculated in src/Harmonie_domains.pm from the values above. ","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"NDLUXG is number of points in x-direction without extension (E) zone.\nNDGUXG is number of points in y-direction without extension (E) zone.\nNMSMAX_LINE is truncation order in longitude. By default (NLON-2)/2. \nNSMAX_LINE is truncation order in latitude. By default (NLAT-2)/2. \nNMSMAX_QUAD is truncation order in longitude. By default (NLON-2)/3. It is used to create filtered orography with lower resolution.\nNSMAX_QUAD is truncation order in latitude. By default (NLAT-2)/3. It is used to create filtered orography with lower resolution.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"(Image: cie domain)","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#Domain-creation-tool","page":"Domain","title":"Domain creation tool","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"To help with the design of a new domain, there is an interactive tool that lets you experiment with the grid parameters described above, and visualize the resulting domain immediately on a map, see figure below.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"(Image: )","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"At present, it only works for Lambert and polar stereographic projection, not rotated mercator.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#Creating-a-new-domain","page":"Domain","title":"Creating a new domain","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"If you are happy with your new domain created with the help of the domain creation tool you can add it to scr/Harmonie_domains.pm for your experiment, my_exp (assuming you have set up the experiment):","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie co scr/Harmonie_domains.pm\n#\n# add domain information for new domain called MYNEWDOM in this file\n#\nvi scr/Harmonie_domains.pm\n#\n# set DOMAIN=MYNEWDOM in the experiment config file\n#\nvi ecf/config_exp.h ","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"You can now start a new experiment with a newly defined domain called MYNEWDOM.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#Create-a-test-domain-with-gl","page":"Domain","title":"Create a test domain with gl","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"Before you go through the full climate generation process you can generate a test domain using gl. Define your domain in the namelist like:","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"&NAMINTERP\nOUTGEO%NLON = 300 ,\nOUTGEO%NLAT = 300,\nOUTGEO%PROJECTION = 3,\nOUTGEO%WEST = 17.0,\nOUTGEO%SOUTH = 58.0,\nOUTGEO%DLON = 2500.0\nOUTGEO%DLAT = 2500.0\nOUTGEO%PROJLAT = 60.0\nOUTGEO%PROJLAT2 = 60.0\nOUTGEO%PROJLON = 0.0,\n/","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"Running gl using this namelist by","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"gl -n namelist_file","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/","page":"Domain","title":"Domain","text":"will create an GRIB file with a constant orography which you can use for plotting. ","category":"page"},{"location":"PostProcessing/PostPP/","page":"PostPP","title":"PostPP","text":"Overview of postprocessing HARMONIE model output ...","category":"page"},{"location":"PostProcessing/PostPP/","page":"PostPP","title":"PostPP","text":"Some links to more detailed documentation:","category":"page"},{"location":"PostProcessing/PostPP/","page":"PostPP","title":"PostPP","text":"HARMONIE data conversion (FA –> GRIB)\nPostprocessing with FULL-POS\nPostprocessing with gl\nPostprocessing with xtool\nPostprocessing with EPyGrAM\nDiagnostics","category":"page"},{"location":"System/Build_local_docs/#Build-system-documentation-locally","page":"Local Documentation","title":"Build system documentation locally","text":"","category":"section"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"This page explains how to build the system documentation locally","category":"page"},{"location":"System/Build_local_docs/#Install-Julia","page":"Local Documentation","title":"Install Julia","text":"","category":"section"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"Documentation is build using Julia. It is strongly recommended that the official generic binaries from the downloads page be used to install Julia","category":"page"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"Download Julia and untar ","category":"page"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"wget https://julialang-s3.julialang.org/bin/linux/x64/1.7/julia-1.7.2-linux-x86_64.tar.gz\ntar zxvf julia-1.7.2-linux-x86_64.tar.gz","category":"page"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"Add the julia bin directory to your PATH","category":"page"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"export PATH=/path/to/julia/bin:$PATH","category":"page"},{"location":"System/Build_local_docs/#Install-dependencies","page":"Local Documentation","title":"Install dependencies","text":"","category":"section"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"To install the Documenter.jl dependency","category":"page"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"julia --project=docs/ -e 'using Pkg; Pkg.instantiate()' ","category":"page"},{"location":"System/Build_local_docs/#Build-documentation","page":"Local Documentation","title":"Build documentation","text":"","category":"section"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"To create the HTML pages from the markdown files run","category":"page"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"julia --project=docs/ docs/make.jl ","category":"page"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"[ Info: SetupBuildDirectory: setting up build directory.\n[ Info: Doctest: running doctests.\n[ Info: ExpandTemplates: expanding markdown templates.\n[ Info: CrossReferences: building cross-references.\n[ Info: CheckDocument: running document checks.\n[ Info: Populate: populating indices.\n[ Info: RenderDocument: rendering document.\n[ Info: HTMLWriter: rendering HTML pages.\n┌ Info: Deployment criteria for deploying devbranch build from GitHub Actions:\n│ - ✔ ENV[\"GITHUB_REPOSITORY\"]=\"github.com/Hirlam/HarmonieSystemDocumentation.git\" occurs in repo=\"github.com/Hirlam/HarmonieSystemDocumentation.git\"\n│ - ✘ ENV[\"GITHUB_EVENT_NAME\"]=\"\" is \"push\"\n│ - ✘ ENV[\"GITHUB_REF\"] matches devbranch=\"pre-CY46h1\"\n│ - ✘ ENV[\"GITHUB_ACTOR\"] exists\n│ - ✘ ENV[\"DOCUMENTER_KEY\"] or ENV[\"GITHUB_TOKEN\"]  exists\n└ Deploying: ✘","category":"page"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"The HTML pages will be put in docs/build. Open index.html in a browser","category":"page"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"firefox docs/build/index.html","category":"page"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"A local build will not deploy the HTML pages to github.com/Hirlam/HarmonieSystemDocumentation.git. ","category":"page"},{"location":"System/Build_local_docs/","page":"Local Documentation","title":"Local Documentation","text":"Also see .github/workflows/documentation.yml ","category":"page"},{"location":"System/StandaloneOdb/#ODB-software","page":"Stand alone ODB","title":"ODB software","text":"","category":"section"},{"location":"System/StandaloneOdb/#Get-the-software","page":"Stand alone ODB","title":"Get the software","text":"","category":"section"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"To make best use of ODB information produced by your Harmonie experiment one should use ODB and ODB-API software developed by ECMWF. Below are instruction on how to obtain the software from ECMWF.","category":"page"},{"location":"System/StandaloneOdb/#ODB-API","page":"Stand alone ODB","title":"ODB-API","text":"","category":"section"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"ODB-API software is open source and released under an Apache licence: https://software.ecmwf.int/wiki/display/ODBAPI ","category":"page"},{"location":"System/StandaloneOdb/#ODB","page":"Stand alone ODB","title":"ODB","text":"","category":"section"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"ODB stands for Observational !DataBase. It is database software to store and retrieve large amounts of meteorological numerical data in an efficient manner while used from within IFS. ODB software mimics relational database queries through its ODB/SQL -compiler and accesses data currently via a Fortran90 library interface. The original documentation is available here: http://www.ecmwf.int/research/ifsdocs/CY28r1/pdf_files/odb.pdf","category":"page"},{"location":"System/StandaloneOdb/#Building-your-ODB-software","page":"Stand alone ODB","title":"Building your ODB software","text":"","category":"section"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"The ODB-API Software bundle uses cmake http://www.cmake.org to configure the make files used to compile the software. The instructions below worked with Redhat 7/GCC 4.8.5 and CentOS 7/GCC 4.8.5. On newer systems python functionality may have to be switched off with -DENABLE_PYTHON=OFF.","category":"page"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"VERSION=0.18.1\nwget https://software.ecmwf.int/wiki/download/attachments/61117379/odb_api_bundle-${VERSION}-Source.tar.gz\ngunzip odb_api_bundle-${VERSION}-Source.tar.gz\ntar -xvf odb_api_bundle-${VERSION}-Source.tar\ncd odb_api_bundle-${VERSION}-Source\nmkdir build\ncd build\ncmake.. -DCMAKE_INSTALL_PREFIX=/opt/metapp/odb_api/${VERSION}/gnu \\\n-DENABLE_ODB_API_SERVER_SIDE=ON -DENABLE_FORTRAN=ON \\\n-DENABLE_GRIB=OFF -DENABLE_ODB_SERVER_TIME_FORMAT_FOUR_DIGITS=ON \\\n-DENABLE_PYTHON=ON -DENABLE_ODB=ON -DODB_SCHEMAS=\"ECMA;CCMA\"\nmake -j 2\nctest\nmake install","category":"page"},{"location":"System/StandaloneOdb/#ACTION:-FOR-EOIN:-everything-below-here-needs-to-be-updated","page":"Stand alone ODB","title":"ACTION: FOR EOIN: everything below here needs to be updated","text":"","category":"section"},{"location":"System/StandaloneOdb/#ODB-data","page":"Stand alone ODB","title":"ODB data","text":"","category":"section"},{"location":"System/StandaloneOdb/#Convert-ODB-1-to-ODB-2","page":"Stand alone ODB","title":"Convert ODB-1 to ODB-2","text":"","category":"section"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"Details on how to convert your ODB-1 (Harmonie experiment) databases to ODB-2 using odb_migrator are described here. I have used version 0.9.31 of ODB-API (I have had some problems with 0.9.32). I will use a Harmonie CCMA conventional ODB-1 database as an example:","category":"page"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"tar -xvf odb_ccma.tar\ncd odb_ccma/CCMA/\ndcagen\ncd ../../\nls -l ../conv.38h1.sql\nwhich odb_migrator\n/opt/metlib/odb_api/0.9.31/gnu/bin/odb_migrator -addcolumns \"expver='    38h1',class=2,stream=1025,type=264\" odb_ccma/CCMA ../conv.38h1.sql var${DTG}.odb\nls -l var${DTG}.odb","category":"page"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"Here is the SQL file used: conv.38h1.sql. To construct my conv.38h1.sql file did carried out the following commands:","category":"page"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"cd odb_ccma/CCMA/\nodbsql -q \"select * from desc,timeslot_index,hdr,body\" | head -1 ","category":"page"},{"location":"System/StandaloneOdb/#Instructions-for-use-on-ecgb","page":"Stand alone ODB","title":"Instructions for use on ecgb","text":"","category":"section"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"On ecgb I have installed odb_migrator in my own account. ODB developers have promise to provide a \"system\" installation of odbmigrator soon. The next version of ODB-API on ecgb should include *odbmigrator*. My installation is here:","category":"page"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"/home/ms/ie/dui/odbapi/0.9.31","category":"page"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"To produce your own ODB-2 file from a Harmonie CCMA tar ball on ecgb:","category":"page"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"module load odb\nexport PATH=/home/ms/ie/dui/odbapi/0.9.31/bin:$PATH\nexport LD_LIBRARY_PATH=/home/ms/ie/dui/odbapi/0.9.31/lib:$LD_LIBRARY_PATH\ncd $SCRATCH\nmkdir ODB2odb\ncd ODB2odb\n## copy your odb_stuff.tar/odb_ccma.tar file from ECFS/your home system to this directory\n## eg -- ecp ec:/dui/harmonie/refSonde38h1p1/2013/12/27/00/odb_stuff.tar .\ntar -xvf odb_stuff.tar\ntar -xvf odb_ccma.tar\ncd odb_ccma/CCMA/\ndcagen\ncd ../../\nodb_migrator odb_ccma/CCMA -addcolumns \"expver='refSonde',class=2,stream=1025,type=264\" /home/ms/ie/dui/odbapi/conv.38h1.sql conv2013122700.odb\nodb header conv2013122700.odb\nodb sql 'select distinct varno' -i conv2013122700.odb\nodb sql 'select count(*) where varno=2' -i conv2013122700.odb\n#\nrm -f bdstrategy odb*.tar ## tidy up the directory if you wish\nrm -rf odb_ccma","category":"page"},{"location":"System/StandaloneOdb/#ODB-visualisation","page":"Stand alone ODB","title":"ODB visualisation","text":"","category":"section"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"ODB-2 data can be visualized (directly) using Metview. On local platforms Metview must be built with ODB support (that uses ODB-API software) to visualize ODB-2 data. Here is an example, odbmap.mv4, of using Metview on ecagte to visualize ODB-2 data using a Metview Macro. This example requires an ODB-2 data file as input. Optionally this macro can also plot domain grid-points from a Harmonie GRIB file (called dom.grib) to indicate the extent of your Harmonie model domain.","category":"page"},{"location":"System/StandaloneOdb/","page":"Stand alone ODB","title":"Stand alone ODB","text":"Usage:\nodbmap:: Usage: metview -b odbmap.mv4 inputfile odbvar odbrequest odblegend outputtype\nodbmap::   where: inputfile  -- ODB-2 file\nodbmap::   where: odbvar     -- ODB variable to be plotted\nodbmap::                     -- (enclosed with inverted commas)\nodbmap::   where: odbrequest -- ODB SQL request \nodbmap::                     -- (enclosed with inverted commas)\nodbmap::   where: odblegend  -- legon/legoff\nodbmap::   where: outputtype -- ps/png\nAn example to plot 2m temperature observation values on 25th December 2013 at 12z:\ncd  $SCRATCH\ncp -r /home/ms/ie/dui/odbMacroTest .\ncd odbMacroTest\nmetview4 -b odbmap.mv4 conv201312.odb \"obsvalue\" \"andate=20131225 and antime=120000 and varno=39\" legon png\nxv odbmap.1.png","category":"page"},{"location":"Build/Centos6Install/#Centos-6-instructions","page":"Centos6 Install","title":"Centos 6 instructions","text":"","category":"section"},{"location":"Build/Centos6Install/#INFORMATION","page":"Centos6 Install","title":"INFORMATION","text":"","category":"section"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"Please note: the version of gcc/gfortran available on CentOS/Redhat 6 platforms ((GCC) 4.4.7 !20120313 (Red Hat 4.4.7-16)) is not recent enough to compile harminie-40h1 code. gcc/gfortran, netCDF and HDF5 must be installed locally (from source).","category":"page"},{"location":"Build/Centos6Install/#Requirements","page":"Centos6 Install","title":"Requirements","text":"","category":"section"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"This is a HOWTO for building and running Harmonie on a CentOS 6 PC with GNU compilers. This should probably work on a Redhat 6 PCs too. harmonie-38h1 code was used to develop this documentation.","category":"page"},{"location":"Build/Centos6Install/#bit-OS","page":"Centos6 Install","title":"64-bit OS","text":"","category":"section"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"Enter the following command in a terminal to check you actually have a 64-bit Linux PC:","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"uname  -m -i -p","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"This should return:","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"x86_64 x86_64 x86_64","category":"page"},{"location":"Build/Centos6Install/#OS-software","page":"Centos6 Install","title":"OS software","text":"","category":"section"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"This list of required software is a guess at the moment. Your system may require the installation of other libraries. The following instructions require root access to your PC.","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"The compilers","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"yum install gcc.x86_64\nyum install gcc-c++.x86_64\nyum install gcc-gfortran.x86_64\nyum install libgcc.x86_64\nyum install openmpi.x86_64\nyum install openmpi-devel.x86_64","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"Get yacc/bison","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"yum install byacc.x86_64 bison.x86_64 bison-devel.x86_64","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"Get BLAS/LAPACK","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"yum install blas.x86_64 blas-devel.x86_64\nyum install lapack.x86_64 lapack-devel.x86_64","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"I installed NetCDF from the EPEL (Extra Packages for Enterprise Linux) repository. Here is how to enable access to this repository by your CentOS 6 PC:","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"wget http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm\nrpm -Uvh epel-release-6*.rpm\nls -1 /etc/yum.repos.d/epel*","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"now install netCDF","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"yum install netcdf.x86_64\nyum install netcdf-devel.x86_64\nyum install netcdf-static.x86_64","category":"page"},{"location":"Build/Centos6Install/#Get-the-code","page":"Centos6 Install","title":"Get the code","text":"","category":"section"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"For the 38h1.2.beta.2 tag:","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"mkdir -p $HOME/harmonie_releases/tags\ncd $HOME/harmonie_releases/tags\nsvn co https://svn.hirlam.org/tags/harmonie-38h1.2.beta.2","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"For trunk:","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"mkdir -p $HOME/harmonie_releases\ncd $HOME/harmonie_releases\nvn co https://svn.hirlam.org/trunk/harmonie \nln -s harmonie trunk","category":"page"},{"location":"Build/Centos6Install/#Compile-Harmonie","page":"Centos6 Install","title":"Compile Harmonie","text":"","category":"section"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"Now let's create our first Harmonie experiment (METIE.LinuxPC setup is designed for standard CentOS 6 Linux PCs):","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"cd $HOME\nmkdir -p hm_home/trunkexp\ncd $HOME/hm_home/trunkexp\n$HOME/harmonie_releases/trunk/config-sh/Harmonie setup -r $HOME/harmonie_releases/trunk -h METIE.LinuxPC","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"Local changes that may be required ... in the Env_system:","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":":\n:\nmodule load metlib/odbapi/0.9.31-gnu\n:\n:\n# Climate data location\nexport HM_CLDATA=/home/ewhelan/harmonie_climate/38h1.1\nexport HM_SAT_CONST=/home/ewhelan/harmonie_sat_const\n:\n:\n# Jb data location\nexport JBDIR=/opt/metdata/harmonie_jbdata\n:\nexport SMSTASKMAX=4","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"Local changes that may be required ... in the Env_submit:","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"  $nprocy=2; # instead of 8 if you only have a dual-/quad-core PC","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"Now use the Harmonie system to build the software:","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"cd $HOME/hm_home/trunkexp\n$HOME/harmonie_releases/trunk/config-sh/Harmonie Install","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"This uses the Harmonie MAKEUP utility to compile the code and create libraries and executables required. Further details on MAKEUP are available here","category":"page"},{"location":"Build/Centos6Install/#Run-an-experiment","page":"Centos6 Install","title":"Run an experiment","text":"","category":"section"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"Instructions for testbed and/or local experiment are detailed here:","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"Your first experiment will require changes to be made to the default settings in $HOME/hm_home/trunkexp/ecf/config_exp.h :","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"DOMAIN=IRELAND150       ## choose a small domain to run on your limited PC.\n                        ## See $HOME/harmonie_releases/trunk/scr/Harmonie_domains.pm for existing definitions\nVLEV=HIRLAM_60          ## I only have (easy access) to HIRLAM model level files on my PC \nANASURF_INLINE=\"no\"     ## I have experienced some issues with my setup calling SODA from inside CANARI\nHOST_MODEL=\"hir\"        ## tell boundary processing that you are using HIRLAM model boundary files\nOBDIR=$HOME/scratch/obs ## tell Harmonie where your BUFR observation files are\nBDDIR=$HOME/scratch/bnd ## tell Harmonie where your input boundary files are (HIRLAM or IFS files normally)\nBDSTRATEGY=available    ## I use a more forgiving boundary file strategy\nBDINT=3                 ## I only have (HIRLAM) boundary files every 3 hours","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"Once you think you have all your ducks in a row you can try to run your first experiment:","category":"page"},{"location":"Build/Centos6Install/","page":"Centos6 Install","title":"Centos6 Install","text":"cd $HOME/hm_home/trunkexp\n$HOME/harmonie_releases/trunk/config-sh/Harmonie start DTG=2014040100 DTGEND=2014040112 LL=03 BUILD=no","category":"page"},{"location":"Build/Redhat7Install/#Redhat-7-instructions","page":"Redhat7 Install","title":"Redhat 7 instructions","text":"","category":"section"},{"location":"Build/Redhat7Install/#Requirements","page":"Redhat7 Install","title":"Requirements","text":"","category":"section"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"This is a HOWTO for building and running Harmonie on a Redhat 7 server with GNU compilers using Open MPI. This should probably work on a CentOS 7 PCs too.","category":"page"},{"location":"Build/Redhat7Install/#bit-OS","page":"Redhat7 Install","title":"64-bit OS","text":"","category":"section"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Enter the following command in a terminal to check you actually have a 64-bit Linux PC:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"uname  -m -i -p","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"This should return:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"x86_64 x86_64 x86_64","category":"page"},{"location":"Build/Redhat7Install/#OS-software","page":"Redhat7 Install","title":"OS software","text":"","category":"section"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"This list of required software is a guess at the moment. Your system may require the installation of other libraries. The following instructions require root access to your PC.","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Install subversion to permit easy download of the code:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"yum install subversion.x86_64","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"To use the mXCdp GUI to monitor the HARMONIE mini-SMS system the perl-Tk library is required. At the time of writing I could not find a trustworthy perl-Tk rpm to install the software. Some system libraries may be required (this list may not be complete):","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"yum install perl-devel.x84_64 perl-Time-HiRes.x86_64\nyum install gcc.x864_64\nyum install libX11-devel.x86_64 libxcb-devel.x86_64 xorg-x11-proto-devel.noarch libXau-devel.x86_64\nyum install libpng-devel.x86_64 zlib-devel.x86_64","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"... and here is how to install perl-Tk from source:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"cd $HOME\nwget http://search.cpan.org/CPAN/authors/id/S/SR/SREZIC/Tk-804.032.tar.gz\ngunzip Tk-804.032.tar.gz\ntar -xvf Tk-804.032.tar\ncd Tk-804.032\nperl Makefile.PL\nmake\nmake test\nmake install","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"The compilers","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"yum install gcc-gfortran.x86_64 libgfortran.x86_64 libquadmath.x86_64 libquadmath-devel.x86_64","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"ksh for the makeup:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"yum install ksh","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Get yacc/bison","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"yum install flex.x86_64\nyum install bison.x86_64 byacc.x86_64","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Get Open MPI:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"yum install environment-modules.x86_64 infinipath-psm.x86_64 libesmtp.x86_64 opensm-libs.x86_64libibumad.x86_64 tcl.x86_64\nyum install openmpi.x86_64 openmpi-devel.x86_64","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Enable access to EPEL software:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"wget https://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm\nyum install epel-release-7-5.noarch.rpm","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Get BLAS/LAPACK (the requirement to create soft-links for BLAS and LAPACK may be corrected in EPEL at some stage).","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"yum install blas.x86_64 blas-devel.x86_64\nyum install lapack.x86_64 lapack-devel.x86_64\ncd /usr/lib64\nln -s liblapack.so.3 liblapack.so\nln -s libblas.so.3 libblas.so","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"I also installed NetCDF and HDF5 from the EPEL:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"yum install netcdf.x86_64 netcdf-cxx.x86_64 netcdf-cxx-devel.x86_64 netcdf-cxx-static.x86_64 netcdf-devel.x86_64 netcdf-fortran.x86_64 netcdf-fortran-devel.x86_64 netcdf-static.x86_64 hdf5.x86_64 hdf5-devel.x86_64 libcurl-devel.x86_64","category":"page"},{"location":"Build/Redhat7Install/#Get-the-code","page":"Redhat7 Install","title":"Get the code","text":"","category":"section"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"For trunk:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"mkdir -p $HOME/harmonie_releases\ncd $HOME/harmonie_releases\nsvn co https://svn.hirlam.org/trunk/harmonie \nln -s harmonie trunk","category":"page"},{"location":"Build/Redhat7Install/#Compile-Harmonie","page":"Redhat7 Install","title":"Compile Harmonie","text":"","category":"section"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Now let's create our first Harmonie experiment (METIE.LinuxPC setup is designed for standard CentOS 6 Linux PCs):","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"cd $HOME\nmkdir -p hm_home/trunkexp\ncd $HOME/hm_home/trunkexp\n$HOME/harmonie_releases/trunk/config-sh/Harmonie setup -r $HOME/harmonie_releases/trunk -h METIE.LinuxRH7gnu","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Local changes that may be required ... in the Env_system:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":":\n:\n:\n# Climate data location\nexport HM_CLDATA=/data/nwp/harmonie_climate/40h1\nexport HM_SAT_CONST=/data/nwp/harmonie_sat_const\n:\n:\n# Jb data location\nexport JBDIR=/data/nwp/harmonie_jbdata\n:\nexport SMSTASKMAX=4","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Local changes that may be required ... in the Env_submit:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"  $nprocy=2; # instead of 8 if you only have a dual-/quad-core PC","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Now use the Harmonie system to build the software:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"cd $HOME/hm_home/trunkexp\n$HOME/harmonie_releases/trunk/config-sh/Harmonie Install","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"This uses the Harmonie MAKEUP utility to compile the code and create libraries and executables required. Further details on MAKEUP are available here","category":"page"},{"location":"Build/Redhat7Install/#Run-an-experiment","page":"Redhat7 Install","title":"Run an experiment","text":"","category":"section"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Instructions for testbed and/or local experiment are detailed here:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Your first experiment will require changes to be made to the default settings in $HOME/hm_home/trunkexp/ecf/config_exp.h :","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"DOMAIN=IRELAND150       ## choose a small domain to run on your limited PC.\n                        ## See $HOME/harmonie_releases/trunk/scr/Harmonie_domains.pm for existing definitions\nVLEV=HIRLAM_60          ## I only have (easy access) to HIRLAM model level files on my PC \nANASURF_INLINE=\"no\"     ## I have experienced some issues with my setup calling SODA from inside CANARI\nHOST_MODEL=\"hir\"        ## tell boundary processing that you are using HIRLAM model boundary files\nOBDIR=$HOME/scratch/obs ## tell Harmonie where your BUFR observation files are\nBDDIR=$HOME/scratch/bnd ## tell Harmonie where your input boundary files are (HIRLAM or IFS files normally)\nBDSTRATEGY=available    ## I use a more forgiving boundary file strategy\nBDINT=3                 ## I only have (HIRLAM) boundary files every 3 hours","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"Once you think you have all your ducks in a row you can try to run your first experiment:","category":"page"},{"location":"Build/Redhat7Install/","page":"Redhat7 Install","title":"Redhat7 Install","text":"cd $HOME/hm_home/trunkexp\n$HOME/harmonie_releases/trunk/config-sh/Harmonie start DTG=2014040100 DTGEND=2014040112 LL=03 BUILD=no","category":"page"},{"location":"Observations/Scatt/#Scatterometers","page":"Scatt","title":"Scatterometers","text":"","category":"section"},{"location":"Observations/Scatt/#Background","page":"Scatt","title":"Background","text":"","category":"section"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"The EUMETSAT OSI SAF produces different scatterometer wind products at KNMI and more will become available in 2019:","category":"page"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"C-band ASCAT-A/B/C overpassing at 9:30/21:30 Local Solar Time (LST), since 2007/2011/2019;\nKu-band !ScatSat overpassing at 8:45/20:45 LST, since 2017;\nKu-band HY2A/B overpassing at 6:00/18:00 LST, since 2013 (n.a. in NRT)/2019;\nKu-band CFOSAT overpassing at 7:00/19:00 LST, expected 2019;\nKu-band OSCAT3 overpassing at 12:00/24:00, expected 2019;\nC/Ku-band !WindRad overpassing at 6:00/18:00, expected 2020.","category":"page"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"Note that the products have different ambiguity and noise properties, that are handled in the generic KNMI processing. We distinguish two types of scatterometers with (1) static beams (ASCAT) and with (2) rotating beams (the rest).","category":"page"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"In the ECMWF model (on ~200 km scales) the availability of three hourly observations is motivated from the experience of assimilating ASCAT and OSCAT (2.5 hours overpass time difference), which showed double the impact of assimilating ASCAT only. So, they appear as independent data sources for the model.","category":"page"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"Since ASCAT overpasses only twice per day we cannot fulfil the temporal requirement and can therefore not expect to analyze open ocean surface winds deterministically at 25 km scales with ASCAT only. Based on this analysis we should therefore focus on larger than 25 km scales (as ECMWF does), also for Harmonie, so typically focus on 100 km scales. This means that scales between ~25-100 km in Harmonie over open sea is mostly noise, which can be removed through supermodding (ref: Mate Mile's project). Note that more scatterometers will be available next year at more times a day (see above).","category":"page"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"ECMWF is testing ASCAT with different aggregation, thinning and weights in order to optimize scatterometer data assimilation, which results may be useful for HARMONIE data assimilation strategy as well.","category":"page"},{"location":"Observations/Scatt/#ASCAT","page":"Scatt","title":"ASCAT","text":"","category":"section"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"ASCAT-12.5km (or ASCAT-coastal) data are available on a 12.5 km grid.\nThe resolution of ASCAT-12.5km is about 25 km (through the application of a Hanning with tails extending beyond 12.5 km)\nAs a result, the errors of neighbouring observations are correlated. For the 6.25 km product:\nalong-track wind component l : neighbor 0.60; next-neighbor 0.19; next-next neighbor 0.02; total noise variance 0.385\ncross-track wind component t : neighbor 0.51; next-neighbor 0.11; next-next neighbor 0.00; total noise variance 0.214\nThis agrees well with the footprint overlap (see point 2). We expect similar values for ASCAT-12.5km, but this could be easily assessed more dedicated.\nTriple collocation tests show obervation error standard deviation for ASCAT-12.5km (or ASCAT-coastal) of ~ 0.7 m/s for u and v.\nThe effective model resolution of Harmonie (with 2.5 km grid) is about 20-25 km.","category":"page"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"Based on this one may conclude that the resolution of ASCAT-12.5km and Harmonie is about the same, so the representativeness error is negligible, and the total error equal to the observation error, i.e., 0.7 m/s and use this value for giving weight to ASCAT in Harmonie.","category":"page"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"However, we think this will not give the best impact. This is because if you want to analyse model states on 25 km scales (Harmonie effective resolution) deterministically, you need a forcing term which accounts for this resolution. Forcing can be either from orography (over land only) or observations. So,  over sea we have to rely on the density of the observation network. To analyse scales up to 25 km deterministically over sea requires high density observations both in space and time, i.e., for the latter at least every hour. This is corroborated by studies with ASCAT A and B, separated in time by 50 minutes, showing high correlation of ASCAT divergence and convergence with moist convection rain, but negligible  correlation between convergence or divergence of the two passes.","category":"page"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"Since ASCAT overpasses only twice per day we can not fulfil the temporal requirement and can therefore not expect to analyse ocean surface winds deterministically at 25 km scales with ASCAT only. Based on this analysis we should therefore focus on larger than 25 km scales (as ECMWF does), also for Harmonie, so typically focus on 100 km scales. This means that scales between ~25-100 km in Harmonie over sea is mostly noise, which can be removed through supermodding, i.e., the project where Mate Mile is working on.","category":"page"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"KNMI are waiting for a data feed from EUMETSAT. Level 1 ASCAT data available 14 March 2019 https://www.eumetsat.int/website/home/TechnicalBulletins/Metop/DAT_4128787.html","category":"page"},{"location":"Observations/Scatt/#Other-scatterometers","page":"Scatt","title":"Other scatterometers","text":"","category":"section"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"25km data are generally available on a the satellite swath grid of WVCs\nThe resolution of this 25 km data is around 100 km (through the application of a spatial filter that successfully suppresses both wind direction ambiguities and noise)\nAs a result, the errors of neighboring observations are correlated over a distance of 100  km or more\nTriple collocation tests show observation error standard deviation ~ 0.7 m/s for u and v\nBiases exist at warm and cold SST of up to 0.5 m/s, which are being corrected; also winds around nadir and, to a lesser extent, in the outer swath are sometimes biased; the IFS takes account of this, but may need retuning for CFOSAT","category":"page"},{"location":"Observations/Scatt/#Further-reading","page":"Scatt","title":"Further reading","text":"","category":"section"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"More information is available on the OSI SAF wind site in the form of training material, product manuals, scientific publications, verification reports and monitoring information. Support and services messages for all products can be obtained through scat at knmi.nl .","category":"page"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"The EUMETSAT NWP SAF provides the following reports:","category":"page"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"High resolution data assimilation guide, v1.2 (NWPSAF-KN-UD-008)\nWind Bias Correction Guide, v1.3 (NWPSAF-KN-UD-007)","category":"page"},{"location":"Observations/Scatt/#Model","page":"Scatt","title":"Model","text":"","category":"section"},{"location":"Observations/Scatt/#Enable-assimilation","page":"Scatt","title":"Enable assimilation","text":"","category":"section"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"Set SCATT_OBS=1 in scr/include.ass\nEnsure ascat${DTG} files are available in $OBDIR (defined in ecf/config_exp.h )","category":"page"},{"location":"Observations/Scatt/#Technical-information","page":"Scatt","title":"Technical information","text":"","category":"section"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"Referred to as NSCAT3 in arpifs (see src/arpifs/module/yomcoctp.F90)\nFrom https://apps.ecmwf.int/odbgov\nobstype=9\ncodetype=139\nsensor=190\nvarno=125/124 for ambiguos u/v wind component","category":"page"},{"location":"Observations/Scatt/#Issues-(CY40/CY43)","page":"Scatt","title":"Issues (CY40/CY43)","text":"","category":"section"},{"location":"Observations/Scatt/#Thinning:-NASCAWVC","page":"Scatt","title":"Thinning: NASCAWVC","text":"","category":"section"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"Number of ASCAT wave vector cells\nDefined in src/arpifs/module/yomthlim.F90\nDefault, set in src/arpifs/obs_preproc/sufglim.F90, is 42 (for 25-km product)\nSet to 82 for 12.5-km scatterometer product in nam/harmonie_namelists.pm (possibly also in sufglim.F90. To be checked)","category":"page"},{"location":"Observations/Scatt/#Observation-error","page":"Scatt","title":"Observation error","text":"","category":"section"},{"location":"Observations/Scatt/","page":"Scatt","title":"Scatt","text":"Set by Bator (src/odb/pandor/module/bator_init_mod.F90) u_err=1.39, v_err=1.54\nSuggested values from KNMI: u_err=1.4, v_err=1.4\nZWE=2.0 set in src/arpifs/obs_preproc/nscatin.F90 but not used (I think)\nObsErr in Jo-table is RMS of all ASCAT obs_error values (SQRT(0.5*(u_err^2 + v_err^2)\nsigma_o can be set by Bator in NADIRS using NADIRS:\nECTERO(9,139,125,1) = 1.39_JPRB\nECTERO(9,139,124,1) = 1.54_JPRB","category":"page"},{"location":"System/DrHook/#Profiling-and-traceback-tool-Dr.Hook","page":"DrHook","title":"Profiling & traceback tool Dr.Hook","text":"","category":"section"},{"location":"System/DrHook/#Background","page":"DrHook","title":"Background","text":"","category":"section"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"Dr.Hook (& the Medicine Head :-) was developed at ECMWF in 2003 to overcome problems in catching runtime errors. Their IBM system at the time was quite impotent to produce meaningful traceback upon crash. It was decided that something need to be done urgently.","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"Dr.Hook gets its name from Fujitsu VPP's hook-functionality in their Fortran compiler, which enabled to call user functions upon enter and exit of a routine. Dr.Hook is of course a former US rock-band from 70's, which probably did not survive to this millenium due to heavy drug use!  ","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"In about week or so late 2003 the first version of Dr.Hook saw daylight. It turned out nearly immediately that we could try to gather information for profiling purposes, too, like wall & CPU clock times, possibly MFlop/s and memory consumption information.","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"One drawback with Dr.Hook was that (initially just) Fortran code needed to be instrumented by subroutine calls, which was a bother. However, for IFS code and automatique insertion script was developed greatly simplifiying the task.","category":"page"},{"location":"System/DrHook/#Activating-Dr.Hook","page":"DrHook","title":"Activating Dr.Hook","text":"","category":"section"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"Two things have to be in place in order to use Dr.Hook:","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"Fortran (or C) codes must contain explicit Dr.Hook calls to enable instrumentation, starting from the main program\nCertain environment variable(s) need to be set","category":"page"},{"location":"System/DrHook/#An-example-of-Fortran-instrumentation","page":"DrHook","title":"An example of Fortran instrumentation","text":"","category":"section"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"SUBROUTINE HOP(KDLEN,KDBDY,KSET,KHORIZ)\n!**** *HOP* - Operator routine for all types of observations.\n!     E. ANDERSSON            ECMWF          01/04/99\n...\nUSE PARKIND1  ,ONLY : JPIM     ,JPRB\nUSE YOMHOOK   ,ONLY : LHOOK,   DR_HOOK\n...\nIMPLICIT NONE\n...\nREAL(KIND=JPRB) :: ZHOOK_HANDLE ! Stack variable i.e. do not use SAVE\n...\n! Before the very first statement\nIF (LHOOK) CALL DR_HOOK('HOP',0,ZHOOK_HANDLE)\n...\n! Before any RETURN-clause\nIF (LLcondition) THEN\n  IF (LHOOK) CALL DR_HOOK('HOP',1,ZHOOK_HANDLE)\n  RETURN\nENDIF\n...\n! Before the very last statement\nIF (LHOOK) CALL DR_HOOK('HOP',1,ZHOOK_HANDLE)\nEND SUBROUTINE HOP\n","category":"page"},{"location":"System/DrHook/#Some-environment-variables","page":"DrHook","title":"Some environment variables","text":"","category":"section"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"To activate Dr.Hook and to enable tracebacks upon failure:","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"export DR_HOOK=1","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"This sets the Fortran-variable LHOOK to .TRUE..","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"By default all usual Unix-signals are caught (like SIGFPE=8, SIGSEGV=11, etc.). Occasionally, during development, some of them can be turned off, e.g. SIGFPE:","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"export DR_HOOK_IGNORE_SIGNALS=8","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"To enable low-overhead wall clock time profiling, set also:","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"export DR_HOOK_OPT=prof","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"Also recommended options are:","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"export DR_HOOK_SHOW_PROCESS_OPTIONS=0\nmkdir -p /some/path/hook\nexport DR_HOOK_PROFILE=/some/path/hook/drhook.prof.%d","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"The former one reduces Dr.Hook informative output upon initialization. This can be messy as so many processors are printing the same, often useless, output to the stderr.","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"The latter defines the profile files' location. The %d will be replaced with MPL-task id (= MPI-task plus 1).","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"Sometimes it is necessary to turn Dr.Hook off and also make sure no signals are caught by Dr.Hook – as this the (unfortunate?) default due to function call to C_DRHOOK_INIT_SIGNALS in arp/setup/sumpini.F. Now there is a new environment variable DR_HOOK_INIT_SIGNALS to prevent this. So, to make sure Dr.Hook does not interfere your run at all, give:","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"export DR_HOOK=0`\nexport DR_HOOK_INIT_SIGNALS=0","category":"page"},{"location":"System/DrHook/#Timeline-memory-profiling","page":"DrHook","title":"Timeline memory profiling","text":"","category":"section"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"It is possible to activate timeline profiling to see jumps in memory usage. Output is written to stdout. Controlling variables are:","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"export DR_HOOK=1\nexport DR_HOOK_TIMELINE=1\n#-- Optional:\nexport DR_HOOK_TIMELINE_FREQ=1 # the default = 1000000\nexport DR_HOOK_TIMELINE_MB=1 # th default jump 1 MByte","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"Upon each DR_HOOK_TIMELINE_FREQ-call to DR_HOOK this will check for one MByte (or DR_HOOK_TIMELINE_MB) jumps in resident memory usage, and will print a line containing cumulutive wall clock time since start, resident memory size right now, high water mark so far, routine name (instrumented to Dr.Hook).","category":"page"},{"location":"System/DrHook/#Implicit-MPL-library-(and-MPI)-initialization","page":"DrHook","title":"Implicit MPL-library (and MPI) initialization","text":"","category":"section"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"Be aware that the very first call to DR_HOOK also attempts to initialize MPL-library for you. Sometimes this is not desired or causes some hard to understand failures, especially with programs where MPI is not involved, but Dr.Hook calls are present. To turn this initialization off, set","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"export DR_HOOK_NOT_MPI=1","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"For example, asyncronous I/O module SAMIO does that – from within its Fortran. It calls before first Dr.Hook call function","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"CALL C_DRHOOK_NOT_MPI()","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"Thus, this can be used elsewhere, too (like in util/gl tools):","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"PROGRAM SOME_UTILGL_TOOL\n...\nCALL C_DRHOOK_NOT_MPI()\n!-- The following now does NOT initialize MPL nor MPI for you\nIF (LHOOK) CALL DR_HOOK('SOME_UTILGL_TOOL',0,ZHOOK_HANDLE)\n...\nIF (LHOOK) CALL DR_HOOK('SOME_UTILGL_TOOL',1,ZHOOK_HANDLE)","category":"page"},{"location":"System/DrHook/#Overheads","page":"DrHook","title":"Overheads","text":"","category":"section"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"The DR_HOOK=1 has practically no overhead on a scalar machine. Profiling with DR_HOOK_OPT=prof causes some 5% overhead.","category":"page"},{"location":"System/DrHook/","page":"DrHook","title":"DrHook","text":"On a vector machine overhead are so big that Dr.Hook should not be used there, unfortunately.","category":"page"},{"location":"DataAssimilation/DigitalFilterInitialization/#Digital-Filter-Initialization","page":"Digital Filter Initialization","title":"Digital Filter Initialization","text":"","category":"section"},{"location":"DataAssimilation/DigitalFilterInitialization/","page":"Digital Filter Initialization","title":"Digital Filter Initialization","text":"Digital Filter Initialization (DFI) is documented by Météo France here. This wiki page is based on the \"Version cycle 40t1\" document available on the gmapdoc web page. By default HARMONIE does not use DFI. ","category":"page"},{"location":"DataAssimilation/DigitalFilterInitialization/#DFI","page":"Digital Filter Initialization","title":"DFI","text":"","category":"section"},{"location":"DataAssimilation/DigitalFilterInitialization/","page":"Digital Filter Initialization","title":"Digital Filter Initialization","text":"The use (or not) of DFI is controlled by the variable DFI in ecf/config_exp.h. By default it is set to none. ","category":"page"},{"location":"DataAssimilation/DigitalFilterInitialization/","page":"Digital Filter Initialization","title":"Digital Filter Initialization","text":"idfi, incremental DFI\nfdfi, full DFI \nnone - no initialization (default)","category":"page"},{"location":"DataAssimilation/DigitalFilterInitialization/","page":"Digital Filter Initialization","title":"Digital Filter Initialization","text":"scr/Dfi is the script which calls the model in order to carry out DFI.","category":"page"},{"location":"DataAssimilation/DigitalFilterInitialization/#References","page":"Digital Filter Initialization","title":"References","text":"","category":"section"},{"location":"DataAssimilation/DigitalFilterInitialization/","page":"Digital Filter Initialization","title":"Digital Filter Initialization","text":"YESSAD K. (METEO-FRANCE/CNRM/GMAP/ALGO) July 7, 2015: DIGITAL FILTERING INITIALISATION IN THE CYCLE 42 OF ARPEGE/IFS\nYESSAD K. (METEO-FRANCE/CNRM/GMAP/ALGO) March 17, 2015: DIGITAL FILTERING INITIALISATION IN THE CYCLE 41T1 OF ARPEGE/IFS\nYESSAD K. (METEO-FRANCE/CNRM/GMAP/ALGO) August 6, 2014: DIGITAL FILTERING INITIALISATION IN THE CYCLE 41 OF ARPEGE/IFS\nYESSAD K. (METEO-FRANCE/CNRM/GMAP/ALGO) March 12, 2014: DIGITAL FILTERING INITIALISATION IN THE CYCLE 40T1 OF ARPEGE/IFS\nYESSAD K. (METEO-FRANCE/CNRM/GMAP/ALGO) July 3, 2013: DIGITAL FILTERING INITIALISATION IN THE CYCLE 40 OF ARPEGE/IFS","category":"page"},{"location":"ForecastModel/HR/#High-Resolution-Modelling","page":"High Res","title":"High Resolution Modelling","text":"","category":"section"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"This page gives some details and advice on settings for running HARMONIE-AROME at sub-kilometre resolutions.  A number of workshops have been held on this topic; more information and presentations may be found on hirlam.org.","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"Default configuration to be made available via scr/Harmonie_configurations.pm. Below are some details on the various options.","category":"page"},{"location":"ForecastModel/HR/#Practicalities","page":"High Res","title":"Practicalities","text":"","category":"section"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"Since experiments with higher resolutions may take a longer time than usual, it can be worthwhile to increase the PATIENCE beyond the default. For example, add the following to Env_submit","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"if ( $ENV{SIMULATION_TYPE} eq 'nwp' ) {\n $job_list{'Forecast'}{'ENV'}              = $submit_type.'-v PATIENCE=28800' ;\n}","category":"page"},{"location":"ForecastModel/HR/#Grid-choice","page":"High Res","title":"Grid choice","text":"","category":"section"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"Within ecf/config_exp.h, it is recommended to use quadratic or cubic spectral truncations:","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"GRID_TYPE=QUADRATIC           # Type of grid (LINEAR|QUADRATIC|CUBIC)","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"These choices are cheaper and more stable. No problematic reduction in accuracy has been reported, and quadratic grids are used operationally at a number of centres. ","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"For 750m resolution and a quadratic grid, TSTEP=30 should work well. For linear truncation, 20s will be needed. ","category":"page"},{"location":"ForecastModel/HR/#Boundaries","page":"High Res","title":"Boundaries","text":"","category":"section"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"Use of IFS boundaries has been found to be satisfactory. ","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"With higher sub-kilometric resolutions, smaller domains are inevitable. To deal with domain spin-up issues with precipitation, it may be worth coupling hydrometeors from the boundary files. Within scr/MARS_get_bd and scr/gl_bd there is an option USE_IFS_CLOUD_COND, which adds cloud and hydrometeors to the the boundary files. This can be added and exported in ecf/config_exp.h if needed. ","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"The coupling is then activated in nam/harmonie_namelists.pm under NAMGFL, e.g. for cloud ice:","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"  'YI_NL%NCOUPLING' => '1,',","category":"page"},{"location":"ForecastModel/HR/#Diffusion-and-controlling-noise","page":"High Res","title":"Diffusion and controlling noise","text":"","category":"section"},{"location":"ForecastModel/HR/#LGWADV","page":"High Res","title":"LGWADV","text":"","category":"section"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"By default, LGWADV=FALSE and LRDBBC=TRUE. These can be found in NAMDYNA.","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"It has been found that changing the values can remove noise visible in certain situations in MSLP fields; i.e. set LGWADV=TRUE and LRDBBC=FALSE. These are the values used at Météo France and ALADIN centres using the predictor-corrector time-stepping, rather than semi-implicit SETTLS (default in HARMONIE). ","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"'''Update''': new defaults in 43h2.1.1, LGWADV=TRUE and LRDBBC=FALSE","category":"page"},{"location":"ForecastModel/HR/#VESL","page":"High Res","title":"VESL","text":"","category":"section"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"Another option to control noise is the off-centering parameter VESL, found in NAMDYN, which is 0 by default in operational 2.5km configurations. A non-zero value such as 0.1 will smooth small-scale noise. ","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"For some (currently unknown) reason, the LGWADV option described above cannot be used with a non-zero value of VESL; forecasts crash quickly. Given the potential of VESL to stabilise higher-resolution runs, it is probably a preferable option, i.e.","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":" NAMDYN=>{\n  'VESL' => '0.1,',\n},\n NAMDYNA=>{\n  'LGWADV' => '.FALSE.,',\n  'LRDBBC' => '.TRUE.,',\n},","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"'''Update post-ASW2021''': this instability may in fact have been a bug, caused by the following in src/arpifs/adiab/lattex.F90:","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"!*       2.1   Momentum equation.\n\n\n! * LSETTLS is replaced by LLSETTLSW=.FALSE. for wind-eqn if VESL>0 because\n!   stable extrapolation deteriorates scores without improving stability.\n\nIF (PESGP > PESGM) THEN\n  LLSETTLSW=.FALSE.\nELSE\n  LLSETTLSW=LSETTLS\nENDIF","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"This is when VESL>0, SETTLS is switched off for wind equations, but no others. Seems to trigger the problem. Removing the if and keeping just ","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"LLSETTLSW=LSETTLS","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"in all cases allows VESL to run stably with LGWADV and SETTLS. (No long term verification scores tested)","category":"page"},{"location":"ForecastModel/HR/#Spectral-horizontal-diffusion","page":"High Res","title":"Spectral horizontal diffusion","text":"","category":"section"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"The strength of the spectral diffusion is controlled by the various RDAMP* parameters. Defaults in nam/harmonie_namelists.pm are:","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":" NAMDYN=>{\n  'RDAMPDIV' => '20.,',\n  'RDAMPPD' => '200000.,',\n  'RDAMPQ' => '20.,',\n  'RDAMPT' => '20.,',\n  'RDAMPVD' => '20.,',\n  'RDAMPVOR' => '20.,',\n},","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"With a quadratic or cubic grid with non-zero VESL, these defaults have been found to be adequate. Without VESL, higher levels of diffusion through lover RDAMP* values of 10 or even 1 are necessary.","category":"page"},{"location":"ForecastModel/HR/#SLHD","page":"High Res","title":"SLHD","text":"","category":"section"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"Experiments at Météo France suggest not to use SLHD on hydrometeors: c.f. ASM 2020 presentation by Yann Seity.","category":"page"},{"location":"ForecastModel/HR/#Other-recommended-settings","page":"High Res","title":"Other recommended settings","text":"","category":"section"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"In ecf/config_exp.h ","category":"page"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"LGRADSP=yes                             # Apply Wedi/Hortal vorticity dealiasing (yes|no)\nLUNBC=yes                               # Apply upper nested boundary condition (yes|no)","category":"page"},{"location":"ForecastModel/HR/#Sample-configurations","page":"High Res","title":"Sample configurations","text":"","category":"section"},{"location":"ForecastModel/HR/","page":"High Res","title":"High Res","text":"Coming soon...","category":"page"},{"location":"Observations/Conrad/#Processing-of-Radar-data-using-CONRAD","page":"Conrad","title":"Processing of Radar data using CONRAD","text":"","category":"section"},{"location":"Observations/Conrad/#Introduction","page":"Conrad","title":"Introduction","text":"","category":"section"},{"location":"Observations/Conrad/","page":"Conrad","title":"Conrad","text":"The need for radar data processing software, CONRAD:","category":"page"},{"location":"Observations/Conrad/","page":"Conrad","title":"Conrad","text":"Need for a way to put local radar data into Harmonie for 3DVAR assimilation\nMétéo-France have a working system\nMétéo-France BUFR  format can be read by BATOR","category":"page"},{"location":"Observations/Conrad/#NMS-details","page":"Conrad","title":"NMS details","text":"","category":"section"},{"location":"Observations/Conrad/#met.no","page":"Conrad","title":"met.no","text":"","category":"section"},{"location":"Observations/Conrad/","page":"Conrad","title":"Conrad","text":"Input Format Prorad XML\nConrad conversion executable proradpol2bufrpol.F90\nScanning strategy Different elevation angles for reflectivity and wind observations. Different resolutions (see data below)\nSensitivity and radar constant Varies between radars, and might vary in time. Not available directly in the local files for all radars (yet). Note that the definition of these variables also varies between version of the radar software.\nQC applied (Vr) No extra filtering done. De-aliasing done in the radar software (Gematronix, Rainbow). Quality seems to be quite good, but might have some problems at edges. The built in median filter and smoothing in Harmonie might be able to deal with this. Not investigated thoroughly yet.\nQC applied (dBz) Several QC algorithms applied: Sea clutter, Ground clutter, Beam blockage, Sun flare, Speckle (boats, birds, etc)","category":"page"},{"location":"Observations/Conrad/#met.ie","page":"Conrad","title":"met.ie","text":"","category":"section"},{"location":"Observations/Conrad/","page":"Conrad","title":"Conrad","text":"Met Éireann radar data is available in an OPERA BUFR format summarised in the table below:","category":"page"},{"location":"Observations/Conrad/","page":"Conrad","title":"Conrad","text":"Input Format OPERA BUFR\nConrad conversion executable decodemetieopera.F90\nScanning strategy Different elevation angles and resolutions for reflectivity and wind observations.\nSensitivity and radar constant Varies between radars, and might vary in time. Not available directly in the local files for all radars.Currently hard-coded in decodemetieopera.F90 .\nQC applied (Vr) Information to be added.\nQC applied (dBz) Information to be added.","category":"page"},{"location":"Observations/Conrad/","page":"Conrad","title":"Conrad","text":"Met Éireann radar data is now also available in an ODIM hdf5 format summarised in the table below:","category":"page"},{"location":"Observations/Conrad/","page":"Conrad","title":"Conrad","text":"Input Format ODIM hdf5\nConrad conversion executable Not yet tested\nScanning strategy Different elevation angles and resolutions for reflectivity and wind observations.\nSensitivity and radar constant Varies between radars, and might vary in time. Not available directly in the local files for all radars.\nQC applied (Vr) Information to be added.\nQC applied (dBz) beamb (built with BALTRAD) has been tested with GTOPO30 DEM","category":"page"},{"location":"Observations/Conrad/","page":"Conrad","title":"Conrad","text":"hdf5 data can be visualised with \nhdfview:  http://www.hdfgroup.org/hdf-java-html/hdfview to produce rays x bins type 2D plots\nwith python based wradlib software: http://wradlib.bitbucket.org to produce nice polar plots","category":"page"},{"location":"Observations/Conrad/#KNMI","page":"Conrad","title":"KNMI","text":"","category":"section"},{"location":"Observations/Conrad/","page":"Conrad","title":"Conrad","text":"Input Format KNMI hdf5\nConrad conversion executable read_knmiradar.F90\nScanning strategy The two Dutch radars (De Bilt and Den Helder) have the same scanning strategy for both reflectivities and radial winds. The number of scanning angles is identical for all elevations, but the number of range bins is different. In the reading routine, however, the largest number of bins is used for all elevations with the missing values replaced by zero.\nSensitivity and radar constant Sensitivity is hard coded in the reading routine and radar constant is read from the local data-files.\nQC applied (Vr) Information to be added.\nQC applied (dBz) Information to be added.","category":"page"},{"location":"Overview/Source/#Harmonie-Source-Code","page":"Source","title":"Harmonie Source Code","text":"","category":"section"},{"location":"Overview/Source/#Introduction","page":"Source","title":"Introduction","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"This wiki page summaries the ARPEGE/IFS source code made available in the HARMONIE system. It is based on documents made available by YESSAD K. (METEO-FRANCE/CNRM/GMAP/ALGO). The relevant document for cycle 40 is available here (or directly here). Documents for other versions are available in the \"References and documentation\" section. This page used the 40T1 document.","category":"page"},{"location":"Overview/Source/#HARMONIE-Source-Library-Structure","page":"Source","title":"HARMONIE Source Library Structure","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"The main source of HARMONIE system originates from IFS/ARPEGE and it consists of a number of \"project\" sources. These are:","category":"page"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"aeolus: Aeolous source code, a package for pre-processing satellite lidar wind data. Inactive for us.\naladin: specific routines only relevant to LAM, (limited area models, in particular ALADIN and AROME).\nalgor: application routines, e.g. to read LFI or Arpege files,interface routines for distributed memory environment, some linear algebra routines, such as lanczos algorithm, minimizers.\narpifs: global model routines (ARPEGE, IFS), and routines common to global and LAM models. This is the core of the ARPEGE/IFS software. The core of ARPEGE/IFS software.\nbiper: Biperiodization routines for the LAM\nblacklist: package for blacklisting\ncoupling: lateral coupling and spectral nudging for LAM models\netrans: spectral transforms for plane geometry, used for LAM\nifsaux: some application routines, for example reading or writing on “LFI” or ARPEGE files, interface routines for distributed memory environment\nmpa: upper air meso-NH/AROME physics (also used in ARPEGE/ALADIN)\nmse: surface processes in meso-NH/AROME (interface for SURFEX)\nodb: ODB (Observational Data Base software), needed by ARPEGE/ALADIN for their analysis or their assimilation cycle\nsatrad: satellite data handling package, needed to run the model analysis/assimilation\nsurf: ECMWF surface scheme\nsurfex: surface processes in meso-NH/AROME - the externalized surface scheme SURFEX\ntrans: spectral transforms for spherical geometry, used for ARPEGE/IFS\nutilities: utility packages, for operational FA to GRIB (PROGRID), OULAN, BATOR, or programs to operate on ODB and radiances bias correction","category":"page"},{"location":"Overview/Source/#Dependencies-and-hierarchy-between-each-project","page":"Source","title":"Dependencies and hierarchy between each project","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"Note: these project names are no longer valid – need to update","category":"page"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"ARP+TFL+XRD+XLA+MPA+MSE+SURFEX: for ARPEGE forecasts with METEO-FRANCE physics.\nARP+ALD+TFL+TAL+XRD+XLA+BIP+MPA+MSE+SURFEX: for ALADIN or AROME forecasts.\nARP+TFL+XRD+XLA+SUR: for IFS forecasts with ECMWF physics.\nARP+TFL+XRD+XLA+MPA+MSE+SURFEX+BLA+ODB+SAT+AEO: for ARPEGE assimilations with METEO-FRANCE physics.\nARP+ALD+TFL+TAL+XRD+XLA+BIP+MPA+MSE+SURFEX+BLA+ODB+SAT+AEO: for ALADIN or AROME assimilations.\nARP+TFL+XRD+XLA+SUR+BLA+ODB+SAT+OBT+SCR+AEO: for IFS assimilations with ECMWF physics.","category":"page"},{"location":"Overview/Source/#Libraries-under-each-project","page":"Source","title":"Libraries under each project","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"Note: this information made need to be updated for CY40","category":"page"},{"location":"Overview/Source/#ARPIFS","page":"Source","title":"ARPIFS","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"adiab\nAdiabatic dynamics\nAdiabatic diagnostics and intermediate quantities calculation, for example the geopotential height (routines GP... or GNH...).\nEulerian advections\nSemi-Lagrangian advection and interpolators (routines LA...)\nSemi-implicit scheme and linear terms calculation (routines SI..., SP..SI..)\nHorizontal diffusion (routines SP..HOR..)\nald inc\nfunction: functions used only in ALADIN\nnamelist: namelists read by ALADIN.\nc9xx: specific configurations 901 to 999 routines (mainly configuration 923). Routines INCLI.. are used in configuration 923. Routines INTER... are interpolators used in configurations 923, 931, 932.\ncanari: routines used in the CANARI optimal interpolation. Their names generally starts by CA.\ncanari common: empty directory to be deleted.\nclimate: some specific ARPEGE-CLIMAT routines.\ncommon: often contains includes\ncontrol: control routines. Contains in particular STEPO and CNT... routines.\ndfi: routines used in the DFI (digital filter initialisation) algorithm\ndia: diagnostics other than FULL-POS. One finds some setup SU... routines specific to some diagnostics and some WR... routines doing file writing.\nfunction: functions (in includes). The qa....h functions are used in CANARI, the fc....h functions are used in a large panel of topics.\ninterface: not automatic interfaces (currently empty).\nkalman: Kalman filter.\nmodule: all the types of module (variables declarations, type definition, active code).\nmwave: micro-wave observations (SSM/I) treatment.* namelist: all namelists.\nnmi: routines used in the NMI (normal mode initialisation) algorithm.\nobs error: treatment of the observation errors in the assimilation.\nobs preproc: observation pre-processing (some of them are called in the screening).\nocean: oceanic coupling, for climatic applications.\nonedvar: 1D-VAR assimilation scheme used at ECMWF.\nparallel: parallel environment, communications between processors.\nparameter: empty directory to be deleted.\nphys dmn: physics parameterizations used at METEO-FRANCE, and HIRLAM physics, ALARO physics.\nphys ec: ECMWF physics. Some of these routines (FMR radiation scheme, Lopez convection scheme) are now also used in the METEO-FRANCE physics.\npointer: empty directory to be deleted.\npp obs: several applications\nobservation horizontal and vertical interpolator. \nFULL-POS. \nvertical interpolator common to FULL-POS and the observation interpolator; some of these routines may be used elsewhere.\nsetup: setup routines not linked with a very specific domain. More specific setup routines are spread among some other subdirectories.\nsinvect: singular vectors calculation (configuration 601).\nsupport: empty directory to be deleted.\ntransform: hat routines for spectral transforms.\nutility: miscellaneous utilitaries, linear algebra routines, array deallocation routines.\nvar: routines involved in the 3DVAR and 4DVAR assimilation, some minimizers (N1CG1, CONGRAD), some specific 3DVAR and 4DVAR setup routines.\nwave: empty directory to be deleted.","category":"page"},{"location":"Overview/Source/#ALADIN","page":"Source","title":"ALADIN","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"adiab: adiabatic dynamics.\nblending: blending scheme (currently only contains the procedure blend.ksh).\nc9xx: specific configurations E901 to E999 routines (mainly configuration E923). Routines EINCLI.. are used in configuration E923. Routines EINTER... are interpolators used in configurations E923, E931, E932.\ncontrol: control routines.\ncoupling: lateral coupling by external lateral boundary conditions.\ndia: diagnostics other than FULL-POS.\ninidata: setup routines specific to file reading (initial conditions, LBC).\nmodule: active code modules only used in ALADIN.\nobs preproc: observation pre-processing (some of them are called in the screening).\nparallel: parallel environment, communications between processors.\npp obs: several applications:\nobservation horizontal and vertical interpolator. \nFULL-POS.\nvertical interpolator common to FULL-POS and the observation interpolator; some of these routines may be used elsewhere.\nprograms: probably designed to contain procedures, but currently contains among others some blending routines, the place of which would be probably better in subdirectory \"blending\".\nsetup: setup routines not linked with a very specific domain. More specific setup routines are spread among some other subdirectories.\nsinvect: singular vectors calculation (configuration E601).\ntransform: hat routines for spectral transforms.\nutility: miscellaneous utilitaries, array deallocation routines.\nvar: routines involved in the 3DVAR and 4DVAR assimilation, some specific 3DVAR and 4DVAR setup routines.","category":"page"},{"location":"Overview/Source/#TFL","page":"Source","title":"TFL","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"build: contains procedures.\nexternal: routines which can be called from another project.\ninterface: not automatically generated interfaces which match with the \"external\" directory routines.\nmodule: all the types of module (variables declarations, type definition, active code).\ntpm ...F90: variable declaration + type definition modules. \nlt.... mod.F90: active code modules for Legendre transforms. \nft.... mod.F90: active code modules for Fourier transforms. \ntr.... mod.F90: active code modules for transpositions. \nsu.... mod.F90: active code modules for setup.\nprograms: specific entries which can be used for TFL code validation. These routines are not called elsewhere.   ","category":"page"},{"location":"Overview/Source/#TAL","page":"Source","title":"TAL","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"external: routines which can be called from another project.\ninterface: not automatically generated interfaces which match with the \"external\" directory routines.\nmodule: all the types of module (variables declarations, type definition, active code). \ntpmald ...F90: variable declaration + type definition modules. \nelt.... mod.F90: active code modules for N-S Fourier transforms. \neft.... mod.F90: active code modules for E-W Fourier transforms. \nsue.... mod.F90: active code modules for setup. \nprograms: specific entries which can be used for TAL code validation. These routines are not called elsewhere.                                                             ","category":"page"},{"location":"Overview/Source/#XRD","page":"Source","title":"XRD","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"arpege: empty directory to be deleted.\nbufr io: BUFR format files reading and writing.\ncma: CMA format files reading and writing.\nddh: DDH diagnostics.\nfa: ARPEGE (FA) files reading and writing.\ngrib io: ECMWF GRIB format files reading and writing.\ngrib mf: METEO-FRANCE GRIB format files reading and writing.\nioassign: empty directory to be deleted.\nlanczos: linear algebra routines for Lanczos algorithm.\nlfi: LFI format files reading and writing.\nminim: linear algebra routines for minimizations. Contains the M1QN3 (quasi-Newton) minimizer.\nmisc: miscellaneous decks.* module: all the types of module (variables declarations, type definition, active code). There are a lot of mpl...F90 modules for parallel environment (interface to MPI parallel environment).\nmrfstools: empty directory to be deleted.\nnewbufrio: empty directory to be deleted.\nnewcmaio: empty directory to be deleted.\nnot used: miscellaneous decks (unused decks to be deleted?).\npcma: empty directory to be deleted.\nsupport: miscellaneous routines. Some of them do Fourier transforms, some others do linear algebra.\nsvipc: contains only svipc.c .\nutilities: miscellaneous utilitaries.","category":"page"},{"location":"Overview/Source/#SUR","page":"Source","title":"SUR","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"build: contains procedures.\nexternal: routines which can be called from another project.* function: specific functions.\ninterface: not automatically generated interfaces which match with the \"external\" directory routines.\nmodule: all the types of module (variables declarations, type definition, active code).\nyos ...F90: variable declaration + type definition modules. \nsu.... mod.F90 but not surf.... mod.F90: active code modules for setup. \nsurf.... mod.F90, v.... mod.F90: other active code modules.\noffline: specific entries which can be used for SUR code validation. These routines are not called elsewhere.","category":"page"},{"location":"Overview/Source/#BLA","page":"Source","title":"BLA","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"compiler.\ninclude: not automatically generated interfaces, functions, and some other includes.\nlibrary: the only containing .F90 decks.\nold2new.\nscripts.","category":"page"},{"location":"Overview/Source/#SAT","page":"Source","title":"SAT","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"bias.\nemiss.\ninterface.\nmodule.\nmwave.\nonedvar.\npre screen.\nrtlimb.\nrttov.\nsatim.\ntest. (Not described in detail; more information has to be provided by someone who knows the content of this project, but there is currently no specific documentation about this topic)","category":"page"},{"location":"Overview/Source/#UTI","page":"Source","title":"UTI","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"add cloud fields: program to add 4 cloud variables (liquid water, ice, rainfall, snow) in ARPEGE files.\nbator: BATOR software (reads observations data in a ASCII format file named OBSOUL and the blacklist, writes them on a ODB format file with some additional information).\ncombi: combination of perturbations in an ensemble forecast (PEARP).\ncontrodb: control of the number of observations.\nextrtovs: unbias TOVS.\nfcq: does quality control and writes this quality control in ODB files.\ngobptout: PROGRIB? (convert ARPEGE files contained post-processed data into GRIB files).\ninclude: all .h decks (functions, COMMON blocks, parameters).\nmandalay: software MANDALAY.\nmodule: all types of modules.\nnamelist: namelists specific to the applications stored in UTI (for example OULAN, BATOR).\noulan: OULAN software (the step just before BATOR: observation extractions in the BDM, samples data in space and time, and writes the sampled data in an ASCII file called \"OBSOUL\").\npregpssol: Surface GPS processing.\nprescat: Scatterometer data processing.\nprogrid: PROGRID? (convert ARPEGE files contained post-processed data into GRIB files).\nprogrid cadre: cf. progrid?\nsst nesdis: program to read the SST on the BDAP. This project has its own entries.","category":"page"},{"location":"Overview/Source/#MPA","page":"Source","title":"MPA","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"It contains first layer of directory","category":"page"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"chem: chemistry.\nconv: convection.\nmicro: microphysics.\nturb: turbulence.","category":"page"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"Each directory contains the following subdirectories","category":"page"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"externals: routines which can be called from another project.\ninclude: all the \"include\" decks (functions, COMMON blocks, parameters).\ninterface: not automatically generated interfaces which match with the \"external\" directory routines.\ninternals: other non-module routines; they cannot be called from another project.\nmodule: all types of modules.","category":"page"},{"location":"Overview/Source/#SURFEX","page":"Source","title":"SURFEX","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"ASSIM: Surface assimilation routines (please note that programs soda.F90, oi_main.F90 and varassim.F90 are located under mse/programs).\nOFFLIN: Surface offline routines (please note that programs pgd.F90, prep.F90 and offline.F90 are located under mse/programs).\nSURFEX: Surface routines for physiography (PGD), initialisation (PREP) and physical processes including e.g. land (ISBA), sea, town (TEB) and lakes.\nTOPD: TOPMODEL (TOPography based MODEL) for soil hydrology.\nTRIP: River routing model TRIP","category":"page"},{"location":"Overview/Source/#MSE","page":"Source","title":"MSE","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"dummy: empty versions of some routines.\nexternals: routines which can be called from another project.\ninterface: not automatically generated interfaces which match with the \"external\" directory routines.\ninternals: other non-module routines; they cannot be called from another project.\nmodule: all types of modules.\nnew: file conversion routines, e.g. fa2lfi, lfi2fa\nprograms: SURFEX programs","category":"page"},{"location":"Overview/Source/#References-and-documentation","page":"Source","title":"References and documentation","text":"","category":"section"},{"location":"Overview/Source/","page":"Source","title":"Source","text":"Yessad K.: Jul 2015: LIBRARY ARCHITECTURE AND HISTORY OF THE TECHNICAL ASPECTS IN ARPEGE/IFS, ALADIN AND AROME IN THE CYCLE 42 OF ARPEGE/IFS\nYessad K.: Mar 2014: LIBRARY ARCHITECTURE AND HISTORY OF THE TECHNICAL ASPECTS IN ARPEGE/IFS, ALADIN AND AROME IN THE CYCLE 40T1 OF ARPEGE/IFS\nYessad K.: Jul 2013: LIBRARY ARCHITECTURE AND HISTORY OF THE TECHNICAL ASPECTS IN ARPEGE/IFS, ALADIN AND AROME IN THE CYCLE 40 OF ARPEGE/IFS\nYessad K.: Nov 2011: BASICS ABOUT ARPEGE/IFS, ALADIN AND AROME IN THE CYCLE 38 OF ARPEGE/IFS\nYessad K.: Nov 2011: LIBRARY ARCHITECTURE AND HISTORY OF THE TECHNICAL ASPECTS IN ARPEGE/IFS, ALADIN AND AROME IN THE CYCLE 38 OF ARPEGE/IFS","category":"page"},{"location":"EPS/System/#Ensemble-mode-in-the-Harmonie-script-system","page":"System","title":"Ensemble mode in the Harmonie script system","text":"","category":"section"},{"location":"EPS/System/#Overview","page":"System","title":"Overview","text":"","category":"section"},{"location":"EPS/System/","page":"System","title":"System","text":"Purpose\nPrerequisites\nOption checking\nEPS in the tdf file","category":"page"},{"location":"EPS/System/#Purpose","page":"System","title":"Purpose","text":"","category":"section"},{"location":"EPS/System/","page":"System","title":"System","text":"The purpose of this document is to give more details about how ensemble mode works in the Harmonie script system than can easily be found in other pages.  It is meant for system people and developers who need to understand or extend the functionality of HarmonEPS.  Such extensions could be e.g. implementation of new initial perturbation techniques.","category":"page"},{"location":"EPS/System/#Prerequisites","page":"System","title":"Prerequisites","text":"","category":"section"},{"location":"EPS/System/","page":"System","title":"System","text":"Before reading further you should have a basic understanding of mSMS.  You should also read the Howto to get acquainted with what is already implemented.","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"Having read the prerequisite pages you know that an ensemble experiment is not very different from a deterministic one, you only need to set a few ensemble related variables ENSMSEL, ENSINIPERT, etc. in ecf/config_exp.h and then make some member specific exceptions in the perl \"module\" msms/harmonie.pm. But there is more going on behind the scenes.","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"First of all, the ENSMSEL member selection variable exists for convenience, what is used in msms/harmonie.tdf and other scripts is an expanded version of it called ENSMSELX. In the script scr/Start this expansion is done by invoking script scr/Ens_util.pl, which is also used to set a couple of other convenience variables:","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"# Compute derived EPS quantities, needed in harmonie.tdf\nexport ENSSIZE ENSMFIRST ENSMLAST\nENSSIZE=`perl -S Ens_util.pl ENSSIZE`\nENSMFIRST=`perl -S Ens_util.pl ENSMFIRST`\nENSMLAST=`perl -S Ens_util.pl ENSMLAST`\nENSCTL=`perl -S Ens_util.pl ENSCTL $ENSCTL`","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"For example, if ENSMSEL=0-8:2, then ENSMSELX=000:002:004:006:008, i.e., a colon-separated list of 3-digit numbers.  In the same example, we will have ENSMFIRST=000 and ENSMLAST=008. ENSSIZE will be 5.","category":"page"},{"location":"EPS/System/#Option-checking","page":"System","title":"Option checking","text":"","category":"section"},{"location":"EPS/System/","page":"System","title":"System","text":"As explained in the prerequisite documents, variables normally set in ecf/config_exp.h  can be overridden for specific ensemble members in msms/harmonie.pm. But how is it verified that the chosen combinations make sense for each member?","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"The main script that checks for a sensible combination of options in Harmonie is scr/CheckOptions.pl. This script runs already from the Start script before mini-SMS is launched in order to catch problems as early as possible. CheckOptions.pl reads ecf/config_exp.h  and creates a new file sms/config_updated.h (but under \"merged\" repository directory $HM_LIB rather than your experiment directory $HM_WD). In config_updated.h you will find some environment variables that are derived from others, e.g., a lot of domain specific variables (NLON,NLAT,TSTEP etc.) are derived from $DOMAIN. Every SMS task in the system includes first config_exp.h and then config_updated.h. At the very bottom of config_updated.h we find:","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"if [ ${ENSMBR--1} -ge 0]; then\n  if [ -s $HM_LIB/sms/config_mbr$ENSMBR.h]; then\n. $HM_LIB/sms/config_mbr$ENSMBR.h\n  fi\nfi","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"That is, in ensemble mode, if sms/config_mbr$ENSMBR.h exists, it will also be sourced by every script. And finally, the way these member specific config files are created can be seen from this passage in the Start script:","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"# Check options for individual ensemble members if relevant\nif [ $ENSSIZE -gt 0]; then\n   perl -S CheckMemberOptions.pl || exit\nfi","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"scr/CheckMemberOptions.pl includes the member specific harmonie.pm of course, and then it loops over all the selected members in turn, running CheckOptions.pl with the particular environment settings for the member in question. If a member ($ENSMBR) passes the tests, the file mentioned above, $HM_LIB/sms/config_mbr$ENSMBR.h is created. It will contain settings for those environment variables mentioned in harmonie.pm that differs from the default settings in config_exp.h. This makes the correct variables available to every script, without those scripts having to repeat the perl &Env checking in harmonie.pm.","category":"page"},{"location":"EPS/System/#EPS-in-the-tdf-file","page":"System","title":"EPS in the tdf file","text":"","category":"section"},{"location":"EPS/System/","page":"System","title":"System","text":"In harmonie.tdf we have many loop constructs like the following example from the MakeCycleInput family:","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"      family Cycle\n         task Prepare_cycle\nloop(EEE,$ENV{ENSMFIRST},$ENV{ENSMLAST})\n  if( $ENV{ENSMSELX} =~ /@EEE@\\b/ and '@EEE@' ne '-1' )\n         family Mbr@EEE@\n            trigger ( Prepare_cycle == complete )\n            complete ( (../../Hour:HH + 24 - $ENV{BeginHour}) % &Env('FCINT','@EEE@') )\n            edit ENSMBR @EEE@\n            task Prepare_cycle\n         endfamily\n  endif\nendloop\n...","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"As can be seen, all the loops over ensemble members go from the smallest number found in ENSMSEL (ENSMFIRST) to the highest number found (ENSMLAST), with steps of 1, but only if the actual number (@EEE@) is present in the expanded list ENSMSELX is anything put to harmonie.def for this potential member. The perl operator =~ is the pattern match operator and \\b means a word boundary (: or end of string in our case).","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"Note also how every member has its own family Mbr@EEE@ (which will expand to Mbr000, Mbr002, etc. in the harmonie.def file).  Another important thing to note is the setting","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"edit ENSMBR @EEE@","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"This will first create an SMS variable %ENSMBR%* with different values for each member, which is also turned into a shell variable $ENSMBR in sms/sms.h, which is included by all SMS tasks. From sms.h:","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"ENSMBR=%ENSMBR%\nif [ ${ENSMBR--1} -ge 0]; then\n  ENSMBR=`echo %ENSMBR% | awk '{printf \"%%3.3d\",$1}'`\n  CYCLEDIR=%YMD%_${HH}/mbr$ENSMBR\nfi\nexport ENSMBR","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"The end message of this is that to get ensemble member number in a script you should use $ENSMBR.  In non-ensemble runs ENSMBR will have the value -1.","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"The statement","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"complete ( (../../Hour:HH + 24 - $ENV{BeginHour}) % &Env('FCINT','@EEE@') )","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"deserves more explanation. It is present to account for the fact that not all members need to have the same \"forecast interval\" FCINT. The Hour families in the tdf now look like this:","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"      family Hour\n         repeat integer HH &Env('FirstHour','min') 23 &Env('FCINT','min')","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"i.e., the loop steps with the minimum FCINT value found among the members.  Thus, if e.g. some members have FCINT=6 and some FCINT=12, then the statement above sets the family immediately complete for members with FCINT=12 at those cycles that are not divisible by 12 relative to the first cycle (BeginHour). I.e., if the run was started at a 06 or 18 cycle, members with FCINT=12 will be complete (not run) at 00 and 12 cycles, but if the run was started at a 00 or 12 cycle, then members with FCINT=12 will not run at 06 and 18 cycles.  This behaviour has confused many users and should perhaps be changed.","category":"page"},{"location":"EPS/System/#Make-member-specific-namelist-changes","page":"System","title":"Make member specific namelist changes","text":"","category":"section"},{"location":"EPS/System/","page":"System","title":"System","text":"In Harmonie most namelists are created on the fly from the namelist dictionary. This allows us to make member specific changes to the namelists used in e.g. the forecast. In the following we will describe two ways of doing this.  ","category":"page"},{"location":"EPS/System/#Through-harmonie.pm","page":"System","title":"Through harmonie.pm","text":"","category":"section"},{"location":"EPS/System/","page":"System","title":"System","text":"Assume we would like to change on parameter in the physcis. First we change the variable in the namelist to be dependent of an environment variable in nam/harmonie_namelists.pm","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":" NAMPHY0=>{\n  'ALMAV' => \"$ENV{ALMAV}\",\n ...\n },","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"Second we make sure in msms/harmonie.pm that this environment variable is specified for each member, in this case four.","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"   'ALMAV'  => [ '200.','100.','50.','300.'],","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"Finally we have to make sure that the variable is exported in ecf/config_exp.h","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":" export ALMAV","category":"page"},{"location":"EPS/System/#Make-changes-to-the-namelist-generation","page":"System","title":"Make changes to the namelist generation","text":"","category":"section"},{"location":"EPS/System/","page":"System","title":"System","text":"Another way is to specify a set of namelist changes for each member in nam/harmonie_namelists.pm. We could simply add a definition for e.g. the first member like","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":" %member_001 = (\n  NAERAD=>{\n   'LRRTM' => '.FALSE.,',\n  },\n  NAMPHY0=>{\n   'BEDIFV' => '0.05,',\n  },\n ), ","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"To activate the change we also need to change scr/Get_namelist, the script that builds the namelist for us to take the member_$ENSMBR change into account.","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":" ...\n forecast|dfi|traj4d)\n    NAMELIST_CONFIG=\"$DEFAULT dynamics $DYNAMICS $PHYSICS ${DYNAMICS}_${PHYSICS} $SURFACE $EXTRA_FORECAST_OPTIONS member_$ENSMBR\"\n ...","category":"page"},{"location":"EPS/System/","page":"System","title":"System","text":"Repeat this for all your members with the changes you would like to apply.","category":"page"},{"location":"System/HarmonieTestbed/#The-HARMONIE-testbed","page":"Testbed","title":"The HARMONIE testbed","text":"","category":"section"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"The HARMONIE testbed provides a facility to run a number of well defined test cases using the existing script environment in HARMONIE. The ALADIN testbed, Mitraillette runs test on the hart of the model, the dynamical core. The HARMONIE testbed tests the full script system as it is supposed to be used.","category":"page"},{"location":"System/HarmonieTestbed/#Defining-the-configurations","page":"Testbed","title":"Defining the configurations","text":"","category":"section"},{"location":"System/HarmonieTestbed/#General","page":"Testbed","title":"General","text":"","category":"section"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"The testbed is a suite that launches and follows new experiments one at a time in a controlled environment. The testbed experiment takes care of compilation and also hosts the climate files generated by the tested configurations. Source and scripts changes shall be done in the testbed experiment and will be synchronized to the child experiment using the hm_CMODS option in HARMONIE. ","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"A number of basic configurations have been defined in scr/Harmonie_configurations.pm as the deviation from the default setup in  ecf/config_exp.h, scr/include.ass and suites/harmonie.pm. These configurations are controlled by the script  scr/Harmonie_testbed.pl. The script also contains a number of extra configurations tested from time to time. With the current settings, a test of AROME without 3DVAR would look like.","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"\n    # AROME no 3D-VAR but default blending of upper air from boundaries\n    'AROME' => {\n      'description' => 'Standard AROME settings without upper air DA',\n      'PHYSICS'     => 'arome',\n      'SURFACE'     => 'surfex',\n      'DYNAMICS'    => 'nh',\n      'ANAATMO'     => 'blending',\n      'ANASURF'     => 'none',\n      'DFI'         => 'none',\n      'HOST_MODEL'  => 'ifs',\n      'DOMAIN'      => 'DKCOEXP',\n      'VLEV'        => '65',\n      'BDINT'       => '3',\n    },\n","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"The resulting output, in this case from AROMEBDARO running at ECMWF would look like:","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"\n Using the configuration AROME_BD_ARO\n\n\n Input /gpfs/scratch/ms/spsehlam/hlam/test_harmonie/ecgb_c2a_testbed_trunk_mkp_12413/ecf/config_exp.h  \n Output ecf/config_exp.h  \n\n Change BUILD=${BUILD-yes}                     to BUILD=no \n Change       BUILD_ROOTPACK=no to BUILD_ROOTPACK=no \n Change BINDIR=${BINDIR-$HM_DATA/bin}                 to BINDIR=$HM_COMDAT/ecgb_c2a_testbed_trunk_mkp_12413/bin \n Change DOMAIN=DKCOEXP                          to DOMAIN=TEST_8 \n Change VLEV=65                                 to VLEV=HIRLAM_60 \n Change LL=${LL-06}                             to LL=6 \n Change DYNAMICS=\"nh\"                           to DYNAMICS=nh \n Change PHYSICS=\"arome\"                         to PHYSICS=arome \n Change SURFACE=\"surfex\"                        to SURFACE=surfex \n Change DFI=\"none\"                              to DFI=none \n Change ANAATMO=3DVAR                           to ANAATMO=none \n Change ANASURF=CANARI_OI_MAIN                  to ANASURF=none \n Change OBDIR=$HM_DATA/observations             to OBDIR=$HM_DATA/../ecgb_c2a_testbed_trunk_mkp_12413/observations/ \n Change HOST_MODEL=\"ifs\"                        to HOST_MODEL=aro \n Change HOST_SURFEX=\"no\"                        to HOST_SURFEX=yes \n Change SURFEX_INPUT_FORMAT=lfi                 to SURFEX_INPUT_FORMAT=fa \n Change BDLIB=ECMWF                             to BDLIB=AROME \n Change BDDIR=$HM_DATA/${BDLIB}/archive/@YYYY@/@MM@/@DD@/@HH@   to BDDIR=$HM_DATA/../ecgb_c2a_testbed_trunk_mkp_12413/archive_AROME/@YYYY@/@MM@/@DD@/@HH@/ \n Change INT_BDFILE=$WRK/ELSCF${CNMEXP}ALBC@NNN@                 to INT_BDFILE=$ARCHIVE_ROOT/$YY/$MM/$DD/$HH/ELSCF${CNMEXP}ALBC@NNN@ \n Change BDSTRATEGY=simulate_operational to BDSTRATEGY=same_forecast \n Change BDINT=1                         to BDINT=3 \n Change SURFEX_PREP=\"no\"                to SURFEX_PREP=yes \n Change CLIMDIR=$HM_DATA/climate                to CLIMDIR=$HM_DATA/../ecgb_c2a_testbed_trunk_mkp_12413/climate/$DOMAIN/$PHYSICS \n Change BDCLIM=$HM_DATA/${BDLIB}/climate        to BDCLIM=$HM_DATA/../ecgb_c2a_testbed_trunk_mkp_12413/climate/TEST_11/arome/ \n Change INT_SINI_FILE=$WRK/SURFXINI.$SURFEX_OUTPUT_FORMAT       to INT_SINI_FILE=$ARCHIVE_ROOT/$YY/$MM/$DD/$HH/SURFXINI.$SURFEX_OUTPUT_FORMAT \n Change ARCHIVE_ECMWF=yes                       to ARCHIVE_ECMWF=no \n Change POSTP=\"inline\"                          to POSTP=inline \n Change MAKEGRIB=no                             to MAKEGRIB=yes \n Add new settings JBDIR=$HM_REV/testbed_data/jb_data \n Add new settings LARGE_EC_BD=no \n Add new settings PLAYFILE=harmonie \n Add new settings config=AROME \n\n\n Input /gpfs/scratch/ms/spsehlam/hlam/test_harmonie/ecgb_c2a_testbed_trunk_mkp_12413/Env_submit \n Output Env_submit \n\n Change $nprocx=16; to $nprocx=2 \n Change $nprocy=16; to $nprocy=2 \n Change $nprocx=8; to $nprocx=2 \n Change $nprocy=16; to $nprocy=2 \n","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"As seen from the example above the script also changes the submission rules. These rules can be defined, per host, at the end of the script. Other host specific settings may also be defined to allow local changes of the test environment. In Harmonie_testbed.pl we find e.g. changes for ecgate:","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"\n 'ecgate' => {\n   'BINDIR'     => '$HM_COMDAT/'.$EXP.'/bin',\n   'BDDIR'      => '$HM_DATA/../'.$EXP.'/$BDLIB/$DOMAIN',\n   'OBDIR'      => '$HM_DATA/../'.$EXP.'/observations/$DOMAIN',\n   'LARGE_EC_BD' => 'no',\n   'CLIMDIR'    => '$HM_DATA/../'.$EXP.'/climate/$DOMAIN/$PHYSICS',\n  },\n","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"The host dependent settings will be imposed on all configurations. If a setting in any configuration is in conflict with the host settings the configuration settings will be used.","category":"page"},{"location":"System/HarmonieTestbed/#Define-changes-in-harmonie.pm","page":"Testbed","title":"Define changes in harmonie.pm","text":"","category":"section"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"The changes to msms/harmonie.pm are controlled with a special syntax, like in the AROME_JB configuration.","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"   # AROME Structure function derivation\n   'AROME_JB' => {\n     'description' => 'Derive structure functions for AROME 3DVAR',\n      ...\n      'harmonie.pm' => ['ENSBDMBR','ENSCTL','SLAFLAG'],\n        'ENSBDMBR'    => '[1,2,3,4]',\n        'ENSCTL'      => '[\"001\",\"002\",\"003\",\"004\"]',\n        'SLAFLAG'     => '[0]',\n    },\n","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"The harmonie.pm key determines which keyword to find and replace. The list guarantees that the same keywords are not changed in e.g. ecf/config_exp.h .","category":"page"},{"location":"System/HarmonieTestbed/#Testbed-members","page":"Testbed","title":"Testbed members","text":"","category":"section"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"Name DOMAIN DTGs Dependencies Description Active in\nAROME TEST_11 2017093018-2017100100 None AROME with 2-D decomposition CY43\nAROME_1D TEST_11 2017093018-2017100100 None AROME with 1-D decomposition CY43\nAROME_2D TEST_11 2017093018-2017100100 None AROME with 2-D decomposition \nAROME_3DVAR IRELAND150 2017093018-2017100100 None AROME_3DVAR CY43\nAROME_3DVAR_MARSOBS IRELAND150 2017093018-2017100100 None AROME_3DVAR including non-conventional observations from MARS CY43\nAROME_3DVAR_2P TEST_11 2017093018-2017100100 None AROME_3DVAR with two patches \nAROME_4DVAR SCANDINAVIA 2017093021-2017100100 None AROME_4DVAR \nAROME_BD_ALA TEST_8 2017093018-2017100100 ALARO AROME with ALARO LBCs \nAROME_BD_ALA_ARO TEST_2.5 2017093018-2017100100 AROME_BD_ALA AROME with AROME LBCs \nAROME_BD_ARO TEST_8 2017093018-2017100100 AROME AROME with AROME LBCs, no IO-server CY43\nAROME_BD_ARO_IO_SERV TEST_8 2017093018-2017100100 AROME AROME with AROME LBCs, with IO-server CY43\nAROME_BD_ARO_2P TEST_8 2017093018-2017100100 AROME AROME two patches with AROME LBCs \nAROME_CLIMSIM TEST_11 2012053100-2012060200 None AROME climate simulation, netcdf output CY43\nAROME_EKF TEST_11 2017093018-2017100100 None AROME with CANARI_EKF_SURFEX \nAROME_EPS_COMP TEST_11 2017093018-2017100100 HarmonEPS AROME_3DVAR comparison of EPS control CY43\nAROME_MUSC TEST_11 2017093018-2017100100 AROME AROME MUSC CY43\nAROME_NONE TEST_11 2017093018-2017100100 None AROME no SFC/UA DA \nAROME_NONE_2D TEST_11 2017093018-2017100100 None AROME no SFC/UA DA \nAROME_NONE_BD_ALA_NONE TEST_8 2017093018-2017100100 ALARO_NONE AROME no SFC/UA DA with ALARO LBCs \nAROME_NONE_BD_ARO_NONE TEST_8 2017093018-2017100100 AROME_NONE AROME no SFC/UA DA with AROME LBCs \nARONE_JB TEST_11 2017093018-2017100100 None Generation of JB statistics CY43\nHarmonEPS TEST_11 2017093018-2017100100 AROME_EPS_COMP HarmonEPS CY43\nHarmonEPS_IFSENSBD TEST_11 2019111021-2019111103 AROME_EPS_COMP HarmonEPS with IFSENS boundaries CY43\nALARO1_3DVAR_OLD TEST_11 2017093018-2017100100 None ALARO1 with 3DVAR and old_surface \nALARO_1D TEST_11 2017093018-2017100100 None ALARO with 1-D decomposition \nALARO_2D TEST_11 2017093018-2017100100 None ALARO with 2-D decomposition \nALARO_3DVAR_OLD TEST_11 2017093018-2017100100 None ALARO3DVAR with oldsurface \nALARO_EKF TEST_11 2017093018-2017100100 None ALARO with CANARI_EKF_SURFEX \nALARO_EPS_COMP TEST_11 2017093018-2017100100 ??? ALARO EPS? \nALARO_MF_60 TEST_11 2017093018-2017100100 None ALARO with VLEV=MF_60 \nALARO_MUSC TEST_11 2017093018-2017100100 ALARO ALARO MUSC \nALARO_NH_1D TEST_11 2017093018-2017100100 None ALARO with NH dynamics and 1-D decomposition \nALARO_NH_2D TEST_11 2017093018-2017100100 None ALARO with NH dynamics and 2-D decomposition \nALARO_NONE TEST_11 2017093018-2017100100 None ALARO with no SFC/UA DA \nALARO_OLD TEST_11 2017093018-2017100100 None ALARO with old_surface \nALARO_OLD_MUSC TEST_11 2017093018-2017100100 ALARO ALARO MUSC with old_surface ","category":"page"},{"location":"System/HarmonieTestbed/#Testbed-domains","page":"Testbed","title":"Testbed domains","text":"","category":"section"},{"location":"System/HarmonieTestbed/#The-playfile","page":"Testbed","title":"The playfile","text":"","category":"section"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"The playfile used for the testbed is  msms/testbed.tdf. Here each configuration is defined with a trigger, a task to create and one to follow the child experiments.  Configurations inluded are listed in the TESTBED_LIST environment variable in ecf/config_exp.h","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":" TESTBED_LIST=\"ALADIN ALADIN_3DVAR AROME\"","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"If the child experiment fails the Followexp task will also fail. When the child experiment problem has been corrected and the task restarted, the follow task should be restarted. When, finally, the child experiment is finished the test family will be completed and next test case will be triggered. We may choose to let the testbed launch a new experiment if the current child experiment fails. This is done by setting in `ecf/configexp.h`  ","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":" TESTBED_CONT_ON_FAILURE=1","category":"page"},{"location":"System/HarmonieTestbed/#Input-data","page":"Testbed","title":"Input data","text":"","category":"section"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"The standard testbed configuration is run over several. The domain and resolution is chosen to be computationally cheap and not to give meteorologically interesting and meaningful results. Input data for running the testbed is ECMWF boundaries, observations, background error statistics and climate files. The latest data may be found  on cca:/scratch/ms/spsehlam/hlam/hm_home/some_recent_testbed_experiment/testbed_data and the data included is the following:","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"ECMWF boundaries for the tested periods\nObservations\nBackground errors\nClimate files\nForcing data for nested experiments","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"Download the data to your machine and put it on the default location $HM_REV/testbed_data or define your location in scr/Harmonie_testbed.pl. If you wish to test the climate generation you simple redefine the location of the climate files or remove the existing ones in the climate directory of the testbed. The testbed data typically includes the following:","category":"page"},{"location":"System/HarmonieTestbed/#Starting-the-testbed","page":"Testbed","title":"Starting the testbed","text":"","category":"section"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"The testbed experiment is setup as any normal experiment with","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"Harmonie setup -r REVISION -h HOST","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"The testbed is launched by","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"Harmonie testbed ","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"Before you start the testbed you should define your reference experiment. The reference experiment is picked automatically as an experiment with the same name but with lower revision number. The reference experiment can also be defined by the by setting REFEXP in ecf/config_exp.h  as the full path to another testbed experiment:","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"export REFEXP=/scratch/ms/spsehlam/hlam/hm_home/test_37h12\n","category":"page"},{"location":"System/HarmonieTestbed/#Evaluation-of-the-result","page":"Testbed","title":"Evaluation of the result","text":"","category":"section"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"At the end of each testbed run the results are compared to a reference experiment using the script scr/Testbed_copmp. The script uses xtool to check the numerical difference for the following output:","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"ECMWF input data\nclimate files\nInterpolated boundary files\nOutput forecast files in FA format\nOutput forecast files in GRIB format\nvfld/vobs files","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"In addition the internal consistency is checked by comparing runs with ","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"1D vs 2D decomposition\nEPS control vs a deterministic run\nRun with and without IO-server","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"The choice of internal consistency tests reflects the history of problems and inconsistencies encountered.","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"\nHARMONIE testbed results from ecgb-vecf\nSat Nov 16 20:41:27 GMT 2019\n\nConfiguration: cca.gnu\n\nCompare experiment ecgb_cca_testbed_develop_gnu_6147 and ecgb_cca_testbed_develop_gnu_6146\n\nCheck:ecmwf_bd\n  Output grib file summary (differ/missing/total) 0/0/37\n   comparison took 164 seconds\n Configuration ecmwf_bd is equal\n\nCheck:climate\n  Output internal file summary (differ/missing/total) 0/0/52\n   comparison took 34 seconds\n Configuration climate is equal\n\nCheck:AROME_3DVAR\n  Output internal file summary (differ/missing/total) 0/0/36\n   comparison took 79 seconds\n  Output grib file summary (differ/missing/total) 0/0/96\n   comparison took 91 seconds\n  vfld/vobs file summary (differ/missing/total) 0/0/36\n   comparison took 4 seconds\n Configuration AROME_3DVAR is equal\n\n ...\n\nCompare AROME_BD_ARO and AROME_BD_ARO_IO_SERV\n No differences found\n   comparison took 423 seconds\n\nTestbed comparison complete\n\n[ Status: OK]\n\n For more details please check /scratch/ms/spsehlam/hlam/hm_home/ecgb_cca_testbed_develop_gnu_6147/testbed_comp_6147.log_details\n","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"All the logs from any testbed experiment are posted to the testbed mailing list [https://hirlam.org/pipermail/testbed]. The test returns three different status signals","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"OK means that all configurations reproduces the result of your reference experiment.\nOK, BUT NO COMPARISON means that the suit run through but that there was nothing to compare with\nFAILED means that the internal comparisons failed\nDIFFER means that one more configurations differ from your reference experiment\nFAILED and DIFFER is a combination of the last two","category":"page"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"In addition to the summary information detailed information can be found in the archive about the art of the difference.","category":"page"},{"location":"System/HarmonieTestbed/#When-to-use-the-testbed","page":"Testbed","title":"When to use the testbed","text":"","category":"section"},{"location":"System/HarmonieTestbed/","page":"Testbed","title":"Testbed","text":"It is recommended to use the testbed when adding new options or make other changes in the configurations. If your new option is not activated the result compared with the reference experiment should be the same, if not you have to start debugging. When changing things for one configuration it's easy to break other ones. In such cases the testbed is a very good tool make sure you haven't destroyed anything.","category":"page"},{"location":"System/Local/QuickStartLocal/#Running-Harmonie-on-your-local-platform","page":"Local","title":"Running Harmonie on your local platform","text":"","category":"section"},{"location":"System/Local/QuickStartLocal/#Introduction","page":"Local","title":"Introduction","text":"","category":"section"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"These \"quick start instructions\" assumes that someone has already put in place a valid configuration for your local platform, CONFIG=linux.local for example.","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"The Harmonie system runs through a number of steps to help you complete your experiment. The chain can be summarized like:","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Configure and start the experiment: This is where you define your domain, choose your settings and specify the period for your experiment.","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Once you have done this you can start the system and let it create the basic infrastructure","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Setup the necessary directories and copy the system files needed (InitRun, Prepare_cycle)\nCompile the binaries you need to run your experiment (Build)\nCreate the constant climate files specifying your domain (Climate)","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"With the basic setup and files in place we can proceed to the integration part where we have three loops taking care of ","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Prepare boundaries and observations (MakeCycleInput)\nRun assimilation and forecasts (Date)\nPost process and archive the result (Postprocessing)","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"The three different task are allowed to run ahead/after each other to get a good throughput.","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"The configuration, the full suite and the relation between different tasks is controlled by the scheduler. This documentation describes how to get started with your first experiment. The description is general for a single host. (The reference Harmonie system on ECMWF platform assumes a dual-hosts setup with the front-end ecgb used to configure and launch experiments and cca is used for all computations except those for operations related to observation verification and monitoring. ","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Following example shows the steps to launch an Harmonie experiment my_exp.","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"If this is the first time to install HARMONIE on your local platform please take a look at the basic install instructions here. ","category":"page"},{"location":"System/Local/QuickStartLocal/#Configure-your-experiment","page":"Local","title":"Configure your experiment","text":"","category":"section"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Create an experiment directory under HOME/hmhome and use the master script Harmonie to set up a minimum environment for your experiment.  ```bash  mkdir -p HOME/hmhome/myexp  cd HOME/hmhome/myexp  PATHTOHARMONIE/config-sh/Harmonie setup -r PATHTO_HARMONIE -h YOURHOST  ```","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"where","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"-r is the path to your downloaded version of HARMONIE\n-h tells which configuration files to use. At ECMWF config.ecgb is the default one. List PATH_TO_HARMONIE/config-sh/config.* for available HOST configurations\nThis setup command  provides the default setup which currently is AROME physics with CANARI+OI_MAIN surface assimilation and 3DVAR upper air assimilations with 3h cycling on a domain covering Denmark using 2.5km horizontal resolution and 65 levels in the vertical.\nNow you can edit the basic configuration file ecf/config_exp.h to configure your experiment scenarios. Modify specifications for model domain, physics (AROME, ALARO), data locations, settings for dynamics, physics, domain, coupling host model etc. Read more about the options in here. You can also use some of the predefined configurations by calling Harmonie with the -c option:","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"mkdir $HOME/hm_home/my_exp\ncd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie setup -r PATH_TO_HARMONIE -h YOURHOST -c CONFIG ","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"where CONFIG is one of the setups defined in scr/Harmonie_configurations.pm. If you give -c with out an argument or a non existing configuration a list of configurations will be printed.","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"In some cases you might have to edit the general system configuration file, Env_system. See here for further information\nThe rules for how to submit jobs are defined in Env_submit. See here for further information\nIf you experiment in data assimilation you might also want to change scr/include.ass","category":"page"},{"location":"System/Local/QuickStartLocal/#Start-your-experiment","page":"Local","title":"Start your experiment","text":"","category":"section"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Launch the experiment by giving start time, DTG, end time, DTGEND, and forecast length, LL","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie start DTG=YYYYMMDDHH DTGEND=YYYYMMDDHH LL=12\n# e.g., PATH_TO_HARMONIE/Harmonie start DTG=2012122400 DTGEND=2012122406 LL=12","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"If you would like to only run long forecasts at 00/12 UTC and short (3h) at 03/06/09/15/18/21, you specify the longer forecast length as LLMAIN. ","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie start DTG=2012122400 LLMAIN=24","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"If successful, ecflow will identify your experiment name and start building your binaries and run your forecast. If not, you need to examine the mSMS log file $HM_DATA/mSMS.log. $HM_DATA is defined in your Env_system file. At ECMWF $HM_DATA=$SCRATCH/hm_home/$EXP where $EXP is your experiment name. Read more about where things happen further down.","category":"page"},{"location":"System/Local/QuickStartLocal/#Continue-your-experiment","page":"Local","title":"Continue your experiment","text":"","category":"section"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"If your experiment have successfully completed and you would like to continue for another period you should write","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie prod DTGEND=YYYYMMDDHH LL=12 ","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"By using prod you tell the system that you are continuing the experiment and using the first guess from the previous cycle. The start date is take from a file progress.log created in your $HOME/hm_home/my_exp directory. If you would have used start the initial data would have been interpolated from the boundaries, a cold start in other words.","category":"page"},{"location":"System/Local/QuickStartLocal/#Start/Restart-of-mXCdp","page":"Local","title":"Start/Restart of mXCdp","text":"","category":"section"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"To start the graphical window for mSMS on ecgb type","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie mon","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"The graphical window, mXCdp runs independently of the mSMS job and can be closed and restarted again with the same command. With the graphical interface you can control and view logfiles of each task. ","category":"page"},{"location":"System/Local/QuickStartLocal/#Making-local-changes","page":"Local","title":"Making local changes","text":"","category":"section"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Very soon you will find that you need to do changes in a script or in the source code. Once you have identified which file to edit you put it into the current $HOME/hm_home/my_exp directory, with exactly the same subdirectory structure as in the reference. e.g, if you want to modify a namelist setting ","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie co nam/harmonie_namelists.pm         # retrieve default namelist harmonie_namelists.pm\nvi nam/harmonie_namelists.pm                        # modify the namelist","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Next time you run your experiment the changed file will be used. You can also make changes in a running experiment. Make the change you wish and rerun the InitRun task in the mXCdp window. The !InitRun task copies all files from your local experiment directory to your working directory $HM_DATA. Once your InitRun task is complete your can rerun the task you are interested in. If you wish to recompile something you will also have to rerun the Build tasks. Read more about how to control and rerun tasks in mini-SMS from mXCdp.","category":"page"},{"location":"System/Local/QuickStartLocal/#Directory-structure","page":"Local","title":"Directory structure","text":"","category":"section"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"On most platforms HARMONIE compiles and produces all its output data under HM_DATA (defined in ~/hmhome/myexp/Env_system)","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"= Description                            = = Location                                                                                  =\nBinaries $BINDIR (set in ecf/config_exp.h ), default is $HM_DATA/bin\nlibraries, object files & source code $HM_DATA/lib/src if MAKEUP=yes, $HMDATA/gmkpack_build if MAKEUP=no\nScripts $HM_DATA/lib/scr\nconfig files (Envsystem & Envsystem $HM_DATA/lib linked to files in $HM_DATA/config-sh\nsms $HM_DATA/lib/sms\nmsms definitions $HM_DATA/lib/msms\nUtilities such as gmkpack, gl & monitor $HM_DATA/lib/util\nClimate files $HM_DATA/climate\nWorking directory for the current cycle $HM_DATA/YYYYMMDD_HH\nArchived files $HM_DATA/archive\nArchived cycle output $HM_DATA/archive/YYYY/MM/DD/HH\nArchived log files $HM_DATA/archive/log/HM_TaskFamily_YYYYMMDDHH.html where TaskFamily=MakeCycleInput,Date,Postprocessing\nTask log files $JOBOUTDIR (set in Env_system) usually $HM_DATA/sms_logfiles\nVerification data (vfld/vobs/logmonitor) $HM_DATA/archive/extract\nVerification (monitor) results $HM_DATA/archive/extract/WebgraF\n\"Fail\" directory $HM_DATA/YYYYMMDD_HH/Failed_Family_Task (look at ifs.stat,NODE.001_01, fort.4","category":"page"},{"location":"System/Local/QuickStartLocal/#Archive-contents","page":"Local","title":"Archive contents","text":"","category":"section"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"$HM_DATA/archive/YYYY/MM/DD/HH is used to store \"archived\" output from HARMONIE cycles. The level of archiving depends on ARSTRATEGY in ecf/config_exp.h . The default setting is medium which will keep the following cycle data:","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Surface analysis: ICMSHANAL+0000\nAtmospheric analysis result: MXMIN1999+0000\nBlending between surface/atmospheric analysis and cloud variable from the first guess: ANAB1999+0000\nICMSHHARM+NNNN and ICMSHHARM+NNNN.sfx are atmospheric and surfex forecast output files\nPFHARM* files produced by the inline postprocessing \nGRIB files produced by the conversion of FA output files to GRIB if MAKEGRIB=yes in ecf/config_exp.h \nODB databases and feedback information in odb_stuff.tar","category":"page"},{"location":"System/Local/QuickStartLocal/#Cleanup-of-old-experiments","page":"Local","title":"Cleanup of old experiments","text":"","category":"section"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Once you have complete your experiment you may wish to remove code, scripts and data from the disks. Harmonie provides some simple tools to do this. First check the content of the different disks by","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":" Harmonie CleanUp -ALL","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"Once you have convinced yourself that this is OK you can proceed with the removal.","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":" Harmonie CleanUp -ALL -go ","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"If you would like to exclude the data stored  HM_DATA ( as defined in Env_system ) you run ","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":" Harmonie CleanUp -d","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"to list the directories intended for cleaning. Again, convince yourself that this is OK and proceed with the cleaning by","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":" Harmonie CleanUp -d -go","category":"page"},{"location":"System/Local/QuickStartLocal/","page":"Local","title":"Local","text":"NOTE that these commands may not work properly in all versions. Do not run the removal before you're sure it's OK","category":"page"},{"location":"Visualization/EPyGrAM/#EPyGrAM","page":"EpyGram","title":"EPyGrAM","text":"","category":"section"},{"location":"Visualization/EPyGrAM/#General","page":"EpyGram","title":"General","text":"","category":"section"},{"location":"Visualization/EPyGrAM/","page":"EpyGram","title":"EpyGram","text":"EPyGram wiki","category":"page"},{"location":"Visualization/EPyGrAM/#Using-EPyGrAM-(version-1.4.13)-at-Atos-AA-(Bologna)","page":"EpyGram","title":"Using EPyGrAM (version 1.4.13) at Atos AA (Bologna)","text":"","category":"section"},{"location":"Visualization/EPyGrAM/","page":"EpyGram","title":"EpyGram","text":"Easy as pie:\nmodule use /perm/hlam/apps/modulefiles/lmod\nmodule load epygram/1.4.13\nTest:\nepy_cartoplot.py -f shortName:t,level:802  fc2020052500+000grib_sfx\nepy_section.py -f shortName:t,typeOfLevel:heightAboveGround -s'-8,53' -e'-7,53' fc2020052500+000grib_sfx\ndomain_maker.py","category":"page"},{"location":"Visualization/EPyGrAM/","page":"EpyGram","title":"EpyGram","text":"Enjoy!","category":"page"},{"location":"System/GitDeveloperDocumentation/#Developing-in-the-Hirlam-GitHub-organization","page":"GitHub workflow","title":"Developing in the Hirlam GitHub organization","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#Introduction","page":"GitHub workflow","title":"Introduction","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"Since 2018 and CY43 HIRLAM have used git for code revision control and gitolite as the git server on hirlam.org. HIRLAM is now moving to using Github for ''software development and version control''. This page provides information on how to access the GitHub Hirlam organisation and how to commit your developments, specifically Harmonie. As was the case with hirlam.org's gitolite a fork-and-branch workflow will be used to manage developments.","category":"page"},{"location":"System/GitDeveloperDocumentation/#Becoming-a-member-of-Hirlam","page":"GitHub workflow","title":"Becoming a member of Hirlam","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"Create a GitHub account: https://github.com – click on Sign up. Details here\nGo to settings and add your full name, company and location to make it easier to identify you\nAdd your public ssh key(s) to the account. Details here\nContact your friendly System-core to be invited to the GitHub Hirlam organisation","category":"page"},{"location":"System/GitDeveloperDocumentation/#Fork-and-branch","page":"GitHub workflow","title":"Fork and branch","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"You can create a user fork of Harmonie by doing the following:","category":"page"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"Go to https://github.com/Hirlam/Harmonie\nClick on Fork to create a fork of Harmonie for your user (USER)\nClone your fork:\ngit clone git@github.com:USER/Harmonie.git $HOME/git/github/USER/Harmonie","category":"page"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"Further information is available here ","category":"page"},{"location":"System/GitDeveloperDocumentation/#Keep-your-fork-synced","page":"GitHub workflow","title":"Keep your fork synced","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"In a terminal change directory to the clone of you fork:\ncd $HOME/git/github/USER/Harmonie\nList the current configured remote repository for your fork.\ngit remote -v\norigin\tgit@github.com:USER/Harmonie.git (fetch)\norigin\tgit@github.com:USER/Harmonie.git (push)\nSpecify a new remote upstream repository that will be synced with the fork.\ngit remote add upstream git@github.com:Hirlam/Harmonie.git\nVerify the new upstream repository you've specified for your fork.\ngit remote -v\norigin\tgit@github.com:USER/Harmonie.git (fetch)\norigin\tgit@github.com:USER/Harmonie.git (push)\nupstream\tgit@github.com:Hirlam/Harmonie.git (fetch)\nupstream\tgit@github.com:Hirlam/Harmonie.git (push)\nUpdate the dev-CY46h1 branch in your fork.\ngit checkout dev-CY46h1\ngit pull upstream dev-CY46h1\ngit push origin dev-CY46h1","category":"page"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"Further information is available here","category":"page"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"An alternative approach is to go to your fork on github.com and click \"Fetch upstream\" which will sync the branch on GitHub followed by a git pull in your clone ","category":"page"},{"location":"System/GitDeveloperDocumentation/#Contributing-developments-–-dev-CY46h1","page":"GitHub workflow","title":"Contributing developments – dev-CY46h1","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"info: Feature branches\nContributions to the dev-CY46h1 branch (and any other branch) should be developed in your Harmonie fork in a development branch (feature/bugfix/...). The following details how you can get your development upstream.","category":"page"},{"location":"System/GitDeveloperDocumentation/#Create-a-feature-branch","page":"GitHub workflow","title":"Create a feature branch","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"cd $HOME/git/github/USER/Harmonie\ngit checkout dev-CY46h1\ngit checkout -b feature/descriptive_name_for_developments","category":"page"},{"location":"System/GitDeveloperDocumentation/#Keep-up-to-date-with-dev-CY46h1","page":"GitHub workflow","title":"Keep up to date with dev-CY46h1","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"Sync your fork as described above\nMerge dev-CY46h1 updates in to your branch\ncd $HOME/git/github/USER/Harmonie\ngit checkout feature/descriptive_name_for_developments\ngit merge dev-CY46h1","category":"page"},{"location":"System/GitDeveloperDocumentation/#Creating-a-pull-request","page":"GitHub workflow","title":"Creating a pull request","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"Once you have committed to your feature branch and wanted them included in the upstream repo you should do the following:","category":"page"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"Push your branch to your fork:\ncd $HOME/git/github/USER/Harmonie\ngit checkout feature/descriptive_name_for_developments\ngit push origin feature/descriptive_name_for_developments\nWhen you push your branch information on how to create a pull request will be presented:\nremote: Resolving deltas: 100% (x/x), completed with x local objects.\nremote: \nremote: Create a pull request for 'feature/descriptive_name_for_developments' on GitHub by visiting:\nremote:      https://github.com/USER/Harmonie/pull/new/feature/descriptive_name_for_developments\nremote: \nFollow this link\nrequest a reviewer\nadd labels to the development (feature/enhancement/...)\nadd comments to help with the review process (Testbed members used/Changes expected if any/...)\nOnce the pull request has been approved by the System-core team it will be merged in to the dev-CY46h1 branch","category":"page"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"Further information is available here","category":"page"},{"location":"System/GitDeveloperDocumentation/#Moving-my-branches-from-hirlam.org","page":"GitHub workflow","title":"Moving my branches from hirlam.org","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"Add your hirlam.org fork as a remote (HLUSER is your hirlam.org username) \ncd $HOME/git/github/USER/Harmonie\ngit remote add hirlamorgfork https://git.hirlam.org/users/HLUSER/Harmonie\ngit fetch hirlamorgfork\nFor each branch BRANCHNAME you want to move to github\ngit checkout -t hirlamorgfork/BRANCHNAME\ngit push origin BRANCHNAME","category":"page"},{"location":"System/GitDeveloperDocumentation/#Useful-git/github-links","page":"GitHub workflow","title":"Useful git/github links","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"learn git branching is an excellent interactive tool to understand git.","category":"page"},{"location":"System/GitDeveloperDocumentation/#Coding-Standards","page":"GitHub workflow","title":"Coding Standards","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/","page":"GitHub workflow","title":"GitHub workflow","text":"See Coding standards for Arpège, IFS and Aladin","category":"page"},{"location":"Overview/da_graph/#Harmonie-Data-Assimilation","page":"Graphical Overview","title":"Harmonie Data Assimilation","text":"","category":"section"},{"location":"Overview/da_graph/","page":"Graphical Overview","title":"Graphical Overview","text":"click to enlarge and enable links","category":"page"},{"location":"Overview/da_graph/","page":"Graphical Overview","title":"Graphical Overview","text":"(Image: da)","category":"page"},{"location":"Overview/da_graph/","page":"Graphical Overview","title":"Graphical Overview","text":"# NOTE: In  local builds, before running make.jl, you might want to \n# export CI=true\n# This will make the links in the figure work  \n\nusing Kroki\nharmonie=graphviz\"\"\"\ndigraph Harmonie {\n\n//-------------------------------------------\n//         Tasks\n//-------------------------------------------\nnode[shape=\"rectangle\"]  \nPrepare_ob    \nBator   \nOulan          \nFirstGuess\nFetch_assim_data\nCanari      \nScreening    \nMinim          \nForecast       \n\n\n//------------------------------------------/\n//          Files\n//------------------------------------------/\nnode[shape=\"note\"] \nMXMIN1999p0000      [label=\"MXMIN1999+0000\", tooltip=\"Upper air Analysis\"]\nICMSHANALp0000sfx   [label=\"ICMSHANAL+0000.sfx\", tooltip=\"Surface Analysis\"]\nobYYYYMMDD          [label=\"observations/ob\\$YYYYMMDD\"]\nOBSOUL              [label=\"oulan/OBSOUL\"]\nodb                 [shape=\"cylinder\"] \nodb_ccma            [shape=\"cylinder\"]\nodb_can_merge       [shape=\"cylinder\"]\nodbvar              [shape=\"cylinder\"]\nodb_can             [shape=\"cylinder\"]\nodb_can_ori         [shape=\"cylinder\"]\nfc_start            \nfirst_guess  \nfirst_guess_sfx    \nfc_start_sfx      \nVarBC_very_fg       [label=\"VarBC.cycle\", tooltip=\"VarBC very first guess\" ]\nVarBC_fg            [label=\"VarBC.cycle\", tooltip=\"VarBC first guess\"]\nVarBC_an            [label=\"VarBC.cycle\", tooltip=\"VarBC analysis\"]\nstabal96            [label=\"stabal96.{cv,bal}\", tooltip=\"Structure Functions\"]\n\n\n/// links\n\n                                                                    Prepare_ob        -> {obYYYYMMDD }\n                                                                    Fetch_assim_data  -> {VarBC_very_fg, stabal96}\n                                                                    FirstGuess        -> {first_guess_sfx, first_guess}\n{obYYYYMMDD                                                    } -> Oulan             -> {OBSOUL }\n{OBSOUL, obYYYYMMDD                                            } -> Bator             -> {odb, odb_can_ori }\n{first_guess, first_guess_sfx, ECMWF_CANARI_SST_SIC,odb_can_ori} -> Canari            -> {ICMSHANALp0000sfx, odb_can, odb_can_merge}\n{first_guess, VarBC_very_fg, odb                               } -> Screening         -> {odb_ccma, odbvar, VarBC_fg }\n{odb_ccma, VarBC_fg, first_guess                               } -> Minim             -> {MXMIN1999p0000, VarBC_an} \n{fc_start, fc_start_sfx                                        } -> Forecast \n\n\n//----------------------------------\n// \"Static\" input,   namelists, climate files,  etc. \n//----------------------------------\n  \n{LISTE_NOIRE_DIAP, LISTE_LOC,param_batorcfg } -> Bator\n{stabal96                    } -> Minim\n  \n\n\n\n//---------------------------------------------------------------\n//                   Symlinks\n//---------------------------------------------------------------\nICMSHANALp0000sfx -> fc_start_sfx  [label=\"Canari\",            style=dashed]\nMXMIN1999p0000    -> fc_start       [label=\"Minim\",             style=dashed]\n\n//-----------------------------------------------------------------\n//            URL\n//----------------------------------------------------------------\nPrepare_ob       [href=\"../Observations/ObservationData/index.html\"]\nCanari           [href=\"../DataAssimilation/Surface/CANARI/index.html\"]\nForecast         [href=\"../ForecastModel/Forecast/index.html\"]\nOulan            [href=\"../Observations/Oulan/index.html\"]\nBator            [href=\"../Observations/Bator/index.html\"]\nMinim            [href=\"http://www.umr-cnrm.fr/gmapdoc/IMG/pdf/ykminim45.pdf\"]\nScreening        [href=\"../DataAssimilation/Screening/index.html\"]\nstabal96       [href=\"../DataAssimilation/Structurefunctions/index.html\"]\nLISTE_NOIRE_DIAP [href=\"../Observations/Bator/index.html#LISTE_NOIRE_DIAP-1\"]\nLISTE_LOC        [href=\"../Observations/Bator/index.html#LISTE_LOC-1\"] \n\n}// Digraph Harmonie\n\"\"\"\nwrite(\"../assets/da_graph.svg\",render(harmonie,\"svg\"))","category":"page"},{"location":"Build/Gmkpack_vs_Make/#GMKPACK-vs.-Make","page":"Gmkpack vs Make","title":"GMKPACK vs. Make","text":"","category":"section"},{"location":"Build/Gmkpack_vs_Make/","page":"Gmkpack vs Make","title":"Gmkpack vs Make","text":"The HARMONIE system, like the ARPEGE/ALADIN/AROME system, uses the (set of scripts) GMKPACK to build the libraries and binaries.","category":"page"},{"location":"Build/Gmkpack_vs_Make/","page":"Gmkpack vs Make","title":"Gmkpack vs Make","text":"GMKPACK builds a script ics_arome that performs the following steps, using scripts from the aux directory:","category":"page"},{"location":"Build/Gmkpack_vs_Make/","page":"Gmkpack vs Make","title":"Gmkpack vs Make","text":"Establishes an order in the projects (in fact, the subdirectories of the src/local directory) using the file conf/projects.\nConstructs interface block header files for the ald and arp directories:\nAuto-generated explicit interface blocks on projects ald arp ...\nfile=ald/adiab/elarche.F90 dir=local/.intfb/ald elarche.intfb.h : no-changes\nfile=ald/adiab/elarmes.F90 dir=local/.intfb/ald elarmes.intfb.h : no-changes\nGenerates a list of include paths:\nInclude/source paths ...\nInclude path :\n-I/home/harmonie/pack/31h1_main.01.420./src/local/ald/module\n-I/home/harmonie/pack/31h1_main.01.420./src/local/arp/module\nBuilds a ordered list of source files to compile:\nOrdered compilation list INCluding dependencies ...\nSource descriptors for local view is trivial ...\nLocal view for modules ...\nCheck for duplicated modules ...\nSort all projects together ...\n------ Level 1\nlocal@xrd/module/parkind1.F90\n------ Level 2\nlocal@.intfb/arp/tridialcz.intfb.h\nlocal@xrd/include/dr_hook_util.h\nBuilds the precompilers odb98.x and bla95.x.\nActually compiles the sources.\n------ Start compilation --------------------------------------\n----------- Level 1 / 250 ---------------------------------------\nNo recursive update:\nCompile:\ngfortran -c -fconvert=swap -ffree-form -DBLAS -DLINUX -DLITTLE_ENDIAN -DLITTLE -DHIGHRES -g -O2 -fbacktrace local/xrd/module/parkind1.F90\nThen it builds the libraries.\n------ Make libraries -----------------------------------------------\n\nar: creating /home/harmonie/pack/31h1_main.01.420./lib/libsur.local.a\na - ./surfexcdriversad.o\nSubsequently, it builds the single executable (AROME):\n========= Linking binary AROME =========\n\nTop libraries actually used :\nlib[01].a=\"/home/harmonie/pack/31h1_main.01.420./lib/libald.local.a\"\n...\nscanning lib[12].a ...\n\ngfortran -g -fbacktrace -Wl,--start-group ./master.o -L. -l[1] -l[2] -l[3] -l[4] -l[5] -l[6] -l[7] -l[8] -l[9] -l[10] -l[11] -l[12] -Wl,--end-group -L/home/harmonie/auxlibs -lecR64 -lgribexR64 -lfdbdummy_000_gnuR64 -lwamdummy_000_gnuR64 -lnaglitedummy_000_gnuR64 -loasisdummy_000_gnuR64 -llapack -lblas -L/home/harmonie/auxlibs -lmpidummy_000_gnuR64 -libmdummy_000_gnuR64","category":"page"},{"location":"Build/Gmkpack_vs_Make/","page":"Gmkpack vs Make","title":"Gmkpack vs Make","text":"To construct a make file, we have to use the outcome of steps 1 - 4.","category":"page"},{"location":"Build/Gmkpack_vs_Make/","page":"Gmkpack vs Make","title":"Gmkpack vs Make","text":"Issues with gmkpack:","category":"page"},{"location":"Build/Gmkpack_vs_Make/","page":"Gmkpack vs Make","title":"Gmkpack vs Make","text":"Hard to understand logic.\nSlow, especially because the handling of the dependencies is done in scripts.\nNot clear how to compile in parallel, unless on an MPI capable system.","category":"page"},{"location":"Build/Gmkpack_vs_Make/#USING-FCM-TO-BUILD-HARMONIE","page":"Gmkpack vs Make","title":"USING FCM TO BUILD HARMONIE","text":"","category":"section"},{"location":"Build/Gmkpack_vs_Make/","page":"Gmkpack vs Make","title":"Gmkpack vs Make","text":"Instead of gmkpack, we could try to use fcm (Flexible Configuration Management).","category":"page"},{"location":"Build/Gmkpack_vs_Make/","page":"Gmkpack vs Make","title":"Gmkpack vs Make","text":"Create a new directory for this build.\nChange directory to the top of this build tree.\nAdd the fcm/bin directory to your PATH (where ever it is based).\nMake a copy of the repository checkout's src directory:\ncp -R /home/harmonie/harmonie/src .\nDelete the following subdirectories:\nrm -rf src/bla/compiler src/odb/y2k.obsolete src/odb/bufrbase src/odb/examples src/odb/prescreen\nMake the MESO-NH sources (.mnh files) available to fcm by giving them a .f90 extension:\nfor f in `find . -name '*.mnh' -print`\ndo\n   d=`dirname $f`\n   b=`basename $f .mnh`\n   (cd $d; ln -s $b.mnh $b.f90)\ndone\nGo to the odb compiler's directory and pre-build the lexical analyzer and parser (actually, fcm should be able to generate rules for this):\ncd src/odb/compiler\nflex -l lex.l\nyacc -d yacc.y\nIn the resulting file lex.yy.c, move the following definitions to before their first use (a bug in versions of flex newer than 2.5.4):\n#define INITIAL 0\n#define LEX_NORMAL 1\n#define LEX_INCLUDE 2\n#define LEX_SET 3\n#define LEX_TYPE 4\n#define LEX_TABLE 5\n#define LEX_VIEW 6\n#define LEX_FROM 7\n#define LEX_ORDERBY 8\n#define LEX_EXCLUDED_BY_IFDEF 9\nAdd the following prototype to odb98.c (it is necessary for odb98.c to be recognized as the source file for the main program of odb98.exe - probably a shortcoming in fcm):\nint main(int, char**);\nReturn to the root of the build directory tree:\ncd ../../..\nConstruct a Build Configuration file bld.cfg.\n# This file is a build configuration\n\ncfg::type  bld\n\n# The root directory of this build\n\ndir::root  $HOME/tmp\n\n# The two target executables to be build\n\ntarget     odb98.exe master.exe\n\n# Their dependencies on the source directories\n\nexe_dep::odb98.exe    odb\nexe_dep::master.exe   ald  arp  bla  mpa  mse  odb  sat  sur  tal  tfl  uti  xrd\n\n# Tools: the compilers\n\ntool::fc   gfortran\ntool::cc   gcc\n\n# Tools: the compile time flags\n\ntool::fflags -g -fconvert=swap -O2 -fdefault-real-8 -fdefault-double-8 -DLINUX -DLITTLE_ENDIAN -DLITLLE -DBLAS -DHIGHRES\ntool::cflags -g -O2 -DLINUX -DLITTLE_ENDIAN -DLITTLE -DCANARI -DSTATIC_LINKING -DXPRIVATE=PRIVATE -UINTERCEPT_ALLOC -UUSE_ALLOCA_H\n\n# Tools: the loader and the load flags\n\ntool::ld   gfortran\ntool::ldflags -g -z muldefs -L/home/harmonie/auxlibs -lecR64 -lgribexR64 -lfdbdummy_000_gnuR64 -lwamdummy_000_gnuR64 -lnaglitedummy_000_gnuR64 -loasisdummy_000_gnuR64 -llapack -lblas -L/home/harmonie/auxlibs -lmpidummy_000_gnuR64 -libmdummy_000_gnuR64\n\n# The extension of include files for autogenerated interface blocks\n\noutfile_ext::interface .intfb.h\n\n# Don't consider this file when computing dependencies\n\nexcl_dep   h::mpif.h\nStart the Build:\nfcm build\nIt now needs the mpif.h file.  Add it to the inc directory, e.g., like this (alternatively, we could add -I/home/harmonie/auxinclude/mpidummy_000 to the fortran compile flags):\ncp /home/harmonie/auxinclude/mpidummy_000/mpif.h inc/\nUnfortunately, the names for the \"done\" files of the interface block include files are not correct in the done directory (this is probably an error in fcm, although it could subtly be dependent on a mistake in the configuration file):\ncd done\nfor f in *.intfb.h.pdone\ndo\n   ln -s $f `basename $f .intfb.h.pdone`.intfb.done\ndone\ncd ..\nIn bld/odb!__aux.mk, add the following directory specification: $(SRCDIR0!__odb!__aux)/ in front of every mention of odbi_shared.c without a directory.  This is necessary because odbi_shared.c is an include file (but not recognized by fcm as such - hence it's not in the inc directory and cannot be found without a full path).  This is something fcm should be able to do right without human intervention.\nEquivalently, in bld/odb!__extras!__ec.mk add $(SRCDIR0!__odb!__extras!__ec)/ in front of every mention of julian_lib.c without a directory.  Idem for error.c.\nEquivalently, in bld/odb!__tools.mk add $(SRCDIR0!__odb!__tools)/ in front of every mention of odbi_direct_main.c without a directory.\nRestart the Build.\nfcm build\nResults:\nProblems:\nHow to deal with the .mnh and .sql files (get processed by gmkpack/ics_arome in a special way).\nHow to deal with the different Fortran / C compile time flags for different subdirectories.\nHow to generate the link command like gmkpack does.","category":"page"},{"location":"assets/README/","page":"-","title":"-","text":"da_graph.svg is created in .github/workflows/documentation.yml. It can be recreated locally by using dot which is part of graphviz","category":"page"},{"location":"assets/README/","page":"-","title":"-","text":"sudo apt install graphviz","category":"page"},{"location":"assets/README/","page":"-","title":"-","text":"dot -Tsvg da_graph.dot -o da_graph.svg","category":"page"}]
}
