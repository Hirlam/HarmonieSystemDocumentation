var documenterSearchIndex = {"docs":
[{"location":"Overview/Content/#Harmonie-Content-1","page":"Content","title":"Harmonie Content","text":"","category":"section"},{"location":"Overview/Content/#Overview-1","page":"Content","title":"Overview","text":"","category":"section"},{"location":"Overview/Content/#","page":"Content","title":"Content","text":"Harmonie is HIRLAM's adaptation of the LAM version of the IFS/ARPEGE project. The common code shared with the ALADIN program, Meteo France and ECMWF only contains the source code. Harmonie adds the build environment, scripts, support for a scheduler, and a number of diagnostics tools for file conversion and postprocessing. In summary a git clone of harmonie from github contains the following main directories","category":"page"},{"location":"Overview/Content/#","page":"Content","title":"Content","text":"config-sh : Configuration and job submission files for different platforms.\nconst : A selected number of constant files for bias correction, assimilation and different internal schemes. A large number of data for climate generation and the RTTOV software is kept outside of the repository. See [wiki:HarmonieSystemDocumentation#Downloaddata].\necf : Directory for the main configuration file config_exp.h and the containers for the scheduler ECFLOW.\nsuites Scripts and suit definition files for ECFLOW, the scheduler for HARMONIE. \nnam : Namelists for different configurations.\nscr : Scripts to run the different tasks.\nsrc : The IFS/ARPEGE source code.\nutil : A number of utilities and support libraries.","category":"page"},{"location":"Overview/Content/#util-1","page":"Content","title":"util","text":"","category":"section"},{"location":"Overview/Content/#","page":"Content","title":"Content","text":"The util directory contains the following main directories","category":"page"},{"location":"Overview/Content/#","page":"Content","title":"Content","text":"auxlibs : Contains gribex, bufr, rgb and some dummy routines\nbinutils : https://www.gnu.org/software/binutils/\nchecknorms : Script for code norm checking\ngl_grib_api : Boundary file generator and file converter\nmakeup : HIRLAM style compilation tool\nmusc : MUSC scripts\nobsmon : Code to produce obsmon sqlite files\noffline : SURFEX offline code\noulan : Converts conventional BUFR data to OBSOUL format read by bator.\nRadarDAbyFA : Field alignment code","category":"page"},{"location":"Verification/Verification/#HARMONIE-Verification-1","page":"Verification","title":"HARMONIE Verification","text":"","category":"section"},{"location":"Verification/Verification/#Introduction-1","page":"Verification","title":"Introduction","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The verification package in HARMONIE is designed to be a self contained stand alone package dealing with pre-extracted model and observational data. The package calculates several standard verification scores such as:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Error as function of forecast lead time summarises the bias and rms­ error and their growth rate over a set of forecasts\nTime sequences and vertical profiles show how your data or error characteristic is distributed in time or in the vertical\nError charts and tables show how some error is distributed in space, and station­wise linear correlation\nScatter plots show the correspondence between forecast and observed values\nMean diurnal cycles show how your mean error changes in the course of the day\nHistograms show the correspondence between the distributions of forecast and observed values\nStudent t-test to show how reliable differences between different experiments are","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"In addition there are a number of scores based on contingency tables like:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Frequency bias (bias score): (h+fa)/(h+m); Compares the frequency of predicted events to the frequency of observed events. Range: 0 -> infinite. Perfect score: 1\nHit rate (probability of detection): h/(h+m); What fraction of the observed events were correctly forecast. Range: 0 to 1.  Perfect score: 1. \nFalse alarm ratio: fa/(h+fa); What fraction of the predicted events did not occur. Range: 0 to 1.  Perfect score: 0.\nFalse alarm rate: fa/((cn+fa); What fraction of the observed \"no\" events were incorrectly forecast as \"yes\". Range: 0 to 1. Perfect score: 0.\nThreat score: h/(h+m+fa); How well did the forecast \"yes\" events correspond to the observed \"yes\" events. Range: 0 to 1. Perfect scrore: 1.\nThe Equitable threat score takes into account the number of random hits (R) and is less sensitive to climatology: ETS=(h­R)/(h+m+fa­R),  R=(h+m)(h+fa)/(h+m+fa+cn). Often used in verification of precipitation. Range: -1/3 to 1, 0 indicates no skill.   Perfect score: 1. \nHansen­Kuipers score: (h/(h+m) ­ fa/(fa+cn)), How well did the forecast separate events from non­events. Range: -1 to 1, 0 indicates no skill. Perfect score: 1.\nExtreme Dependency Scores: What is the association between forecast and observed rare events? Range: -1 to 1, 0 indicates no skill. Perfect score: 1","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"A more detailed explanation about verification can found here","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The scores can be presented per station for the whole data set or filtered through different selection criteria based on e.g. a geographical domain or properties of the data itself. One key feature missing in earlier HIRLAM verification packages is that the comparison is done over exactly the same set of data ( in time and space ) when comparing different experiments or models. The scores are finally presented with a portable web interface, [#WebgraF WebgraF], that allows you to easily share the information with others. Since the verification is station based it is less suitable for moving platforms or fields.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Other examples on how products from the verification package looks like today can be found here:     * The monitor test data set     * FMI     * Mast verification     * HIRLAM verification portal","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"In the following we describe the different parts of the verification package. For preparation of verification data read more here.","category":"page"},{"location":"Verification/Verification/#Getting-and-compiling-the-code-1","page":"Verification","title":"Getting and compiling the code","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The verification code can be fetched from the hirlam code repository by","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"git clone https://git.hirlam.org/monitor\ncd monitor","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"For a specific version do ","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\ngit ls-remote \nUsername for 'https://git.hirlam.org': uandrae\nPassword for 'https://uandrae@git.hirlam.org': \nFrom https://git.hirlam.org/monitor\n6e733fdaf0b47aefbd62721a33c10d30b9320759\tHEAD\n6e733fdaf0b47aefbd62721a33c10d30b9320759\trefs/heads/master\n9c0aa45d658edc052f427c9f1d750f31732f64cb\trefs/tags/v1.0\n20dcc868000e94e3f3178e38cf661befff6d9e03\trefs/tags/v1.0^{}\n7817776b7676ecf80020d0e4994a02fef3870d36\trefs/tags/v1.1\n2989b8ff40de90abb2dde1521350107e2770aef5\trefs/tags/v1.1^{}\n...\n\ngit checkout vX.X\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The checkout gives you the following directories:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"cmastat: for assimilation monitoring, not covered here\nconfig: configuration files for different platforms\ndoc: README files\nmod: Different modules. module_data.f90 contains all namelist variables\nprg: Main programs\nrdr: Routines for reading data\nscr: Scripts\nsrc: The hard core verification routines\ntools: Compilation tools\nWebgraF: The web interface","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"You compile by finding the appropriate configuration file for your platform and type","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"gmake ARCH=YOUR_ARCH","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The package has no external dependencies but relies on gnuplot for generation of the graphics.","category":"page"},{"location":"Verification/Verification/#The-verification-step-by-step-1","page":"Verification","title":"The verification step by step","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The strategy in the verification is to separate the data input from the calculations of the different scores. This allows as to go through the data several times using different filtering criteria. Of course keeping everything in memory sets a limit on how much data one can handle at the same time. In the way it's used in HARMONIE a typical sequence is:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Read namelist\nCall my_choices\nRead observations\nRead model data\nPerform quality control\nLoop over all namelists\nSelection of data \nCalculate the scores and write data\nCheck for new namelist","category":"page"},{"location":"Verification/Verification/#Reading-the-data-1","page":"Verification","title":"Reading the data","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The program can handle several data sources. Which one you use is depending on the value of DATA_SOURCE and is controlled in the routine monitor/rdr/my_choices.f90. At namelist level we also control which experiments we should read, the period (SDATE,EDATE), interval between cycles (FCINT), which forecasts (FCLEN) and the interval of the observations (OBINT). We can also already at this point select which stations to use by specifying a station list (STNLIST). ","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The HARMONIE tools to extract data for verifiation are described in here.","category":"page"},{"location":"Verification/Verification/#A-general-input-format-1","page":"Verification","title":"A general input format","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"For the every day verification the model and observation data are read with the routines ","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"monitor/rdr/read_vfld.f90, monitor/rdr/read_vobs.f90, monitor/rdr/read_vfld_temp.f90, monitor/rdr/read_vobs_temp.f90 Where the two first are for surface data and are used when DATA_SOURCE=vfld and the two latter for temp data and are used when DATA_SOURCE=vfld_temp. During the evolution of the verification package the format of the input data has changed and we are now at version four. The new format allows an arbitrary number of different types of point data to be included in the model vfld- or observation vobs- files.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The generalized input format is defined as ","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"nstation_synop nstation_temp version_flag  # written in fortran format '(1x,3I6)' )\n# where version_flag == 4\n# If ( nstation_synop > 0 ) we read the variables in the file, their descriptors and\n# their accumulation time\n#\nnvar_synop\nDESC_1 ACC_TIME_1\n...\nDESC_nvar_synop ACC_TIME_nvar_synop\n# Station information and data N=nstation_synop times\nstid_1 lat lon hgt val(1:nvar_synop)\n...\nstid_N lat lon hgt val(1:nvar_synop)\n\n# If ( nstation_temp > 0 )\nnlev_temp\nnvar_temp\nDESC_1 ACC_TIME_1\n..\nDESC_nvar_temp ACC_TIME_nvar_temp\n# Station information and data nstation_temp times\n# and station data nlev_temp times for each station\nstid_1 lat lon hgt\npressure(1) val(1:nvar_temp)\n...\npressure(nlev_temp) val(1:nvar_temp)\nstid_2 lat lon hgt\n...","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The accumulation time allows us to e.g. easily include different precipitation accumulation intervals. Any variable can be included in the file and verified without any code changes. Once you have defined a variable in your data you have to describe its properties in the monitor/scr/plotdefs.pm described in the  parameter settings ","category":"page"},{"location":"Verification/Verification/#Quality-control-1","page":"Verification","title":"Quality control","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The quality control is activated by the namelist flag LQUALITY_CONTROL. It is mainly there as a gross error check to remove the unrealistic observations. The check has the following features:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The forecast lengths used for quality control can be set by namelist variable QC_FCLEN. If QC_FCLEN is not set The forecasts < FORECAST_INTERVAL will be used.\nAn observation is accepted if ABS(mod - exp) < err_limit for ANY experiment used in the verification. \nThe quality control limits can be set explicitly in namelist by VARPROP%LIM for any variable or by QC_LIM for all variables. See the section about parameter settings for further instructions.\nBy setting ESTIMATE_QC_LIMIT the QC_LIM will be set as SCALE_QC_LIM * STDV for the forecasts in QC_FCLEN.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"QC diagnostic information output may be controlled by PRINT_QC={0,1,2}. It is also possible to blacklist stations through the STNLIST_BL parameter.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"In the HARMONIE implementation all quality control levels are estimated on the fly with a limit where the standard deviation is scaled by 5.","category":"page"},{"location":"Verification/Verification/#The-selection-process-1","page":"Verification","title":"The selection process","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Before we run through the actual comparison of data we can select a subset of the data depending on different criteria.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Initial time of the forecast (INI_HOURS)\nSelect data that are valid at a certain hour (SHOW_TIMES) \nPick only some of the forecast lengths in memory (USE_FCLEN)\nSelection using a station list (STNLIST)\nSelect to produce statistics for some selected stations in addition to the overall statistics (STNLIST_PLOT)\nDefine a geographical box through the definition of the corners (CBOX%ACTIVE,CBOX%S,CBOX%W,CBOX%N,CBOX%E).\nUse an area defined by a polygon (LPOLY,POLYFILE)\nSelect by station height (HGT_LLIM,HGT_ULIM)\nConditions given by the data (described later)\nReverse all selections but the conditional (REVERSE_SELECTION)","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Of course all the selections can be combined.","category":"page"},{"location":"Verification/Verification/#The-verification-loop-1","page":"Verification","title":"The verification loop","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"For each station we loop over all selected stations, initial times, forecast lengths for the given period and accumulate the statistics. Data is stored by:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Statistic by forecast length or time of day, for each station and accumulated for all stations and is used to generate error maps(MAP), vertical profiles(VERT), standard forecast length verification(GEN) and daily variation scores (DAYVAR)\nTime serie arrays for each station and accumulated for all stations (TIME)\nOne array containing all the selected data used for scatter plots (SCAT) and contingency table calculations (CONT). From the contingency table we can calculate several different scores described later.","category":"page"},{"location":"Verification/Verification/#Output-format-1","page":"Verification","title":"Output format","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Early versions of the package was based on the ECMWF graphics package MAGICS. Due to the poor portability of MAGICS the package now a days produces text files that are parsed through monitor/scr/verobs2gnuplot.pl that produces plots using gnuplot. It may not be the most elegant graphics package, but it is available almost everywhere. Some verification are also produced in form of tables. The contingency tables are parsed through monitor/scr/contingency2gnuplot.pl to produce skill scores.","category":"page"},{"location":"Verification/Verification/#HARMONIE-user-interface-1","page":"Verification","title":"HARMONIE user interface","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"In HARMONIE a set of scripts is build around the code for generation of plots and building the web page. There are two main scripts monitor/scr/Run_verobs_surface for verification of surface variables and monitor/scr/Run_verobs_temp for verification of radio sonde data. Both of them need a configuration file, Env_exp, as input:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"./Run_verobs_surface MY_ENV_EXP\n./Run_verobs_temp MY_ENV_EXP","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"There is also a master script monitor/scr/Run_verobs_all which cleans the webpage, runs through both types of verification and creates a tar file suitable to add to an existing WebgraF page. It is used in the same way like:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"./Run_verobs_all MY_ENV_EXP","category":"page"},{"location":"Verification/Verification/#The-main-configuration-file-1","page":"Verification","title":"The main configuration file","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"For most of the cases you can configure your verification by just editing the configuration file, monitor/scr/Env_exp. The above mentioned script can take files with any name so it's a good idea to have different files for different sets of experiments. First you have to identify your experiments and their location. ","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"# Experiment names and paths,should be space separated\nDBASE=/scratch/ms/dk/nhz/oprint\nEXP=\"RCR C22\"\nDISPLAY_EXP=\"$EXP\"\nOBSPATH=$DBASE/OBS2/\nP1=$DBASE/RCR/\nP2=$DBASE/C22/\nMODPATH=\"$P1 $P2\"","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The experiment name should of course match the name on the vfld files. At the moment there is an upper limit of ten experiments, but already at five the plots starts to get pretty messy. It is possible to disply more meaningful names by setting DISPLAY_EXP to something different.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"From harmonie-40h1.1.1.rc1 the output from the verification for the initial setup has changed so that ","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"vfldEXPYYYYMMDDHH represents analysis data\nvfldEXPYYYYMMDDHHLL represents output from the forecast model at +00","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"To handle this USE_ANALYSIS has been introduced to allow different choices and combination of the new and old convention.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n# Use analysis at +00h, set per experiment\nUSE_ANALYSIS=\".FALSE.,.TRUE.,.FALSE.,.TRUE.,.TRUE.\"","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"We should now defined the name on the WebgraF page and write some text ( in simple html syntax ) describing the experiments.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n# Project name, will be the name on the web page\nPROJECT=monitor\n\n# Explanation on webpage\nHELP=\"Observation verification comparison between \\\n      <br> FMI(RCR) \\\n      <br> SMHI(C22) \\\n     \"","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"By having several configuration files with different PROJECT names you can gather all your verification plots under one web page. In next section we define the verification period. We can also say if we would like to verify the full period in one go or in monthly pieces by setting PERIOD_TYPE.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"# Date handling\n# PERIOD_TYPE 1 : SDATE - EDATE,\n#             2 : SDATE - EDATE in monthly pieces\n#\n# IDATE is the very first date for PERIOD_TYPE=2 it determines the\n# lentgh of the date menu in WebgraF\n#\nPERIOD_TYPE=1\n\nSDATE=20080901\nEDATE=20080905\nIDATE=$SDATE","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"For operational runs it might be useful to set PERIOD_TYPE=2 like FMI has done.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"If you would like to monitor some special stations you can list them by station number.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n#\n# Single stations can be defined with comma separated\n# station number and a text for the web page\n#\n# STNLIST_PLOT=\"00002574,00006348\"\n# STNLIST_PLOT_TXT=\"NORRKOPING,CABAUW\"\n#\nSTNLIST_PLOT=-1\nSTNLIST_PLOT_TXT=-1\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Note that the zeros in the name matters since the plots are created with station id as eight digit numbers. A list of parameters to be verified are selected through SURFPAR and TEMPPAR respectively. ","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n######################\n# Surface parameters #\n######################\n#\n# Change in the file plotdefs.pm for text and limits\n#\n# PS : Mslp\n# TT : T2m\n# TTHA : T2m, adjusted for model and observation station height differences\n# TN : Min T2m\n# TX : Max T2m\n# TD : Td2m\n# FF : Wind speed\n# FX : Max wind speed\n# GG : Wind gust\n# GX : Max wind gust\n# DD : Wind direction\n# QQ : Specific humidity\n# RH : Relative humidity\n# PE : Precipitation\n# NN : Total Cloud cover\n# VI : Visibility, not in vfld files yet\n#\n\n# Active parameters\nSURFPAR=\"PS FF FX GG GX DD TT TN TX TD RH QQ NN PE\"\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Note that not all parameters are available in the vfld files for HARMONIE yet. The number of levels to be used for TEMP is set in LEV_LST.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Comment in the code fldextr_pp.f explains TTHA, the moist adiabatic adjustment:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"c adiabatic height correction of station values\nc T2M_corr=T2M+[(STATION_HEIGHT-MODEL_HEIGHT)*ADIABATIC_LAPSE_RATE]\nc ex: STATION_HEIGHT = 400 masl; MODEL_HEIGHT = 500 masl T2M = 10\nc     T2M_corr=10+(400-500)*(-0.0065)=10+0.6=10.6","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"By setting SURFPLOT and TEMPPLOT we choose what kind of statistics we would like to produce. ","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"# Things to plot:\n# GEN    : General forcast length statistics\n# TIME   : Timeserie statistics\n# SCAT   : Scatterplot\n# MAP    : Bias maps\n# FREQ   : Frequency plots\n# DAYVAR : Daily variation\n# XML    : Station statistics in xml format\n# CONT   : Contingency tables\n# VERT   : Vertical profiles only available for TEMP data\n# SEAS   : Seasonal cycle\n#\nSURFPLOT=\"GEN TIME MAP FREQ SCAT CONT XML DAYVAR\"","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"From the contingency tables we are also able to produce a number of skill scores either defined by their classes or thresholds.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"# Select skill scores to be plotted if CONT is activated in SURFPLOT\n# Frequency     : Frequency\n# Frequencybias : Frequency bias\n# POD           : Probability of detection ( hit rate )\n# FAR           : False alarm ratio\n# FA            : False alarm rate\n# TS            : Threath score\n# WILSON        : Wilson diagram, a combination of POD, TS, FAR and frequency bias\n# KSS           : Hansen-Kupiers skill score\n# AI            : Area index\n# EDS           : Extreme Dependency Score\n# SEDS          : Symmetric Extreme Dependency Score\n# EDI           : Extremal Dependency Index\n# SEDI          : Symmetric Extremal Dependency Index\n# ETS           : Equitable threat score\n\nSCORELIST=\"WILSON KSS Frequency\"\n\n# Select whether skill scores are based on classes and/or thresholds (CONT must be activated)\nSCORETYPES=\"classes thresholds\"\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The meaning of the different abbreviations will be given in next section.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"A selection of data is done by SURFSELECTION and TEMPSELECTION. The name in these list refers to definitions in monitor/scr/selection.pm. We also select time and forecast interval by the OBINT, FCINT and FCLEN parameters.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"At the end you would possibly like to change the graphics format of the output files.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n#####################\n# GRAPHICS and misc #\n#####################\n\n# Select output_type\n# 1  Postscript + PNG\n# 2  PNG\n# 3  JPEG\n# 4  SVG\nOUTPUT_TYPE=2\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The difference between the first OUTPUT_TYPE and the others is that in the first case gnuplot will produce postscript files that will be converted to PNG files and both files will be available on the web page. The SVG format (Scalable Vector Graphics) should allow plots with zoom functionality, but this does not yet work in the web interface. ","category":"page"},{"location":"Verification/Verification/#Setting-parameters-for-different-types-of-plots-1","page":"Verification","title":"Setting parameters for different types of plots","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"If the settings in the main configuration file does not cover your needs you go to next level of the definition files. The namelists defining your verification run is build by monitor/scr/Build_namelist.pl by using your configuration file and three perl modules defining different parts. The logics behind the GEN, MAP, TIME switches are hidden in monitor/scr/maindefs.pm. The first part defines the reading part:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n%nameread=(\n 'read_section' => {\n    'SDATE'   => $ENV{SDATE},\n    'EDATE'   => $ENV{EDATE},\n    'NEXP'    => $nexp,\n    'EXPNAME' => $exp,\n    'MODPATH' => $modpath,\n    'OBSPATH' => '\\''.$ENV{OBSPATH}.'\\'',\n    'LQUALITY_CONTROL' => 'T',\n    'ESTIMATE_QC_LIMIT'=> 'T',\n    'MAXSTN'           => 5000,\n    'STNLIST'          => 0,\n    'STNLIST_PLOT'     => $ENV{STNLIST_PLOT},\n    'LVERIFY'          => 'F',\n    'PRINT_READ'       => 1,\n    'PERIOD_TYPE'      => $ENV{PERIOD_TYPE},\n    'OUTPUT_TYPE'      => $ENV{OUTPUT_TYPE},\n    'OUTPUT_MODE'      => 2,\n },\n) ;\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Any new variable added here will also be set in the reading part of the namelist. The next part, def, defines values that are reset every time we loop through the verification and reads a new namelist. In the selectionloop part  we find the magic switches for the different SURFPLOT/TEMPPLOT keywords. The first one SEAS is only interesting if you run with a few parameters over several seasons. In the normal case several years of data doesn't fit in the memory so this is left for the experienced user.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The next one is GEN:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n 'GEN' => {\n # Fclen plots\n 'LSTAT_GEN'  => 'T',\n 'LSIGN_TEST' => 'T',\n 'SIGN_TIME_DIFF' => '-1',\n 'LPLOT_STAT' => 'T',\n 'SHOW_BIAS' => 'T',\n 'SHOW_RMSE' => 'T',\n 'SHOW_VAR'  => 'T',\n },\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Here we set things for standard forecast length verification. We can choose to show bias, rmse and stdv by the SHOW_* variable. If SHOW_VAR is set plots comparing the model variability to the observed one will be produced. The significance of the difference between different experiments can be shown by setting LSIGN_TEST.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The next section handles the production of bias maps","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n 'MAP' => {\n # Map plots\n 'PLOT_BIAS_MAP' => 'T',\n 'PLOT_RMSE_MAP' => 'T',\n 'LSTAT_GEN'     => 'T',\n 'LPLOT_STAT'    => 'F',\n 'LFCVER'        => 'F',\n 'SHOW_TIMES'    => '00,12',\n 'USE_FCLEN' => join(',',split(' ',$ENV{FCLEN_MAP})),\n },\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Here we can show decide to show any of bias,rmse and stdv maps. The bias intervals for a given parameter are defined in monitor/scr/plotdefs.pm discussed later. Here we have chosen to show only the 00 and 12 UTC maps.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Time serie statistics of the observed values and departures are produced by the TIME section.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n 'TIME' => {\n  # Timeseries\n  'LTIMESERIE_STAT'=> 'T',\n  'USE_FCLEN' => join(',',split(' ',$ENV{FCLEN_TIME})),\n },\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Note that we explicitly set the forecast lengths we use. As for the GEN part the activation of bias, rmse and stdv plots are controlled by the SHOW_* parameters. The averaging period for time series are controlled per variable through the TWIND_SURF and TWIND_TEMP parameters in monitor/scr/plotdefs.pm.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Scatter plots and contingency tables are set in","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":" 'scat_ver' => {\n  # Scatterplots and freq,\n  'LPREP_XML' => 'T',\n  'LPLOT_FREQ'=> 'T',\n  'LPLOT_SCAT'=> 'T',\n  'USE_FCLEN' => join(',',split(' ',$ENV{FCLEN_SCAT})),\n },\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"By setting LPREP_XML we will get a list of stations sorted by decreasing rmse on the web page. This allows you to find the worst stations for different variables. The contingency part of this is defined in monitor/scr/plotdefs.pm. It is possible to create cross variable scatter plots where we compare different model parameters against each other or the observations. This is however not a part of the script system but be defined on the low level. Read more` in here.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"In some cases it's interesting to see how the model handles the daily cycle. In DAYVAR we define the flags to get this. The LFCVER tell the program that we should organize the statistics by time of day rather than by forecast length. We have also chosen to allow for a special set of forecast length here through the environment variable FCLEN_DAYVAR set in your configuration file","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n 'DAYVAR' => {\n  # Daily variation\n  'LPLOT_STAT' => 'T',\n  'LSTAT_GEN'  => 'T',\n  'LFCVER'     => 'F',\n  'USE_FCLEN'  => join(',',split(' ',$ENV{FCLEN_DAYVAR})),\n  'SHOW_OBS'   => 'T',\n  'SHOW_VAR'   => 'F',\n },\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The final part of monitor/scr/maindefs.pm deals with the vertical profiles. LPLOT_VERT is the flag telling us that we are doing a vertical profile. The major difference between this and GEN is that here we have chosen to split between night and daytime soundings by setting SHOW_TIMES.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Any valid namelist variable added to these sections will be picked up and used in the verification. ","category":"page"},{"location":"Verification/Verification/#Settings-for-different-meteorological-parameters-1","page":"Verification","title":"Settings for different meteorological parameters","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The different treatment of the different meteorological variables are done in monitor/scr/plotdefs.pm. In the first section we define the default values for all variables.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n 'def'=>{\n   'TWIND_SURF' => 06,\n   'TWIND_TEMP' => 12,\n   'QC_LIM_SCALE' => 5.,\n   'MAP_BIAS_INTERVAL'=> '7*-1',\n },\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The TWIND_* parameter sets the time averaging window for time series for surface data and temp respectively. QC_LIM_SCALE is the scaling factor for the stdv used in the quality control. The TEXT variables sets the text for the title in the plots and the web page. For e.g. the cloud cover we have defined the classes for the contingency tables by setting CONT_CLASS and CONT_LIM. MAP_BIAS_INTERVAL set, as the name indicates, the intervals for the bias maps.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n'NN'=>{\n   'TEXT'        => 'Cloud cover',\n   'CONT_CLASS'  => 7,\n   'CONT_LIM'    => '1.,2.,3.,4.,5.,6.,7.',\n   'PRE_FCLA'    => '1.,2.,3.,4.,5.,6.,7.',\n   'MAP_BIAS_INTERVAL'=> '-6.,-4.,-2.,0.,2.,4.,6.',\n },\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"It is also possible to set the time window for timeseries separately for each variables like it is done for e.g. precipitation (PE). For accumulated and max/min parameters we also need to set the accumulation period. E.g. the maximum temperature for the past 12 hours is defined as","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n 'TX'=>{\n   'TWIND_SURF'  => 12,\n   'TEXT'        => 'Max T2m',\n   'MAP_BIAS_INTERVAL'=> '-6.,-4.,-2.,0.,2.,4.,6.',\n   'ACC'         => 12,\n   'ACCTYPE'     => 3,\n },\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Where ACCTYPE defines if it's an accumulated (1), minimum (2) or maximum (3)  parameter.","category":"page"},{"location":"Verification/Verification/#Defining-a-new-verification-parameter-1","page":"Verification","title":"Defining a new verification parameter","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"As described in the [#Ageneralinputformat input format] section you can add any variable to the verification. Let's say you would like to verify precipiation accunulated over three hours and that you have called it PE3H in your data files.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n 'PE3H'=>{\n   'TEXT' => 'Precipitation 3h',\n   'ACC'  => 3,\n   'UNIT' => 'mm/3h',\n },\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Where TEXT is the description to be displayed on the plot and the webpage, ACC is the accumulation period in hours and UNIT is the unit to be written to the plot. Of course the above mentioned properties can be set as well. Remember to add PE3H to the SURFPAR list in your definition file.","category":"page"},{"location":"Verification/Verification/#Selection-options-1","page":"Verification","title":"Selection options","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"It easy to select a subset of your data for verification and we have already discussed how it can be done by setting a list of stations or select different forecast lengths. In monitor/selection.pm a number of different kind of selections have been defined. Several of them are just a list of stations like e.g. the well known (but perhaps not so well defined) EWGLAM list. An example of how a box can be defined is found for the Netherlands.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n 'Netherland' => {\n   'CBOX%ACTIVE' => 'T',\n   'CBOX%SLAT' => '51.',\n   'CBOX%WLON' => '1.5',\n   'CBOX%NLAT' => '54.5',\n   'CBOX%ELON' => '9.',\n   }\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"We may also define our area of selection through a polygon.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n 'BalticSea' => {\n   'STNLIST'=> 0,\n   'LPOLY'               => 'T',\n   'POLYFILE'            => '\\'Baltic_sea.poly\\'',\n   },\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"A more specialized case is the selection of stations by station height where the upper and lower limits are given.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n  'masl_300m' => {\n   'STNLIST'=> 0,\n   'LSTN_HGT_CHECK'=>'.T.',\n   'HGT_ULIM'=>1.e6,\n   'HGT_LLIM'=>300.,\n   },\n","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Finally we can also do the selection based on meteorological criteria. In the example below we are interested in cases with low temperatures and weak winds.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"\n  'temp_and_wind_limit' => {\n    'COND%IND' => 'FF,TT',\n    'COND%ULIM' => ' 5.,-10.',\n    'COND%LLIM' => '-1.,-55.',\n    'COND%LOBS' => 'T,F',\n    'COND%ALL_MOD' => 'T,F',\n  },","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Example of conditional selection","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"COND%IND sets the parameters\nCOND%LLIM sets the lower limits\nCOND%ULIM sets the upper limits\nCOND%LOBS T means apply condition on observations, F applies condition on model data\nCOND%ALL_MOD, T means condition is required for ALL models, F for ANY model","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"All the above mentioned selections can of course be combined in any way you can imagine.","category":"page"},{"location":"Verification/Verification/#WebgraF-1","page":"Verification","title":"WebgraF","text":"","category":"section"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"One idea with the HARMONIE verification packages is that it should be easy to share you results with others. This is where WebgraF comes in. It was originally written to mimic the ECMWF \"chart\" facility like here. The ECMWF solution is a perl based server solution and needs some installation and WebgraF is a javascript running locally which makes it more portable. The idea with WebgraF is that each page is defined by a simple definition file which spans the space of the menu axes on the page. ","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Examples :       * GLAMEPS Definition file       * Daily maps Definition file","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"At the end of both of the scripts Run_verobs_surface/Run_verobs_temp there is a call to monitor/scr/Create_ver_js.pl that builds the webpage depending on your configuration file. It is also possible to (re)generate the webpage directly by running:","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Create_ver_js YOUR_CONFIG_FILE","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The WebgraF page is controlled by monitor/WebgraF/bin/WebgraF. It has commands to e.g. list, add, remove the content of a page. To start mastering your own page you first have to let the script know the location of the page by setting the environment variable WEBGRAF_BASE ","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"# in bash\nexport WEBGRAF_BASE=SOME_PATH/monitor/WebgraF\n# or in tcsh\nsetenv WEBGRAF_BASE SOME_PATH/monitor/WebgraF","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Now you can list the content of you page by","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"WebgraF/bin/WebgraF -l ","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"A more comprehensive list of commands can be found in monitor/doc/README_WebgraF. The rules and functions available for your definition file is found monitor/WebgraF/src/input.html.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Two useful tools is the export and transport commands. Both creates an portable extraction of your verification page but in two different ways.","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"The export.tar file is a stand alone web page that you can untar anywhere and open in your browser.\nThe transport.tar file is suitable to add to an already existing WebgraF page by  WebgraF -a TARFILE","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Both are accessible through the script monitor/scr/Transport_ver which is used like","category":"page"},{"location":"Verification/Verification/#","page":"Verification","title":"Verification","text":"Transport_ver YOUR_CONFIG_FILE","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Experiment-configuration-1","page":"Experiment","title":"Experiment configuration","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Introduction-1","page":"Experiment","title":"Introduction","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"There are several levels on configuration available in HARMONIE. The highest level of configuration is done in ecf/config_exp.h. It includes the environment variables, which are used to control the experimentation. In the following we describe the meaning of the different variables and are described in the order they appear in ecf/config_exp.h.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Host specific paths and environment variables for your system are defined in Env_system. Read more here.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Build-options-1","page":"Experiment","title":"Build options","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** Build and bin paths ****\n# Definitions about Build, should fit with hm_rev\nBUILD=${BUILD-yes}                     # Turn on or off the compilation and binary build (yes|no)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"BUILD is a switch for compiling HARMONIE code (yes|no).","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"BINDIR=${BINDIR-$HM_DATA/bin}                 # Binary directory","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"BINDIR is the location of where your HARMONIE binaries will be installed. You can use this to point to binaries outside of your experiment. A few other options for non default configurations exists as well:","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"COMPILE_ENKF=${COMPILE_ENKF-\"no\"}             # Compile LETKF code (yes|no)\nCOMPILE_DABYFA=${COMPILE_DABYFA-\"no\"}         # Compile FA/VC code (yes|no)\nSURFEX_OFFLINE_BINARIES=\"no\"                  # Switch to compile and use offline SURFEX binaries","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#General-settings-1","page":"Experiment","title":"General settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** Misc, defined first because it's used later ****\n\nCNMEXP=HARM                             # Four character experiment identifier\nWRK=$HM_DATA/$CYCLEDIR                  # Work directory","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"CNMEXP: experiment identifier used by MASTERODB\nWRK is the work directory. The suggested path on cca is $SCRATCH/hm_home/${EXP}/$CYCLEDIR","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Archive-settings-(ECMWF)-1","page":"Experiment","title":"Archive settings (ECMWF)","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Since $SCRATCH is cleaned regularly on cca and ecgb some files are transferred to ECFS for a more permanent storage by the scripts scr/Archive_host1 and scr/Archive_ecgb. ","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** Paths to archive ****\n# We need to define ARCHIVE early since it might be used further down\n\nARCHIVE_ROOT=$HM_DATA/archive           # Archive root directory\nECFSLOC=ectmp                           # Archiving site at ECMWF-ECFS: \"ec\" or ECFS-TMP \"ectmp\"\nECFSGROUP=hirald                        # Group in which to chgrp the ECMWF archive, \"default\" or \"hirald\"\nEXTRARCH=$ARCHIVE_ROOT/extract          # Archive for fld/obs-extractions","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"ARCHIVE_ROOT is the path to forecast file archive. Note that at ECMWF this directory is not a permanent storage\nEXTRARCH is the path to field extraction archive. Note that at ECMWF this directory is not a permanent storage\nECFSLOC Archiving site at ECMWF-ECFS  (ectmp|ec) Note that files archived on ectmp will be lost after 90 days. If you wish your files to stay longer you should set ECFSLOC=ec. \nECFSGROUP Group in which to chgrp the ECMWF archive, (hirald|default)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Running-Mode-1","page":"Experiment","title":"Running Mode","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** Running mode ****\nRUNNING_MODE=research                   # Research or operational mode (research|operational)\n                                        # operational implies that the suite will continue even if e.g.\n                                        # observations are missing or assimilation fails\n\nSIMULATION_TYPE=nwp                     # Type of simulation (nwp|climate)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"RUNNING_MODE can be research or operational. Operational is more forgiving in the error handling and e.g. the assimilation will be skipped if Bator doesn't find any observations. Exceptions handled by the operational mode are written to $HM_DATA/severe_warnings.txt\nSIMULATION_TYPE Switch between nwp and climate type of simulation. The climate simulations are still in an experimental stage. ","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Model-domain-settings-1","page":"Experiment","title":"Model domain settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Horizontal domain settings. Further information is available here","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"\n# **** Model geometry ****\nDOMAIN=DKCOEXP                          # See definitions in scr/Harmonie_domains.pm\nTOPO_SOURCE=gmted2010                   # Input source for orography. Available are (gmted2010|gtopo30)\nGRID_TYPE=LINEAR                        # Type of grid (LINEAR|QUADRATIC|CUBIC)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"DOMAIN defines your domain according to the settings in scr/Harmonie_domains.pm (DKCOEXP). The spectral truncation for your domain is determined from NLON and NLAT by scr/Harmonie_domains.pm. Further information on model domains are available here\nTOPO_SOURCE: Defines input source for model orography (gmted2010|gtopo30). Further information available here: hi-res topography\nGRID_TYPE: This variable is used to define the spectral truncation used (LINEAR|QUADRATIC|CUBIC). GRID_TYPE is used in scr/Climate and scr/Forecast","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Vertical-levels-1","page":"Experiment","title":"Vertical levels","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Set the number vertical levels to use. Further information is available here","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"VLEV=65                                 # Vertical level definition name\n                                        # HIRLAM_60, MF_60,HIRLAM_40, or\n                                        # BOUNDARIES = same number of levs as on boundary file.\n                                        # See the other choices from scr/Vertical_levels.pl","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"VLEV is the name of the vertical levels defined in scr/Vertical_levels.pl (65). Further information is available here. If you intend to run upper air assimilation you must select the same domain and level definition for which you have derived structure functions. Read more Structure Functions","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Forecast-model-1","page":"Experiment","title":"Forecast model","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Higher level forecast model settings.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** High level forecast options ****\nNAMELIST_BASE=\"harmonie\"                # Input for namelist generation (harmonie|alaro1)\n                                        #   harmonie : The default HARMONIE namelist base nam/harmonie_namelists.pm\n                                        #   alaro1   : For ALARO-1 baseline with only a few configurations available\n                                        #              nam/alaro1_namelists.pm\nDYNAMICS=\"nh\"                           # Hydrostatic or non-hydrostatic dynamics (h|nh)\nVERT_DISC=vfd                           # Discretization in the vertical (vfd,vfe)\n                                        # Note that vfe does not yet work in non-hydrostatic mode\nPHYSICS=\"arome\"                         # Main model physics flag (arome|alaro)\nSURFACE=\"surfex\"                        # Surface flag (old_surface|surfex)\nDFI=\"none\"                              # Digital filter initialization (idfi|fdfi|none)\n                                        # idfi : Incremental dfi\n                                        # fdfi : Full dfi\n                                        # none : No initialization (AROME case)\nLSPBDC=no                               # Spectral boundary contions option off(no) | on(yes)\nLGRADSP=yes                             # Apply Wedi/Hortal vorticity dealiasing\nLUNBC=yes                               # Apply upper nested boundary condition","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"NAMELIST_BASE: Two different namelist sets are available (harmonie|alaro).\nDYNAMICS: Hydrostatic or non-hydrostatic dynamics (h|nh)\nVERT_DISC: Vertical discretization (vfd,vfe)\nPHYSICS: HARMONIE uses either AROME or ALARO for its forecast model physics (arome|alaro)\nSURFACE: Surface physics flag to use either the SURFEX or the ALADIN surface scheme(surfex|old_surface)\nDFI: Digital filter initialization switch (idfi|fdfi|none). idfi - incremental dfi, fdfi - full dfi, none - no initialization. See Digital filter for more information\nLSPBDC: Specify whether the boundary conditions are spectral or not (yes|no)\nLGRADSP: Switch to apply vorticity dealiasing (yes|no)\nLUNBC: Switch to apply upper boundary conditions (yes|no)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Physics-1","page":"Experiment","title":"Physics","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Physics options.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# Highlighted physics switches\nCISBA=\"3-L\"                             # Type of ISBA scheme in SURFEX. Options: \"3-L\" and \"2-L\".\nCROUGH=\"NONE\"                           # SSO scheme used in SURFEX \"NONE\"|\"'Z01D'\"|\"'BE04'\"\nSURFEX_SEA_ICE=\"none\"                   # Treatment of sea ice in surfex (none|sice)\nMASS_FLUX_SCHEME=edmfm                  # Version of EDMF scheme (edkf|edmfm)\n                                        # Only applicable if PHYSICS=arome\n                                        # edkf is the AROME-MF version\n                                        # edmfm is the KNMI implementation of Eddy Diffusivity Mass Flux scheme for Meso-scale\nHARATU=\"yes\"                            # Switch for HARATU turbulence scheme (no|yes)\nALARO_VERSION=0                         # Alaro version (1|0)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"CISBA: If SURFACE is set to surfex this selects the type of ISBA scheme to use in SURFEX. (3-L|2-L). See src/surfex_namelists.pm  Namelists\nCROUGH: If SURFACE is set to surfex this selects the sub-grid scale orography scheme used in SURFEX. (NONE|Z01D|BE04). See src/surfex_namelists.pm  Namelist\nSURFEX_SEA_ICE: Treatment of sea ice in surfex (none|sice). See nam/surfex_namelists.pm\nMASS_FLUX_SCHEME: If PHYSICS is set to arome choose the mass flux scheme to be used by AROME; edkf to use the AROME-MF scheme or edmfm to use the KNMI developed scheme\nHARATU: Switch to use the HARATU turbulence scheme\nALARO_VERSION: If PHYSICS is set to alaro select version of ALARO to use (0|1)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Assimilation-1","page":"Experiment","title":"Assimilation","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Data assimilation settings. More assimilation related settings, in particular what observations to assimilate, can be found in src/include.ass","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** Assimilation ****\nANAATMO=3DVAR                           # Atmospheric analysis (3DVAR|4DVAR|blending|none)\nANASURF=CANARI_OI_MAIN                  # Surface analysis (CANARI|CANARI_OI_MAIN|CANARI_EKF_SURFEX|none)\n                                        # CANARI            : Old style CANARI\n                                        # CANARI_OI_MAIN    : CANARI + SURFEX OI\n                                        # CANARI_EKF_SURFEX : CANARI + SURFEX EKF ( experimental )\n                                        # none              : No surface assimilation\nANASURF_MODE=\"before\"                   # When ANASURF should be done\n                                        # before            : Before ANAATMO\n                                        # after             : After ANAATMO\n                                        # both              : Before and after ANAATMO (Only for ANAATMO=4DVAR)\nINCV=\"1,1,1,1\"                          # Active EKF control variables. 1=WG2 2=WG1 3=TG2 4=TG1\nINCO=\"1,1,0\"                            # Active EKF observation types (Element 1=T2m, element 2=RH2m and element 3=Soil moisture) \n\nMAKEODB2=no                             # Conversion of ODB-1 to ODB-2 using odb_migrator\n\nSST=BOUNDARY                            # Which SST fields to be used in surface analysis\n                                        # BOUNDARY          : SST interpolated from the boundary file. ECMWF boundaries utilize a special method.\n                                        #                     HIRLAM and HARMONIE boundaries applies T0M which should be SST over sea.\nLSMIXBC=no                              # Spectral mixing of LBC0 file before assimilation\n[ \"$ANAATMO\" = 3DVAR] && LSMIXBC=yes\nJB_INTERPOL=no                          # Interpolation of structure functions from a pre-defined domain to your domain\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"ANAATMO: Atmospheric analysis (3DVAR|4DVAR|blending|none)\nANASURF: Surface analysis (CANARI|CANARIOIMAIN|CANARIEKFSURFEX|none). See nam/surfex_namelists.pm\nANASURF_MODE:When the surface should be called (before|after|both)\nINCV: Active EKF control variables. 1=WG2 2=WG1 3=TG2 4=TG1 (0|1)\nINCO: Active EKF observation types (Element 1=T2m, element 2=RH2m and element 3=Soil moisture) (0|1)\nMAKEODB2: Option to convert ODB-1 databases to ODB-2 files for DA monitoring\nSST: which sea surface temperature field to use in the surface analysis\nLSMIXBC Spectral mixing of LBC0 file before assimilation (no|yes)\nJB_INTERPOL Interpolation of structure functions from a pre-defined domain to your domain (no|yes). Note that this has to be used with some caution.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Observations-1","page":"Experiment","title":"Observations","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** Observations ****\nOBDIR=$HM_DATA/observations             # Observation file directory\nRADARDIR=$HM_DATA/radardata             # Radar observation file directory\nSINGLEOBS=no                            # Run single obs experiment with observation created by scr/Create_single_obs (no|yes)\n\nUSE_MSG=no                              # Use MSG data for adjustment of inital profiles, EXPERIMENTAL! (no|yes)\nMSG_PATH=$SCRATCH/CLOUDS/               # Location of input MSG FA file, expected name is MSGcloudYYYYMMDDHH","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"OBDIR: Defines the directory that your (BUFR) observation files (obYYYYMMDDHH) are to read from\nRADARDIR: Defines the directory that your (OPERA HDF5) radar observation files are to be read from. BALTRAD OPERA HDF5, MF BUFR and LOCAL files are treated in scr/Prepradar\nSINGLEOBS Run single obs experiment with synthetic observation created by scr/Create_single_obs scr/Create_single_obs (no|yes)\nUSE_MSG: Use MSG data for adjustment of inital profiles, EXPERIMENTAL! (no|yes)\nMSG_PATH:  Location of input MSG FA file, expected name is MSGcloudYYYYMMDDHH. Note that the pre-processing software to generate input files is not yet included in HARMONIE","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#DVAR-settings-1","page":"Experiment","title":"4DVAR settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"4DVAR settings (experimental)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** 4DVAR ****\nNOUTERLOOP=1                            # 4DVAR outer loops, need to be 1 at present\nILRES=2,2                               # Resolution (in parts of full) of outer loops\nTSTEP4D=360,360                         # Timestep length (seconds) of outer loops TL+AD\nTL_TEST=yes                             # Only active for playfile tlad_tests\nAD_TEST=yes                             # Only active for playfile tlad_tests","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"NOUTERLOOP: Number of outer loops, need to be 1 at present\nILRES:  Resolution (in parts of full) of outer loops\nTSTEP4D: Timestep length (seconds) of outer loops TL+AD\nTL_TEST: Only active for playfile tlad_tests (yes|no)\nAD_TEST: Only active for playfile tlad_tests (yes|no)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Digital-filter-settings-1","page":"Experiment","title":"Digital filter settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Digital filter initialization settings if DFI is not equal to \"none\"","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** DFI setting ****\nTAUS=5400                               # cut-off frequency in second\nTSPAN=5400                              # 7200s or 5400s","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"TAUS cut-off frequency in seconds \nTSPAN length of DFI run in seconds","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Boundaries-and-initial-conditions-1","page":"Experiment","title":"Boundaries and initial conditions","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Settings for generation of lateral boundaries conditions for HARMONIE. Further information is available here","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** Lateral boundary conditions ****\nHOST_MODEL=\"ifs\"                        # Host model (ifs|hir|ald|ala|aro)\n                                        # ifs : ecmwf data\n                                        # hir : hirlam data\n                                        # ald : Output from aladin physics\n                                        # ala : Output from alaro physics\n                                        # aro : Output from arome physics\n\nHOST_SURFEX=\"no\"                        # yes if the host model is run with SURFEX\nSURFEX_INPUT_FORMAT=lfi                 # Input format for host model run with surfex (lfi|fa)\n\nNBDMAX=12                               # Number of parallel interpolation tasks\nBDLIB=ECMWF                             # Boundary experiment, set:\n                                        # ECMWF to use MARS data\n                                        # RCRa  to use RCRa data from ECFS\n                                        # Other HARMONIE/HIRLAM experiment\n\nBDDIR=$HM_DATA/${BDLIB}/archive/@YYYY@/@MM@/@DD@/@HH@   # Boundary file directory,\n                                                        # For more information, read in scr/Boundary_strategy.pl\nINT_BDFILE=$WRK/ELSCF${CNMEXP}ALBC@NNN@                 # Interpolated boundary file name and location\n\nBDSTRATEGY=simulate_operational # Which boundary strategy to follow\n                                # as defined in scr/Boundary_strategy.pl\n                                #\n                                # available            : Search for available files in BDDIR, try to keep forecast consistency\n                                #                        This is ment to be used operationally\n                                # simulate_operational : Mimic the behaviour of the operational runs using ECMWF LBC,\n                                #                        i.e. 6 hour old boundaries\n                                # same_forecast        : Use all boundaries from the same forecast, start from analysis\n                                # analysis_only        : Use only analysises as boundaries\n                                # era                  : As for analysis_only but using ERA interim data\n                                # latest               : Use the latest possible boundary with the shortest forecast length\n                                # RCR_operational      : Mimic the behaviour of the RCR runs, ie\n                                #                        12h old boundaries at 00 and 12 and\n                                #                        06h old boundaries at 06 and 18\n                                # enda                 : use ECMWF ENDA data for running ensemble data assimilation\n                                #                        or generation of background statistic.\n                                #                        Note that only LL up to 9h is supported\n                                #                        with this you should set your ENSMSEL members\n                                # eps_ec               : ECMWF EPS members (on reduced gaussian grid)\n                                #                      : Only meaningful with ENSMSEL non-empty, i.e., ENSSIZE > 0\n\nBDINT=1                         # Boundary interval in hours\n\nSURFEX_PREP=\"yes\"                # Use offline surfex prep facility (Alt. gl + Fullpos + prep )","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"HOST_MODEL defines the host model that provides the lateral boundaries conditions for your experiment\nhir for HIRLAM.\nald for ALADIN \nala for ALARO\naro for AROME\nifs for ECMWF-IFS. \nHOST_SURFEX Set to yes if host model runs with SURFEX. (no|yes)\nSURFEX_INPUT_FORMAT Input format for host model run with surfex (lfi|fa)\nBDLIB is the experiment to be used as boundaries. Possible values, ECMWF for IFS from MARS (default), RCRa for HIRLAM-RCR from ECFS or other HARMONIE experiment. \nBDDIR is the boundary file directory. The possible date information in the path must be given by using UPPER CASE letters (@YYYY@=year,@MM@=month,@DD@=day,@HH@=hour,@FFF@=forecast length).  \nBDSTRATEGY Which boundary strategy to follow i.e. How to find the right boundaries with the right age and location. Read more\nBDINT is boundary interval in hours.\nBDCLIM is the path to climate files corresponding the boundary files, when nesting HARMONIE to HARMONIE.\nINT_BDFILE is the name and location of the interpolated boundary files. These files are removed every cycle, but if you wish to save them you can specify a more permanent location here. By setting INT_BDFILE=ARCHIVE the interpolated files will be stored in your archive directory.\nNBDMAX Number of parallel boundary interpolation tasks in mSMS. The current default value is 12.\nSURFEX_PREP Use SURFEX tool PREP instead of gl+FULLPOS to prepare SURFEX initial conditions. This is now the default. The gl+FULLPOS version is still working but will not be maintained in the future (no|yes)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Read more about the boundary file preparation here.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Ensemble-mode-settings-1","page":"Experiment","title":"Ensemble mode settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# *** Ensemble mode general settings. ***\n# *** For member specific settings use msms/harmonie.pm ***\nENSMSEL=                                # Ensemble member selection, comma separated list, and/or range(s):\n                                        # m1,m2,m3-m4,m5-m6:step    mb-me == mb-me:1 == mb,mb+1,mb+2,...,me\n                                        # 0=control. ENSMFIRST, ENSMLAST, ENSSIZE derived automatically from ENSMSEL.\nENSINIPERT=                             # Ensemble perturbation method (bnd). Not yet implemented: etkf, hmsv.\nENSCTL=                                 # Which member is my control member? Needed for ENSINIPERT=bnd. See harmonie.pm.\nENSBDMBR=                               # Which host member is used for my boundaries? Use harmonie.pm to set.\nENSMFAIL=0                              # Failure tolerance for all members.\nENSMDAFAIL=0                            # Failure tolerance for members doing own DA. Not implemented.\nSLAFK=1.0                               # best set in harmonie.pm\nSLAFLAG=0                               # --- \" ---\nSLAFDIFF=0                              # --- \" ---\n\n# *** This part is for EDA with observations perturbation\nPERTATMO=none                           # ECMAIN  : In-line observation perturbation using the default IFS way.\n                            \t\t\t# CCMA    : Perturbation of the active observations only (CCMA content)\n\t                            \t\t#           before the Minimization, using the PERTCMA executable.\n                            \t\t\t# none    : no perturbation of upper-air observations\n\nPERTSURF=none                           # ECMA    : perturb also the surface observation before Canari (recommended\n                            \t\t\t#         : for EDA to have full perturbation of the initial state).\n                                        # model   : perturb surface fields in grid-point space (recursive filter)\n\t\t\t                            # none    : no perturbation for surface observations.\n\nFESTAT=no                               # Extract differences and do Jb calculations (no|yes)\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"ENSMSEL  Ensemble member selection, comma separated list, and/or range(s):\n # m1,m2,m3-m4,m5-m6:step    mb-me == mb-me:1 == mb,mb+1,mb+2,...,me\n  # 0=control. ENSMFIRST, ENSMLAST, ENSSIZE derived automatically from ENSMSEL.\nENSINIPERT Ensemble perturbation method (bnd). Not yet implemented: etkf, hmsv, slaf.\nENSMFAIL Failure tolerance for all members. Not yet implemented.\nENSMDAFAIL Failure tolerance for members doing own DA. Not yet implemented.\nENSCTL Which member is my control member? Needed for ENSINIPERT=bnd. See harmonie.pm.\nENSBDMBR Which host member is used for my boundaries? Use harmonie.pm to set.\nSLAFK Perturbation coefficients for SLAF, experimental\nSLAFLAG Time lag for boundaries in SLAG, experimental","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"For member dependent settings see msms/harmonie.pm.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"PERTATMO Observation perturbation with three options \nECMA : In-line observation perturbation using the default IFS way.\nCCMA : Perturbation of the active observations only (CCMA content) before the Minimization, using the PERTCMA executable.\nnone : no perturbation of upper-air observations\nPERTSURF Perturbation of surface observations before Canari (recommended for EDA to have full perturbation of the initial state) (no|yes).","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"FESTAT Extract differences and do Jb calculations (no|yes). Read more about the procedure here.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Climate-file-settings-1","page":"Experiment","title":"Climate file settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Climate file generation settings. Further information is available here","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** Climate files ****\nCREATE_CLIMATE=${CREATE_CLIMATE-yes}    # Run climate generation (yes|no)\nCLIMDIR=$HM_DATA/climate                # Climate files directory\nBDCLIM=$HM_DATA/${BDLIB}/climate        # Boundary climate files (ald2ald,ald2aro)\n                                        # This should point to intermediate aladin \n                                        # climate file in case of hir2aro,ifs2aro processes.\n\n# Physiography input for SURFEX\nECOCLIMAP_VERSION=2.2                   # Version of ECOCLIMAP for surfex (1,2)\n                                        # Available versions are 1.1-1.5,2.0-2.2\nSOIL_TEXTURE_VERSION=FAO                # Soil texture input data FAO|HWSD_v2","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"CREATE_CLIMATE: Run climate generation (yes|no). If you already have a full set of climate files generated in CLIMDIR you can set this flag to no for a faster run.\nCLIMDIR: path to the generated climate files for your specific domain. The input data for the climate generation is defined by HM_CLDATA defined in Env_system -> config-sh/config.YOURHOST\nBDCLIM: path to intermediate climate files\nECOCLIMAP_VERSION is the version of ECOCLIMAP to be used with SURFEX. Available versions are 1.1-1.5,2.0,2.1,2.2. See surfex_namelists.pm Namelist\nSOIL_TEXTURE_VERSION Soil texture input data (FAO|HWSD_v2). See surfex_namelists.pm more info.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Archiving-settings-1","page":"Experiment","title":"Archiving settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** Archiving settings ****\nARCHIVE_ECMWF=yes                       # Archive to $ECFSLOC at ECMWF (yes|no)\n# Archiving selection syntax, settings done below\n#\n# [fc|an|pp]_[fa|gr|nc] : Output from\n#  an : All steps from upper air and surface analysis\n#  fc : Forecast model state files from upper air and surfex\n#  pp : Output from FULLPOS and SURFEX_LSELECT=yes (ICMSHSELE+NNNN.sfx)\n# in any of the formats if applicable\n#  fa : FA files\n#  gr : GRIB[1|2] files\n#  nc : NetCDF files\n# sqlite|odb|VARBC|bdstrategy : odb and sqlite files stored in odb_stuff.tar\n# fldver|ddh|vobs|vfld : fldver/ddh/vobs/vfld files\n# climate : Climate files from PGD and E923\n# Some macros\n# odb_stuff=odb:VARBC:bdstrategy:sqlite\n# verif=vobs:vfld\n# fg : Required files to run the next cycle","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Forecast-output-1","page":"Experiment","title":"Forecast output","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** Cycles to run, and their forecast length ****\n\nTFLAG=\"h\"                               # Time flag for model output. (h|min)\n                                        # h   = hour based output\n                                        # min = minute based output\n\n\n# The unit of HWRITUPTIMES, FULLFATIMES, ..., SFXFWFTIMES should be:\n#   - hours   if TFLAG=\"h\"\n#   - minutes if TFLAG=\"min\"\n\n# Writeup times of # history,surfex and fullpos files\n# Comma separated list, and/or range(s) like:\n# t1,t2,t3-t4,t5-t6:step    tb-te == tb-te:1 == tb,tb+1,tb+2,...,te\n\nif [ -z \"$ENSMSEL\"] ; then\n  # Standard deterministic run\n  HH_LIST=\"00-21:3\"                       # Which cycles to run, replaces FCINT\n  LL_LIST=\"12,3\"                          # Forecast lengths for the cycles [h], replaces LL, LLMAIN\n                                          # The LL_LIST list is wrapped around if necessary, to fit HH_LIST\n  HWRITUPTIMES=\"00-21:3,24-60:6\"          # History file output times\n  FULLFAFTIMES=$HWRITUPTIMES              # History FA file IO server gather times\n  PWRITUPTIMES=\"00-60:3\"                  # Postprocessing times\n  PFFULLWFTIMES=-1                        # Postprocessing FA file IO server gathering times\n  VERITIMES=\"00-60:1\"                     # Verification output times, may change PWRITUPTIMES\n  SFXSELTIMES=$HWRITUPTIMES               # Surfex select file output times\n                                          # Only meaningful if SURFEX_LSELECT=yes\n  SFXSWFTIMES=-1                          # SURFEX select FA file IO server gathering times\n  SWRITUPTIMES=\"00-06:3\"                  # Surfex model state output times\n  SFXWFTIMES=$SWRITUPTIMES                # SURFEX history FA file IO server gathering times\n  if [ \"$SIMULATION_TYPE\" == climate]; then  #Specific settings for climate simulations\n    HWRITUPTIMES=\"00-760:6\"                 # History file output times\n    FULLFAFTIMES=\"00-760:24\"                # History FA file IO server gather times\n    PWRITUPTIMES=$HWRITUPTIMES              # Postprocessing times\n    VERITIMES=$HWRITUPTIMES                 # Verification output times, may change PWRITUPTIMES\n    SFXSELTIMES=$HWRITUPTIMES               # Surfex select file output times - Only meaningful if SURFEX_LSELECT=yes\n    SWRITUPTIMES=\"00-760:12\"                # Surfex model state output times\n    SFXWFTIMES=$SWRITUPTIMES                # SURFEX history FA file IO server gathering times\n  fi\n\n  ARSTRATEGY=\"climate:fg:verif:odb_stuff: \\\n              [an|fc]_fa:pp_grb\"          # Files to archive on ECFS, see above for syntax\n\nelse\n  # EPS settings\n  HH_LIST=\"00-21:3\"                       # Which cycles to run, replaces FCINT\n  LL_LIST=\"36,3,3,3\"                      # Forecast lengths for the cycles [h], replaces LL, LLMAIN\n  HWRITUPTIMES=\"00-06:3\"                  # History file output times\n  FULLFAFTIMES=$HWRITUPTIMES              # History FA file IO server gather times\n  PWRITUPTIMES=\"00-48:1\"                  # Postprocessing times\n  PFFULLWFTIMES=-1                        # Postprocessing FA file IO server gathering times\n  VERITIMES=\"00-60:3\"                     # Verification output times, may change PWRITUPTIMES\n  SFXSELTIMES=$HWRITUPTIMES               # Surfex select file output times\n                                          # Only meaningful if SURFEX_LSELECT=yes\n  SFXSWFTIMES=-1                          # SURFEX select FA file IO server gathering times\n  SWRITUPTIMES=\"00-06:3\"                  # Surfex model state output times\n  SFXWFTIMES=$SWRITUPTIMES                # SURFEX history FA file IO server gathering times\n\n  ARSTRATEGY=\"climate:fg:verif:odb_stuff: \\\n              an_fa:pp_grb\"               # Files to archive on ECFS, see above for syntax\n\nfi\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"The writeup times of model output can be defined as a space separated list or as a fixed frequency for model history files, surfex files and postprocessed files respectively. The unit of the steps of WRITUPTIMES, SWRITUPTIMES, PWRITUPTIMES and OUTINT should be in hours or minutes depending on the TFLAG Regular output interval can be switched on by setting OUTINT>0. Consequently, OUTINT will override the WRITUPTIMES lists!","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"TFLAG: Time flag for model output. Hourly or minute-based output (h|min)\nHWRITUPTIMES:  Output list for history files. Default is 00-21:3,24-60:6 which will output files every 3 hours for 00-21 and every 6 hours for 24-60.\nVERITIMES:  Output list for verification files. Default is 00-60:1 which will produce file every 1 hour for 00-60\nSWRITUPTIMES  Output list for surfex files. Default is 00-06:3 which output a  SURFEX file every 3 hours for 00-06.\nPWRITUPTIMES  Output list for fullpos (post-processed) files. Default is 00-21:3,24-60:6 which will output files every 3 hours for 00-21 and every 6 hours for 24-60.","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"SURFEX_LSELECT=\"yes\"                    # Only write selected fields in surfex outpute files. (yes|no)\n                                        # Check nam/surfex_selected_output.pm for details.\n                                        # Not tested with lfi files.\nINT_SINI_FILE=$WRK/SURFXINI.fa          # Surfex initial file name and location\n\n# **** Postprocessing/output ****\nIO_SERVER=yes                           # Use IO server (yes|no). Set the number of cores to be used\n                                        # in your Env_submit\nIO_SERVER_BD=yes                        # Use IO server for reading of boundary data\nPOSTP=\"inline\"                          # Postprocessing by Fullpos (inline|offline|none).\n                                        # See Setup_postp.pl for selection of fields.\n                                        # inline: this is run inside of the forecast\n                                        # offline: this is run in parallel to the forecast in a separate task\n\nFREQ_RESET_TEMP=3                       # Reset frequency of max/min temperature values in hours, controls NRAZTS\nFREQ_RESET_GUST=1                       # Reset frequency of max/min gust values in hours, controls NXGSTPERIOD\n                                        # Set to -1 to get the same frequency _AND_ reset behaviour as for min/max temperature\n                                        # See yomxfu.F90 for further information.\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"SURFEX_LSELECT: Switch to write a selection of fields in SURFEX output files (yes|no). See surfex_selected_output.pm for more info. Namelist\nINT_SINI_FILE: name and location of the initial SURFEX file\nARCHIVE_ECMWF: archive files to ECFSLOC at ECMWF (yes|no)\nIO_SERVER: Use IO server (yes|no). If set to \"yes\" changes may be required in Env_submit -> config-sh/submit.YOURHOUST\nPOSTP: Postprocessing by Fullpos (inline|offline|none).\nFREQ_RESET_[TEMP|GUST]: Reset frequency of max/min values in hours, controls NRAZTS. Default is every 3/1 hours","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** GRIB ****\nCONVERTFA=yes                           # Conversion of FA file to GRIB/nc (yes|no)\nARCHIVE_FORMAT=GRIB1                    # Format of archive files (GRIB1|GRIB2|nc). nc format yet only available in climate mode\nNCNAMES=nwp                             # Nameing of NetCDF files follows (climate|nwp) convention.\nRCR_POSTP=no                            # Produce a subset of fields from the history file for RCR monitoring\n                                        # Only applicable if ARCHIVE_FORMAT=GRIB\nMAKEGRIB_LISTENERS=1                    # Number of parallel listeners for Makegrib\n                                        # Only applicable if ARCHIVE_FORMAT=GRIB\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"More options on fullpos postprocessing can be found in scr/Select_posp.pl","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"CONVERTFA: Conversion of FA files to GRIB or NetCDF (yes|no)\nARCHIVE_FORMAT: Format of archive files (GRIB1|nc). NetCDF format yet only available in climate mode\nRCR_POSTP: Produce a subset of fields from the history file for RCR monitoring (yes|no). This is only applicable if ARCHIVE_FORMAT=GRIB1|GRIB2\nMAKEGRIB_LISTENERS: Number of parallel listeners for Makegrib. Only applicable if ARCHIVE_FORMAT=GRIB1|GRIB2","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"More options on file conversion can be found in scr/Makegrib","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Verification-and-monitoring-1","page":"Experiment","title":"Verification and monitoring","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# **** Verification extraction ****\nOBSEXTR=yes                             # Extract observations from BUFR (yes|no)\nFLDEXTR=yes                             # Extract model data for verification from model files (yes|no)\nFLDEXTR_TASKS=1                         # Number of parallel tasks for field extraction\nVFLDEXP=$EXP                            # Experiment name on vfld files","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"OBSEXTR: Extract observations for verification from BUFR (yes|no)\nFLDEXTR: Extract model data for verification from model files (yes|no)\n*FLDEXTR_TASKS: Number of parallel tasks for field extraction\nVFLDEXP:","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Read more about the verification package here","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Field-verification-1","page":"Experiment","title":"Field verification","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# *** Field verification ***\nFLDVER=no                               # Main switch for field verification (yes|no)\nFLDVER_HOURS=\"06 12 18 24 30 36 42 48\"  # Hours for field verification\n","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"FLDVER Main switch for field verification (yes|no). The field verification extracts some selected variables for calculation of bias, rmse, stdv and averages on the model grid.\nFLDVER_HOURS Hours for field verification","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"More options on field verification can be found in scr/Fldver and scr/AccuFldver","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Observation-monitoring-and-general-diagnostics-1","page":"Experiment","title":"Observation monitoring and general diagnostics","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"# *** Observation monitoring ***\nOBSMONITOR=obstat                       # Create Observation statistics plots\n                                        # Format: OBSMONITOR=Option1:Option2:...:OptionN\n                                        # obstat: Daily usage maps and departures\n                                        # no: Nothing at all\n                                        #\n                                        # obstat is # only active if ANAATMO != none\nOBSMON_SYNC=no                          # Sync obsmn sqlite tables to ecgate (yes|no)","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"OBSMONITOR Selection for observation statistics plots","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"obstat Observations usage. Read more here.\nno No monitoring","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Note that this is only active if ANAATMO != none","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#Field-monitoring-(-experimental-)-1","page":"Experiment","title":"Field monitoring ( experimental )","text":"","category":"section"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"Make various charts for daily monitoring","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"#  *** Monitoring maps for hirlam.org. ***\n#      Note that at ECMWF this is run on ecgb (grads is only there)\n#      In  this version You must check out manually contrib/mapbin to the \n#      directory referred as MAPBIN \nFIELDMONITOR=no\nMAPBIN=$HM_DATA/lib/util/mapbin","category":"page"},{"location":"ExperimentConfiguration/ConfigureYourExperiment/#","page":"Experiment","title":"Experiment","text":"FIELDMONITOR Main switch (no|yes)\nMAPBIN Path to plotting settings. Read more in scr/Monitoring_maps","category":"page"},{"location":"DataAssimilation/MTEN/#Moist-Total-Energy-Norm-(MTEN)-diagnostic-1","page":"MTEN","title":"Moist Total Energy Norm (MTEN) diagnostic","text":"","category":"section"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"MTEN shows the sensitivity of the forecast model to different observations withdrawn from the full analysis system.  There are two ways of computing the MTEN diagnostic: A special branch was created in CY40 (see below) where the MTEN diagnostic can be requested. This approach uses Harmonie ensemble system to perform series of observation denial independent runs. This means that the following settings are used in msms/harmonie.pm","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"    'ENSBDMBR' => [ 0 ],\n    'ENSCTL'   => [ '000',  '001',  '002',  '003', '004', '005', '006', '007' ],\n    'AIRCRAFT_OBS' => [ 0, 1, 1, 1, 1, 1, 1, 1],\n    'BUOY_OBS'     => [ 1, 0, 1, 1, 1, 1, 1, 1],\n    'AMSUA_OBS'    => [ 1, 1, 0, 1, 1, 1, 1, 1],\n    'AMSUB_OBS'    => [ 1, 1, 1, 0, 1, 1, 1, 1],\n    'POL_OBS'      => [ 1, 1, 1, 1, 0, 1, 1, 1],\n    'HRW_OBS'      => [ 1, 1, 1, 1, 1, 0, 1, 1],\n    'TEMP_OBS'     => [ 1, 1, 1, 1, 1, 1, 0, 1],\n    'IASI_OBS'     => [ 1, 1, 1, 1, 1, 1, 1, 0],","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"In this particular example, we are interested in the impact of aircraft, Buoy, amsu-a, amsu-b/mhs, polar winds, high-resolution geowinds, radiosonde, and iasi observations. This setting is activated in config.exp with the following choice:","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"export  REFEXP DOMTEN\nexport SYNOP_OBS=1             # All synop\nexport AIRCRAFT_OBS=1          # AMDAR, AIREP, ACARS\nexport BUOY_OBS=1              # Buoy\nexport POL_OBS=1               # Satob polar winds\nexport GEO_OBS=0               # Satob geo winds\nexport HRW_OBS=1               # Satob HRWind\nexport TEMP_OBS=1              # TEMP, TEMPSHIP\nexport PILOT_OBS=1             # Pilot, Europrofiler\nexport SEVIRI_OBS=0            # Seviri radiances\nexport AMSUA_OBS=1             # AMSU-A\nexport AMSUB_OBS=1             # AMSU-B, MHS\nexport IASI_OBS=1              # IASI\nexport PAOB_OBS=0              # PAOB not defined everywhere\nexport SCATT_OBS=0             # Scatterometer data not defined everywhere\nexport LIMB_OBS=0              # LIMB observations, GPS Radio Occultations\nexport RADAR_OBS=0             # Radar\nexport GNSS_OBS=0              # GNSS","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"Where REFEXP is the reference experiment (see below), and DOMTEN (yes,no) is activate the MTEN choice when fetching the First-guess and the VarBC files for the MTEN computation, as follows: in /scr/Fetch_assim_data:","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"if [ ${DOMTEN} = \"yes\" ]; then\n  HM_REFEXP=/sbt/harmonie/$REFEXP\n  adir=${ECFSLOC}:${HM_REFEXP}/$YY/$MM/$DD/$HH\nelse\n  adir=$( ArchDir $HM_EXP $YY $MM $DD $HH )\nfi","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"in scr/FirstGuess (be careful this happens twice in the script)","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"if [ ${DOMTEN} = \"yes\" ]; then\n  HM_REFEXP=/sbt/harmonie/$REFEXP\n  adir=${ECFSLOC}:${HM_REFEXP}/$FGYY/$FGMM/$FGDD/$FGHH\nelse\n  adir=$( ArchDir $HM_EXP $FGYY $FGMM $FGDD $FGHH )\nfi","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"The MTEN can be also computed using a deterministic system. In this case, you need to take care of the First-guess and the VarBC files, which should come from the reference experiment. You need to carefully set the choice of the observations to be tested in scr/include.ass. In this case, you need to adapt the above Fetch_assim_data and FirstGuess scripts accordingly.","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"The MTEN diagnostic, similarly to DFS, is case sensitive, so it's better to male the computation with times and dates enough distant (by 5 days or more).","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"The MTEN can be computed the example below:","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"  for EXP in EXP1 EXP2;\n    for RANGE in 06 12 18 24 30 36 42 48;\n    do\n\n       YY=`echo $DTG | cut -c 1-4`\n       mm=`echo $DTG | cut -c 5-6`\n       dd=`echo $DTG | cut -c 7-8`\n       hh=`echo $DTG | cut -c 9-10`\n       # -- Get the FA files\n       # ===================\n       ecp ec:/$USER/harmonie/$REFEXP/$YY/$mm/$dd/$hh/ICMSHHARM+00$RANGE ./FAREF$RANGE\n       ecp ec:/$USER/harmonie/${EXP}/$YY/$mm/$dd/$hh/ICMSHHARM+00$RANGE ./${EXP}$RANGE\n       $MTEN_BIN/MTEN ./FAREF$RANGE ./${EXP}$RANGE\n\n    done\n  done\n","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"See Storto & Randriamampianina, 2010 for more details.","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"The harmonie-40h1_DA branch can be used to calculate MTEN values for selected (40h1.2/trunk) forecasts by using the ensemble functionality to carry multiple observation withdrawal experiments. Follow these instructions:","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"Check out the DA branch:\nmkdir -p $SCRATCH/harmonie_releases/branches\ncd $SCRATCH/harmonie_releases/branches\nsvn co https://svn.hirlam.org/branches/harmonie-40h1_DA\nSet up your (MTEN) experiment:\nmkdir -p $HOME/hm_home/mtenEXP\ncd $HOME/hm_home/mtenEXP\n$SCRATCH/harmonie_releases/branches/harmonie-40h1_DA/config-sh/Harmonie setup -r $SCRATCH/harmonie_releases/branches/harmonie-40h1_DA\nYou will need to edit the ecf/config_exp.h  file and msms/harmonie.pm file to produce the required observation withdrawal experiments.","category":"page"},{"location":"DataAssimilation/MTEN/#","page":"MTEN","title":"MTEN","text":"... More to follow ...","category":"page"},{"location":"PostProcessing/gl/#GL-1","page":"GL","title":"GL","text":"","category":"section"},{"location":"PostProcessing/gl/#xtool-1","page":"GL","title":"xtool","text":"","category":"section"},{"location":"System/MFaccess/#Using-Météo-France-Servers-1","page":"MF Access","title":"Using Météo-France Servers","text":"","category":"section"},{"location":"System/MFaccess/#Introduction-1","page":"MF Access","title":"Introduction","text":"","category":"section"},{"location":"System/MFaccess/#","page":"MF Access","title":"MF Access","text":"The procedure to get access to MF servers and their read-only git repository is outlined here","category":"page"},{"location":"System/MFaccess/#First-steps-1","page":"MF Access","title":"First steps","text":"","category":"section"},{"location":"System/MFaccess/#","page":"MF Access","title":"MF Access","text":"Discuss your requirements for access to MF servers with the HIRLAM System project leader, Daniel Santos (dsantosm@aemet.es).\nDownload two forms \"Undertaking for the use of Météo-France computer resources\" and \"Demande d'authorisation de conexion au résau de Météo Franc\" from http://www.cnrm.meteo.fr/aladin/spip.php?article157. \nThe \"Undertaking for the use of Météo-France computer resources\" form  is to be signed by you only\nThe \"Demande d'authorisation de conexion au résau de Météo France\" must be signed by you and your department head. It must also include an institute stamp. You should enter details in Contacts, Compte d'accesés aux machines du Centre de Cacul and at the bottom with authorization from you institute manager with institute stamp.   - A scan of both forms with a brief introductory note should be sent to Eric Escaliere (eric.escaliere@meteo.fr) and cc'ed to Daniel Santos (dsantosm@aemet.es) and Claude Fischer (claude.fischer@meteo.fr).\nBe careful with the \"Machine du client\". I had to specify the name and IP address of my institute's Firewall server as this is what the outside world sees when I access external servers from my PC.\nMétéo-France will send (by post) your username (Identificateur) and password (Mot de passe) for log in.\nThe authentication process itself remains in two steps (first “parme”, then target), as before. \nA few specific examples follow (see MF's instructions for full details):\nbeaufix:","category":"page"},{"location":"System/MFaccess/#","page":"MF Access","title":"MF Access","text":"ewhelan@realin23:gcc-8.3.1:.../~> which beaufix\nalias beaufix='telnet beaufix.meteo.fr'\n\t/usr/bin/telnet\newhelan@realin23:gcc-8.3.1:.../~> beaufix \nTrying 137.129.240.110...\nConnected to beaufix.meteo.fr.\nEscape character is '^]'.\nCheck Point FireWall-1 authenticated Telnet server running on mascarpone\nUser: whelane\npassword: your_parme_password\nUser whelane authenticated by FireWall-1 authentication\n\nConnected to 137.129.240.110\nRed Hat Enterprise Linux Server release 6.9 (Santiago)\nKernel 2.6.32-696.6.3.el6.x86_64 on an x86_64\nbeaufixlogin0 login: whelane\nPassword: your_ldap_password\nLast login: Tue Oct 13 10:15:53 from gw2.met.ie\n _                           __  _       \n| |                         / _|(_)      \n| |__    ___   __ _  _   _ | |_  _ __  __\n| '_ \\  / _ \\ / _` || | | ||  _|| |\\ \\/ /\n| |_) ||  __/| (_| || |_| || |  | | >  < \n|_.__/  \\___| \\__,_| \\__,_||_|  |_|/_/\\_\\ \n\n[whelane@beaufixlogin0 ~]$ ","category":"page"},{"location":"System/MFaccess/#What-next?-**TO-BE-CONFIRMED**-1","page":"MF Access","title":"What next? TO BE CONFIRMED","text":"","category":"section"},{"location":"System/MFaccess/#Access-to-MF-servers-via-parme-1","page":"MF Access","title":"Access to MF servers via parme","text":"","category":"section"},{"location":"System/MFaccess/#","page":"MF Access","title":"MF Access","text":"Once you are happy that you can access PARME from your PC you should once again contact Eric Escaliere (eric.escaliere@meteo.fr) and request login details for merou (Eric will send you a temporary password) and LDAP login details to front-id to enable access to COUGAR, YUKI, BEAUFIX and ID-FRONT\nAn automatic e-mail will be sent from expl-identites@meteo.fr with you LDAP repository password.\nfront-id requires certain criteria for your password. These are detailed in French below. When you have received LDAP login details for front-id:\newhelan@eddy:~> telnet parme.meteo.fr\nTrying 137.129.20.1...\nConnected to parme.meteo.fr.\nEscape character is '^]'.\nCheck Point FireWall-1 authenticated Telnet server running on parmesan\nUser: whelane\npassword: ********\nUser whelane authenticated by FireWall-1 authentication\nHost: front-id\n\nConnected to id-front\nRed Hat Enterprise Linux AS release 4 (Nahant Update 5)\nKernel 2.6.9-55.ELsmp on an x86_64\nlogin: whelane\nPassword: \nLast login: Mon Nov  4 05:14:22 from gw2.met.ie\nBienvenue EOIN WHELAN\nVous pouvez changer votre mot de passe\n-------------------------------------------------------------------------\n- Controle de validite sur les mots de passe avant de poster la demande -\n- Le OLD doit etre fourni. -\n- Au moins 8 car, au plus 20 car. -\n- Au moins 2 car. alpha et 2 car. non-alpha. -\n- Ne pas ressembler a UID NAME et OLD sur une syllabe de + de 2 car. -\n-------------------------------------------------------------------------\n-------------------------------------------------------------------------\nHello EOIN WHELAN\nYou may change your password\n-------------------------------------------------------------------------\n- Validity control before demand acceptation -\n- You must enter the old password first -\n- The new password must contain: -\n- At least 8 characters, 20 characters maximum -\n- At least 2 alphanumeric characters and 2 non-alphanumeric characters -\n- The passwd must contain a part of UID NAME -\n-------------------------------------------------------------------------\nChanging password for user 'whelane(56064)'.\nEnter login(LDAP) password: \nNew password: \nRe-enter new password: \nVotre mot de passe a ete change\nWhen you have received login details for merou from Eric:\newhelan@eddy:~> telnet parme.meteo.fr\nTrying 137.129.20.1...\nConnected to parme.meteo.fr.\nEscape character is '^]'.\nCheck Point FireWall-1 authenticated Telnet server running on parmesan\nUser: whelane\npassword: ********\nUser whelane authenticated by FireWall-1 authentication\nHost: merou\n\nConnected to merou\nRed Hat Enterprise Linux Server release 5.6 (Tikanga)\nKernel 2.6.18-238.el5 on an x86_64\nlogin: whelane\nPassword: \nLast login: Tue Nov  5 10:06:35 from gw2.met.ie\n[whelane@merou ~]$ passwd\nChanging password for user whelane.\nChanging password for whelane\n(current) UNIX password: \nNew UNIX password: \nRetype new UNIX password: \npasswd: all authentication tokens updated successfully.\n[whelane@merou ~]$ ","category":"page"},{"location":"System/MFaccess/#Access-to-(read-only)-MF-git-arpifs-git-repository-1","page":"MF Access","title":"Access to (read-only) MF git arpifs git repository","text":"","category":"section"},{"location":"System/MFaccess/#","page":"MF Access","title":"MF Access","text":"MF use ssh keys to allow access to their read-only git repository. If approved by the HIRLAM System PL you should request access to the repository by sending a request e-mail to Eric Escaliere and cc'ed to Daniel Santos and Claude Fischer your ssh public key attached.","category":"page"},{"location":"System/MFaccess/#","page":"MF Access","title":"MF Access","text":"Once you have been given access you can create a local clone by issuing the following commands:","category":"page"},{"location":"System/MFaccess/#","page":"MF Access","title":"MF Access","text":"cd $HOME\nmkdir arpifs_releases\ncd arpifs_releases\ngit clone ssh://reader054@git.cnrm-game-meteo.fr/git/arpifs.git","category":"page"},{"location":"System/MFaccess/#","page":"MF Access","title":"MF Access","text":"Happy gitting!","category":"page"},{"location":"DataAssimilation/ObservationOperators/#Observation-operators-1","page":"HOP_DRIVER","title":"Observation operators","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"This documentation summarises the observation operator in HARMONIE and the use of the HOP_DRIVER tool. The test harness, HOP_DRIVER, calls the observation operator and generates FG departures without calling any model code or initialising any model modules. Firstly, the IFS is used to dump a single-observation gom_plus to file from the 1st trajectory of an experiment. Dumping multiple observations would require a more complex and full-featured dump (good file format, multi-process parallel). For code refactoring HOP_DRIVER can be used to test changes to the observation operator of a particular observation type.","category":"page"},{"location":"DataAssimilation/ObservationOperators/#HARMONIE-and-HOP_DRIVER-1","page":"HOP_DRIVER","title":"HARMONIE and HOP_DRIVER","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"The HOP_DRIVER program was first added to CY42R2 code. The tool was initially implemented to test refactoring of the IFS observation operator code src/arpifs/op_obs/hop.F90. At the moment the refactor branch (branches/refactor/harmonie) is the only HARMONIE code set that includes HOP_DRIVER. Instructions on how to prepare the code and run HOP_DRIVER using HARMONIE are outlined below.  Presentation made at [wiki:HirlamMeetings/ModelMeetings/ObOpWorkshop OOPS Observation Operator Workshop] may provide some useful background information.","category":"page"},{"location":"DataAssimilation/ObservationOperators/#Comments-on-the-branch-1","page":"HOP_DRIVER","title":"Comments on the branch","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"Code changes were required in order to compile cy42r2bf.04 + mods (provided by MF/ECMWF) in the HARMONIE system: [14312], [14325], [14326], [14330], [14331], [14332], [14333], [14334].\nChanges were made to makeup in order to compile HOP_DRIVER correctly: [14310], [14327], [14328], [14329], [14335], [14362], [14382], [14392].\nIncluded in [14362] is a change to ODBSQLFLAGS which is set to \"ODBSQLFLAGS=-O3 -C -UCANARI -DECMWF ODBEXTRAFLAGS\" in order to use ECMWF flavoured ODB used by HOP_DRIVER\nOn cca GNU compilers 4.9 are not fully supported, ie I had to build GRIB-API and NetCDF locally using gcc/gfortran 4.9 on cca\nAn environment variable, HOPDIR, is used to define the location of necessary input data for HOP_DRIVER\nAn environment variable, HOPCOMPILER, is used by the HOP_driver script to define the compiler used. This is used to compare results.","category":"page"},{"location":"DataAssimilation/ObservationOperators/#Running-on-ecgb/cca-1","page":"HOP_DRIVER","title":"Running on ecgb/cca","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"cd $SCRATCH\nmkdir -p harmonie_releases/branches/refactor\ncd harmonie_releases/branches/refactor\nsvn co https://svn.hirlam.org/branches/refactor/harmonie-42R2\ncd $HOME\nmkdir -p hm_home/rfexp\ncd hm_home/rfexp\n$SCRATCH/harmonie_releases/branches/refactor/harmonie-42R2/config-sh/Harmonie setup -r $SCRATCH/harmonie_releases/branches/refactor/harmonie-42R2\n$SCRATCH/harmonie_releases/branches/refactor/harmonie-42R2/config-sh/Harmonie hop_driver","category":"page"},{"location":"DataAssimilation/ObservationOperators/#Running-on-local-platforms-1","page":"HOP_DRIVER","title":"Running on local platforms","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"So far, only METIE.LinuxRH7gnu, which uses gfortran 4.9 and openmpi, has been tested. Input data for the amsua test case is available on ECFS at ECMWF: ec:/dui/hopdata.tar.gz","category":"page"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"cd $HOME\nmkdir -p harmonie_releases/branches/refactor\ncd harmonie_releases/branches/refactor\nsvn co https://svn.hirlam.org/branches/refactor/harmonie-42R2\ncd $HOME\nmkdir -p hm_home/rfexp\ncd hm_home/rfexp\n$HOME/harmonie_releases/branches/refactor/harmonie-42R2/config-sh/Harmonie setup -h METIE.LinuxRH7gnu -r $HOME/harmonie_releases/branches/refactor/harmonie-42R2\n$HOME/harmonie_releases/branches/refactor/harmonie-42R2/config-sh/Harmonie hop_driver","category":"page"},{"location":"DataAssimilation/ObservationOperators/#HOPOBS:-amsua-1","page":"HOP_DRIVER","title":"HOPOBS: amsua","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"Currently there is only one observation type, AMSU-A (HOPOBS=amsua), available for testing with HOP_DRIVER. Alan Geer (ECMWF) has already carried out the refactoring of the HOP code related to AMSU-A observations. A single observation is provided in the ECMA and is used to test the refactoring of the HOP code. To carry out the testing of the amsua refactoring HOPOBS should be set to amsua in ecf/config_exp.h.","category":"page"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"reportype@hdr obstype@hdr sensor@hdr statid@hdr stalt@hdr date@hdr time@hdr degrees(lat) degrees(lon) report_status@hdr datum_status@body obsvalue@body varno@body vertco_type@body\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 12 173.28 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 12 158.86 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 3 227.40 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 3 260.82 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 256.90 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 239.60 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 12 NULL 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 3 217.69 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 209.39 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 214.05 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 223.02 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 234.42 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 245.14 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 1 257.18 119 3\n1007 7 3 '        4' 832800 !20140131 215914 -29.5906 0.3113 1 12 227.91 119 3","category":"page"},{"location":"DataAssimilation/ObservationOperators/#HOP_DRIVER-1","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/#Using-HOP_DRIVER-1","page":"HOP_DRIVER","title":"Using HOP_DRIVER","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"With LHOP_RESULTS=.TRUE. HOP_DRIVER will write results to a file called hop_results${MYPROC} for comparison between online and offline results. (The results file is opened by src/arpifs/var/taskob.F90. HOP_DRIVER results are written to hop_results${MYPROC} in src/arpifs/op_obs/hop.F90:","category":"page"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":" :\n :\nIF(LHOP_RESULTS) THEN\n!$OMP CRITICAL\n  ! Output for comparison between online and offline results:\n  WRITE(CFILENAME,'(\"hop_results\",I4.4)') MYPROC\n  OPEN(NEWUNIT=IU,FILE=CFILENAME,POSITION='APPEND',ACTION='WRITE',FORM='FORMATTED')\n  DO JOBS = 1,KDLEN\n    DO JBODY=1,IMXBDY\n      IF (JBODY>ICMBDY(JOBS)) CYCLE\n      IBODY = ROBODY%MLNKH2B(JOBS)+(JBODY-1)\n      WRITE(IU,'(6I8,2F30.14)') MYPROC, KSET, JOBS, NINT(ROBHDR%DATA(JOBS,ROBHDR%SEQNO_AT_HDR)),&\n        & NINT(ROBODY%DATA(IBODY,ROBODY%VERTCO_REFERENCE_1_AT_BODY)), &\n        & NINT(ROBODY%DATA(IBODY,ROBODY%VARNO_AT_BODY)), ZHOFX(JOBS,JBODY), ZXPPB(JOBS,JBODY)\n\n    ENDDO\n  ENDDO\n  CLOSE(IU)\n!$OMP END CRITICAL\nENDIF\n :\n :","category":"page"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"The HOPdriver script (based a script provided by MF) sorts the contents of the `hopresults0001` file for comparison with some results made available by ECMWF/MF:","category":"page"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":" :\n :\n#\n# Check HOP_DRIVER results (available for gfotran and intel)\n#\nln -s $HOPDIR/${HOPOBS}/results.$HOPCOMPILER .\ncat hop_results* | sort -k1,1n -k2,2n -k3,3n -k5,5n -k6,6n > results.driver\necho\ncmp -s results.$HOPCOMPILER results.driver\nif [ $? -eq 0] ; then\n  echo \"RESULTS ARE STRICTLY IDENTICAL TO THE REFERENCE FOR HOPCOMPILER=$HOPCOMPILER :-)\"\nelse\n  echo Compare exactly against the results dumped from hop:\n  echo \"xxdiff results.$HOPCOMPILER results.driver &\"\n  diff results.$HOPCOMPILER results.driver\n  exit 1\nfi\n :\n :","category":"page"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"On cca you will find useful output from HOPDRIVER in cca:TEMP/hmhome/rfexp/archive/HOPDRIVEROUT:","category":"page"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"fort.4\nNODE.001_01\nhop_results0001\nresults.gfortran\nresults.driver","category":"page"},{"location":"DataAssimilation/ObservationOperators/#The-code-1","page":"HOP_DRIVER","title":"The code","text":"","category":"section"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"HOP_DRIVER is a short program written by Deborah Salmond (ECMWF) to test code changes made to the observation operator. The program src/arpifs/programs/hop_driver.F90 is summarised here.","category":"page"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"The program sets up the model geometry and observations:","category":"page"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":" :\n :\nCALL GEOMETRY_SET(YRGEOMETRY)\nCALL MODEL_SET(YRMODEL)\n\nCALL IFS_INIT('gc7a')\n\nCALL SUINTDYN\n\nCALL SUGEOMETRY(YRGEOMETRY)        !From GEOMETRY_SETUP\n\nCALL SURIP(YRGEOMETRY%YRDIM)             !From MODEL_CREATE\n\n! Set up Observations, Sets\nCALL SUDIMO(YRGEOMETRY,NULOUT)     !From SU0YOMB\nCALL SUOAF              !From SU0YOMB\nCALL SUALOBS            !From SU0YOMB\nCALL SURINC             !From SU0YOMB\nCALL SETUP_TESTVAR      !From SU0YOMB\nCALL SUOBS(YRGEOMETRY)              !From CNT1\nCALL ECSET(-1,NOBTOT,0) !From OBSV\nCALL SUPHEC(YRGEOMETRY,NULOUT)\n\n! Setup varbc (from cnt1.F90) and read VARBC.cycle\nCALL YVARBC%SETUP_TRAJ\n :\n :","category":"page"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":"HOP_DRIVER then loops over the number of observation sets (NSETOT) and reads a GOM PLUS for each observation set. HRETR and HOP are then called:","category":"page"},{"location":"DataAssimilation/ObservationOperators/#","page":"HOP_DRIVER","title":"HOP_DRIVER","text":" :\n :\nDO ISET=1,NSETOT\n  IDLEN   = MLNSET(ISET)\n  IMXBDY = MAX(MMXBDY(ISET),1)\n\n  ALLOCATE(ZHOFX(IDLEN,IMXBDY))\n  ZHOFX=RMDI\n\n  ! READ GOM_PLUS FROM DUMP\n  CALL GOM_PLUS_READ_DUMP(YGP5,ISET)\n\n  IF(IDLEN /= YGP5%NDLEN) THEN\n    CALL ABOR1('Sets are incompatible')\n  ENDIF\n\n  :\n  :\n  :\n\n  CALL HRETR(YRGEOMETRY%YRDIMV,IDLEN,IMXBDY,ISET,1,YGP5,YVARBC)\n\n  CALL HOP(YRGEOMETRY%YRDIMV,YGP5,YVARBC,IDLEN,IMXBDY,ISET,1,LDOOPS=.TRUE.,PHOFX=ZHOFX)\n\n  !write(0,*)'ZHOFX',ZHOFX\n  DEALLOCATE(ZHOFX)\n\n  CALL GOM_PLUS_DESTROY(YGP5)\n\nENDDO\n\n :\n :","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Preparation-of-initial-and-boundary-files-1","page":"Preparation","title":"Preparation of initial and boundary files","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/#Introduction-1","page":"Preparation","title":"Introduction","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"HARMONIE can be coupled with external models as IFS, ARPEGE, HIRLAM. Internally it is possible to nest the different ALADIN/ALARO/AROME with some restrictions. In the following we describe the host initial and boundary files are generated depending on different configurations. Boundary file preparation basically includes two parts: forecast file fetching and boundary file generation.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"The ECFLOW tasks for initial and boundary preparation","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Boundary-strategies-1","page":"Preparation","title":"Boundary strategies","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"There are a number of ways to chose which forecast lengths you use as boundaries. The strategy is determined by BDSTRATEGY in ecf/config_exp.h  and there are a number of strategies implemented.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"available            : Search for available files in BDDIR adn try to keep forecast consistency. This is ment to be used operationally since it will at least keep your run going, but with old boundaries, if no new boundaries are available.\nsimulate_operational : Mimic the behaviour of the operational runs using ECMWF 6h old boundaries.\nsame_forecast        : Use all boundaries from the same forecast, start from analysis\nanalysis_only        : Use only analyses as boundaries. Note that BDINT cannot be shorter than the frequency of the analyses.\nlatest               : Use the latest possible boundary with the shortest forecast length\nRCR_operational      : Mimic the behaviour of the RCR runs, ie\n12h old boundaries at 00 and 12 and\n06h old boundaries at 06 and 18\njb_ensemble          : Same as same_forecast but used for JB-statistics generation. With this you should export JB_ENS_MEMBER=some_number\neps_ec               : ECMWF EPS members (on reduced Gaussian grid). It is only meaningful with ENSMSEL non-empty, i.e., ENSSIZE > 0","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"All the strategies are defined in scr/Boundary_strategy.pl. The script generates a file bdstrategy in your working directory that could look like:","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":" Boundary strategy\n\n       DTG: 2011090618\n        LL: 36\n     BDINT: 3\n   BDCYCLE: 6\n  STRATEGY: simulate_operational\n     BDDIR: /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/ECMWF/archive/@YYYY@/@MM@/@DD@/@HH@\nHOST_MODEL: ifs\nINT_BDFILE: /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/20110906_18/ELSCFHARMALBC@NNN@\n\n# The output bdstrategy file has the format of \n# NNN|YYYYMMDDHH INT_BDFILE BDFILE BDFILE_REQUEST_METHOD \n# where \n# NNN        is the input hour\n# YYYYMMDDHH is the valid hour for this boundary\n# INT_BDFILE is the final boundary file\n# BDFILE                is the input boundary file\n# BDFILE_REQUEST_METHOD is the method to the request BDFILE from e.g. MARS, ECFS or via scp\n\nSURFEX_INI| /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/20110906_18/SURFXINI.lfi \n000|2011090618 /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/20110906_18/ELSCFHARMALBC000 /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/ECMWF/archive/2011/09/06/12/fc20110906_12+006 MARS_umbrella -d 20110906 -h 12 -l 6 -t\n003|2011090621 /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/20110906_18/ELSCFHARMALBC001 /cca/tmp/ms/se/snh/hm_home/alaro_37h1_trunk/ECMWF/archive/2011/09/06/12/fc20110906_12+009 MARS_umbrella -d 20110906 -h 12 -l 9 -t\n...","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"Meaning that the if the boundary file is not found under BDDIR the command MARS_umbrella -d YYYYMMDD -h HH -l LLL -t BDDIR will be executed. A local interpretation could be to search for external data if your file is not on BDDIR. Like the example from SMHI:","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":" Boundary strategy\n\n       DTG: 2011090112\n        LL: 24\n     BDINT: 3\n   BDCYCLE: 06\n  STRATEGY: latest\n     BDDIR: /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/g05a/archive/@YYYY@/@MM@/@DD@/@HH@\nHOST_MODEL: hir\nINT_BDFILE: /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/20110901_12/ELSCFHARMALBC@NNN@\n EXT_BDDIR: smhi_file:/data/arkiv/field/f_archive/hirlam/G05_60lev/@YYYY@@MM@/G05_@YYYY@@MM@@DD@@HH@00+@LLL@H00M\nEXT_ACCESS: scp\n\n# The output bdstrategy file has the format of \n# NNN|YYYYMMDDHH INT_BDFILE BDFILE BDFILE_REQUEST_METHOD \n# where \n# NNN        is the input hour\n# YYYYMMDDHH is the valid hour for this boundary\n# INT_BDFILE is the final boundary file\n# BDFILE                is the input boundary file\n# BDFILE_REQUEST_METHOD is the method to the request BDFILE from e.g. MARS, ECFS or via scp\n\n# hh_offset is 0 ; DTG is  \nSURFEX_INI| /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/20110901_12/SURFXINI.lfi \n000|2011090112 /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/20110901_12/ELSCFHARMALBC000 /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/g05a/archive/2011/09/01/12/fc20110901_12+000 scp smhi:/data/arkiv/field/f_archive/hirlam/G05_60lev/201109/G05_201109011200+000H00M \n003|2011090115 /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/20110901_12/ELSCFHARMALBC001 /nobackup/smhid9/sm_esbol/hm_home/ice_36h1_4/g05a/archive/2011/09/01/12/fc20110901_12+003 scp smhi:/data/arkiv/field/f_archive/hirlam/G05_60lev/201109/G05_201109011200+003H00M ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"In this example an scp from smhi will be executed if the expected file is not in BDDIR. There are a few environment variables that one can play with in sms/confi_exp.h that deals with the initial and boundary files","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"HOST_MODEL : Tells the origin of your boundary data      * ifs : ecmwf data      * hir : hirlam data      * ald : Output from aladin physics, this also covers arpege data after fullpos processing.      * ala : Output from alaro physics      * aro : Output from arome physics\nBDINT : Interval of boundaries in hours\nBDLIB : Name of the forcing experiment. Set\nECMWF to use MARS data\nRCRa  to use RCRa data from ECFS\nOther HARMONIE/HIRLAM experiment\nBDDIR : The path to the boundary file. In the default location BDDIR=$HM_DATA/${BDLIB}/archive/@YYYY@/@MM@/@DD@/@HH@ the file retrieved from e.g. MARS will be stored in a separate directory. On could also consider to configure this so that all the retrieved files are located in your working directory $WRK. Locally this points to the directory where you have all your common boundary HIRLAM or ECMWF files.\nINT_BDFILE : is the full path of the interpolated boundary files. The default setting is to let the boundary file be removed by directing it to $WRK.\nINT_SINI_FILE : The full path of the initial surfex file. ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"There are a few optional environment variables that could be used that are not visible in config_exp.h ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"EXT_BDDIR : External location of boundary data. If not set rules are depending on HOST_MODEL\nEXT_ACCESS : Method for accessing external data. If not set rules are depending on HOST_MODEL\nBDCYCLE : Assimilation cycle interval of forcing data, default is 6h.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"More about this can be bounds in the Boundary_strategy.pl script.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"The bdstrategy file is parsed by the script ExtractBD. ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"scr/ExtractBD Checks if data are on BDDIR otherwise copy from EXT_BDDIR.  The operation performed can be different depending on HOST and HOST_MODEL. IFS data at ECMWF are extracted from MARS, RCR data are copied from ECFS.\nInput parameters: Forecast hour\nExecutables: none.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"In case data should be retrieved from MARS there is also a stage step. When calling MARS with the stage command we ask MARS to make sure data are on disk. In HARMONIE we ask for all data for one day of r forecasts ( normally four cycles ) at the time.  ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Initial-and-Boundary-file-generation-1","page":"Preparation","title":"Initial and Boundary file generation","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"To be able to start the model we need the variables defining the model state.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"T,U,V,PS in spectral space\nQ in gridpoint or spectral space","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"Optional:","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"Q,,l,,, Q,,i,,, Q,,r,,, Q,,g,,,  Q,,s,,, Q,,h,,\nTKE","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"For the surface we need the different state variables for the different tiles. The scheme selected determines the variables.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"Boundary files (coupling files) for HARMONIE are prepared in two different ways depending on the  nesting procedure defined by HOST_MODEL.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Using-gl-1","page":"Preparation","title":"Using gl","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"If you use data from HIRLAM or ECMWF gl_grib_api will be called to generate boundaries. The generation can be summarized in the following steps:","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"Setup geometry and what kind of fields to read depending on HOST_MODEL\nRead the necessary climate data from a climate file\nTranslate and interpolate the surface variables horizontally if the file is to be used as an initial file. All interpolation respects land sea mask properties. The soil water is not interpolated directly but interpolated using the Soil Wetness Index to preserve the properties of the soil between different models. The treatment of the surface fields is only done for the initial file.\nHorizontal interpolation of upper air fields as well as restaggering of winds.\nVertical interpolation using the same method (etaeta) as in HIRLAM\nConserve boundary layer structure\nConserve integrated quantities\nOutput to an FA file ( partly in spectral space )","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"gl_grib_api is called by the script scr/gl_bd where we make different choices depending on PHYSICS and HOST_MODEL","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"When starting a forecast there are options to whether e.g. cloud properties and TKE should be read from the initial/boundary file through NREQIN and NCOUPLING. At the moment these fields are read from the initial file but not coupled to. gl reads them if they are available in the input files and sets them to zero otherwise. For a Non-Hydrostatic run the non-hydrostatic pressure departure and the vertical divergence are demanded as an initial field. The pressure departure is by definition zero if you start from a non-hydrostatic mode and since the error done when disregarding the vertical divergence is small it is also set to zero in gl. There are also a choice in the forecast model to run with Q in gridpoint or in spectral space.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"It's possible to use an input file without e.g. the uppermost levels. By setting LDEMAND_ALL_LEVELS=.FALSE. the missing levels will be ignored. This is used at some institutes to reduce the amount of data transferred for the operational runs. ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Using-fullpos-1","page":"Preparation","title":"Using fullpos","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"If you use data generated by HARMONIE you will use fullpos to generate boundaries and initial conditions. Here we will describe how it's implemented in HARMONIE but there are also good documentation on the gmapdoc site.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"fullpos","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"In HARMONIE it is done by the script scr/E927. It contains the following steps:","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"Fetcht climate files. Fullpos needs a climate file and the geometry definition for both the input and output domains. \nSet different moist variables in the namelists depending if your run AROME or ALADIN/ALARO.\nCheck if input data has Q in gridpoint or spectral space.\nDemand NH variables if we run NH.\nDetermine the number of levels in the input file and extract the correct levels from the definition in scr/Vertical_level.pl\nRun fullpos","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"E927 is also called from 4DVAR when the resolution is changed between the inner and outer loops.","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#Generation-of-initial-data-for-SURFEX-1","page":"Preparation","title":"Generation of initial data for SURFEX","text":"","category":"section"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"For SURFEX we have to fill the different tiles with correct information from the input data. This is called the PREP step in the SURFEX context. scr/Prep_ini_surfex creates an initial SURFEX file from an FA file if you run with SURFACE=surfex. ","category":"page"},{"location":"Boundaries/BoundaryFilePreparation/#","page":"Preparation","title":"Preparation","text":"Read more about SURFEX","category":"page"},{"location":"ClimateGeneration/DownloadInputData/#Download-input-data-1","page":"Input Data","title":"Download input data","text":"","category":"section"},{"location":"ClimateGeneration/DownloadInputData/#","page":"Input Data","title":"Input Data","text":"Before you can start running HARMONIE experiments some input data (external from the code repository) needs to be available on your platform. The input data contains physiography data, topography information and climatological values determined from a one year ARPEGE assimilation experiment with a resolution of T79.","category":"page"},{"location":"ClimateGeneration/DownloadInputData/#","page":"Input Data","title":"Input Data","text":" E923_DATA-harmonie-43h2.1.tar.gz: Climate and physiography data for atmospheric climate generation (E923)\n PGD-harmonie-43h2.1.tar.gz: Physiography data for SURFEX (PGD)\nGMTED2010-harmonie-43h2.1.tar.gz : Digital elevation model from UGS\n SOILGRID-harmonie-43h2.1.tar.gz: Soil type data from SOILGRIDS\n sat-harmonie-43h2.1.tar.gz: Constants for satellite information\nrttov7L54-harmonie-43h2.1.tar.gz  : RTTOV constants\nECOCLIMAP second generation is available from here. It's also available on cca:/project/hirlam/harmonie/climate_in_order/ECOCLIMAP2G\ntestbed-harmonie-43h2.1.tar.gz: Test data set with boundaries and observations for a small 50x50 domain]","category":"page"},{"location":"QuickStart/ECMWF/TheHarmonieScript/#The-Harmonie-Script-1","page":"The Harmonie Script","title":"The Harmonie Script","text":"","category":"section"},{"location":"DataAssimilation/LSMIXandJk/#Jk-as-a-pre-mixing-method-1","page":"LSMIX and Jk","title":"Jk as a pre-mixing method","text":"","category":"section"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"The 3D-Var cost function including the Jk term can be written:","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"J(x) = J_b + J_o + J_k = frac12 (x - x_b)^rm T B^-1(x - x_b) + frac12 (y - Hx)^rm TR^-1(y - Hx) + frac12 (x - x_LS)^rm T V^-1(x - x_LS)","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"Setting the gradient to zero, we have at the optimal x:","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"nabla J = B^-1(x - x_b) - H^rm TR^-1(y - Hx) + V^-1(x - x_LS) = 0 ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"or","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"leftB^-1 + V^-1 + H^rm TR^-1Hright left(x - x_b right) = H^rm TR^-1(y - Hx_b) + V^-1(x_LS - x_b) ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#Equivalent-pre-mixed-first-guess-1","page":"LSMIX and Jk","title":"Equivalent pre-mixed first guess","text":"","category":"section"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"Assume now that widetildex_b is some yet unknown, pre-mixed field depending on x_b and x_LS that we want to determine. By adding and subtracting identical terms to the gradient equation, we have","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"B^-1(x - x_b + widetildex_b - widetildex_b) - H^rm TR^-1(y - Hx + Hwidetildex_b - Hwidetildex_b) + V^-1(x - x_LS + widetildex_b - widetildex_b) = 0","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"which, when reorganized gives","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"leftB^-1 + V^-1 + H^rm TR^-1H right left(x - widetildex_bright) = H^rm TR^-1(y - Hwidetildex_b) + B^-1(x_b - widetildex_b) + V^-1(x_LS - widetildex_b) ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"If the last two terms on the right hand side add up to zero, i.e.,","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"B^-1(x_b - widetildex_b) + V^-1(x_LS - widetildex_b) = 0 ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"which means that","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"widetildex_b = B^-1 + V^-1^-1 ( B^-1 x_b + V^-1 x_LS ) ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"then we see that by using this mixed first guess the Jk term can be omitted, provided we use a modified B-matrix with the property that","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"widetildeB^-1 = B^-1 + V^-1  ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"By writing","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"B^-1 + V^-1 = B^-1(B + V)V^-1 = V^-1(B + V)B^-1 ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"we easily see by simply inverting that","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"widetildeB = B^-1 + V^-1^-1 = B(B + V)^-1V = V(B + V)^-1B  ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"To conclude, a 3D-Var minimization with Jk is equivalent to a minimization without the Jk term, provided that one pre-mixes the two first guess fields according to","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"widetildex_b = B^-1 + V^-1^-1 ( B^-1 x_b + V^-1 x_LS ) = widetildeB( B^-1 x_b + V^-1 x_LS ) = V(B + V)^-1x_b + B(B + V)^-1x_LS ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"and use the following covariance matrix for this mixed first guess:","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"widetildeB = B^-1 + V^-1^-1 = B(B + V)^-1V = V(B + V)^-1B  ","category":"page"},{"location":"DataAssimilation/LSMIXandJk/#","page":"LSMIX and Jk","title":"LSMIX and Jk","text":"Whether this is implementable in practice is a different story, it just shows the theoretical equivalence, and how LSMIXBC should ideally be done if Jk is the right answer.","category":"page"},{"location":"ForecastModel/Forecast/#Forecast-1","page":"Forecast","title":"Forecast","text":"","category":"section"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"scr/Forecast is the script, which initiates actual  forecast run (ALADIN/AROME/ALARO depending on FLAG and PHFLAG).","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"Input parameters: none.\nData: Boundary files (ELSCF*-files). Initial file (fc_start). If data assimilation is used, fc_start is the analysis file. In case of dynamical adaptation, fc_start is the first boundary file. In case of AROME, Surfex initial file (SURFXINI.lfi) is also needed scr/Prep_ini_surfex. \nNamelists: namelist templates nam/namelist_fcst${FLAG}_default are fetched based on FLAG and PHFLAG. The templates are completed in scr/Forecast based on the choices of NPROCX, NPROCY (see config-sh/submit.ecgb), TFLAG, OUTINT, BDINT and REDUCELFI. In case of AROME also the namelists to control SURFEX-scheme  nam/TEST.des and nam/EXSEG1.nam are needed.\nExecutables: as defined by MODEL.\nOutput: Forecast files (spectral files ICMSHALAD+*). In case of AROME, Surfex files containing the surface data (AROMOUT_*.lfi). ","category":"page"},{"location":"ForecastModel/Forecast/#Forecast-namelists-1","page":"Forecast","title":"Forecast namelists","text":"","category":"section"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"The current switches in the HARMONIE system (in ecf/config_exp.h) provide only very limited possibility to control the different aspects of the model. If the user wants to have more detailed control on the specific schemes etc., one has to modify the variety of the namelists options.","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"In general, the different namelist options are documented in the source code modules (e.g. src/arp/module/*.F90). Below is listed information on some of the choices.   ","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"NH-dynamics/advection/time stepping:","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"A detailed overview of the such options has been given by Vivoda (2008). ","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"Upper air physics switches","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"Switches related to different schemes of ALADIN/ALARO physics, src/arp/module/yomphy.F90.\nSwitches related to physics schemes in AROME src/arp/module/yomarphy.F90.\nSwitches to tune different aspects of physics, src/arp/module/yomphy0.F90, src/arp/module/yomphy1.F90, src/arp/module/yomphy2.F90 and src/arp/module/yomphy3.F90\nSwitches related to HIRLAM physics, src/arp/module/yhloption.F90 and src/arp/setup/suhloption.F90.","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"Initialization switch","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"Initialization is controlled by namelist NAMINI/NEINI, src/arp/module/yomini.F90.","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"Horizontal diffusion switches","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"Horizontal diffusion is controlled by namelist NAMDYN/RDAMP*, src/arp/module/yomdyn.F90#L55. Larger the coefficient, less diffusion.","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"MPP switches","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"The number of processors in HARMONIE are given in config-sh/submit.ecgb. These values are transfered in to src/arp/module/yomct0.F90#L276 and src/arp/module/yommp.F90.","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"Surface SURFEX switches","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"The SURFEX scheme is controlled through namelist settings in nam/surfex_namelists.pm. The different options are described here.","category":"page"},{"location":"ForecastModel/Forecast/#Archiving-1","page":"Forecast","title":"Archiving","text":"","category":"section"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"Archiving has a two layer structure. Firstly, all the needed analysis forecast and field extract files   are stored in ARCHIVE directory by scr/Archive_fc. This is the  place where the postprocessing step expects to find the files. ","category":"page"},{"location":"ForecastModel/Forecast/#","page":"Forecast","title":"Forecast","text":"At ECMWF all the requested files are stored to ECFS into directory ECFSLOC by the script scr/Archive_ECMWF","category":"page"},{"location":"DataAssimilation/DFS/#DFS-1","page":"DFS","title":"DFS","text":"","category":"section"},{"location":"DataAssimilation/DFS/#","page":"DFS","title":"DFS","text":"The DFS calculation software is still in under development. It is however possible to run the software already now even though it is not fully automated. Start by downloading the tar-file dfscalc.tar","category":"page"},{"location":"DataAssimilation/DFS/#","page":"DFS","title":"DFS","text":"The following instructions is also available in the README file.","category":"page"},{"location":"DataAssimilation/DFS/#","page":"DFS","title":"DFS","text":"To run the DFS calculations and plot the results do the following","category":"page"},{"location":"DataAssimilation/DFS/#","page":"DFS","title":"DFS","text":"Edit the include.paths script to set dates and paths to your experiment and dfs directory\nRun prep_dfs in batch mode. This script generates two sets of ccma files, one with perturbed observations and one original unperturbed. \nBefore running the minimisation for the disturbed observations the namelist of your experiment must be copied to a file called \"namelminim\" in the dfs script dir. In one of your HMDate-files search for \"Start of log: fort.4\" and copy everything between the hash lines \"#############################\" into namel_minim.\nRun RunMinim in batch mode. This will generate new an-fg statistics for the perturbed observations.\nBefore running extract you need to create a compiled MANDALAY. This is easiest done by doing the following:","category":"page"},{"location":"DataAssimilation/DFS/#","page":"DFS","title":"DFS","text":"Set up a new experiment Check out src/obd/ddl/mandalay.sql Replace this file by the mandalay.sql in dfs_scr (included in this tar-file) Start your experiment and let it run until the compilation is complete Copy the created MANDALAY to dfs_scr/bin (or point to it in extract_dfs)","category":"page"},{"location":"DataAssimilation/DFS/#","page":"DFS","title":"DFS","text":"It is also necessary to compile dfscomp.F90 and dfstot.F90 e.g:","category":"page"},{"location":"DataAssimilation/DFS/#","page":"DFS","title":"DFS","text":"gfortran -o bin/dfscomp.x dfscomp.F90\ngfortran -o bin/dfstot.x dfstot.F90","category":"page"},{"location":"DataAssimilation/DFS/#","page":"DFS","title":"DFS","text":"Now run extract_dfs in batch mode. This generates the file $STARTDIR/OUTPUT/dsf.tot (STARTDIR in specified in include.paths) that contains the DFS statistics for the period. This can be plotted e.g. with the included R-script.\nPlot the resuts with dfs_plot.R. Place dfs.tot and dfs_plot.R in the same directory and type: Rscript dfs_plot.R This generates the file dfs_tot.eps in the same directory.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#Configure-for-faster-setup-on-cca-1","page":"Fast start on cca","title":"Configure for faster setup on cca","text":"","category":"section"},{"location":"QuickStart/Fast_start_on_cca/#Background-1","page":"Fast start on cca","title":"Background","text":"","category":"section"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"Since ECMWF switched from the IBM based c2a to the new Cray XC30 cca setting up and start a new HARMONIE experiment has been a very slow process. With the current settings it may take several hours from the moment you say Harmonie start until the code is fully compiled and the real run starts. In the following we describe how to configure your experiment to get started in about half an hour.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#What's-special-with-cca?-1","page":"Fast start on cca","title":"What's special with cca?","text":"","category":"section"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"The main cause for the slow synchronization and compilation time on cca is the Lustre file system. Such a file system is optimized for large files but is less efficient for large numbers of small files. As a comparison it takes ~2 minutes to copy the full harmonie system from ecgb:$HOME to $PERM whereas it takes about 1 hour or more to make the same copy to cca:$SCRATCH. It's not the transfer to cca that takes time but the IO on the $SCRATCH file system.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#How-to-set-it-all-up-1","page":"Fast start on cca","title":"How to set it all up","text":"","category":"section"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"The changes required can be viewed in [13814] and in below we go through what you have to do as a user. At the bottom of the page you'll find the versions adapted so far.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#Using-the-:PERM-disk-1","page":"Fast start on cca","title":"Using the PERM disk","text":"","category":"section"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"Normally HARMONIE is setup to use $SCRATCH as the main disk for both ecgb and cca. Two changes are required in Env_system to use the $PERM disk on cca.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"Set HM_LIB=/hpc$PERM/build_harmonie/$EXP/lib for the ecgb part of Env_system\nSet HM_LIB=$PERM/build_harmonie/$EXP/lib for the cca part of Env_system","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"This means that ecgb:$HM_LIB == cca:$HM_LIB so the synchronization between ecgb and cca is for free. Your working directories and results will still be found under $SCRATCH/hm_home/$EXP.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#Compiling-with-gmkpack-1","page":"Fast start on cca","title":"Compiling with gmkpack","text":"","category":"section"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"Harmonie comes with two way of compiling the code, Makeup and Gmkpack. Both methods have their benefits but in this particular case Gmkpack gives us the fastest throughput for two reasons.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"We only compile the source you have changed for the rest we reuse the already built libraries. This is partly true for Makeup as well but in the search for what to actually compile takes longer time.\nLess disk space and number of inodes is required compared to Makeup. We only copy the code to be compiled.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"The above mentioned is only true if you make small changes for larger number of changes that requires recompilation of big parts of the code it may not necessary be true any longer. To compile with gmkpack set","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"MAKEUP=no in Env_system\nMake sure BUILD_ROOTPACK=no in ecf/config_exp.h. The already compiled libraries are listed below. If you have to compile the full code Makeup is faster.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"The reproducibility between compilation with Makeup and gmkpack has not been checked yet although great care has been taken to use the same compilation options. To avoid unpleasant surprises you are not recommended to change compilation method between to experiments that you expect to compare.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#Things-to-keep-an-eye-on-1","page":"Fast start on cca","title":"Things to keep an eye on","text":"","category":"section"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"The disk space on cca:$PERM is limited not only for the disk space in GB but also for the number of files, the inodes. A typical experiment takes 2.5GB and uses 10000 inodes. Thus the number of experiments you can setup this ways is limited. To check your quota on cca:$PERM type","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"quota -v","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"on cca.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#What-about-existing-experiments?-1","page":"Fast start on cca","title":"What about existing experiments?","text":"","category":"section"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"There is not a great need to change your setup if you have a running experiment. The costly part is the initial setup and compilation. If you still feel you would like to do it you are advised to remove everything from cca:$SCRATCH/hm_home/$EXP and ecgb:$SCRATCH/hm_home/$EXP after you have checked that the necessary restart files are available on ecfs. The other alternative is to start a new experiment with the same changes and make sure you start with initial conditions from your old experiment.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#Updated-HARMONIE-configurations-1","page":"Fast start on cca","title":"Updated HARMONIE configurations","text":"","category":"section"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"The changes will progressively be implemented in the used configurations as listed below. If you miss any configuration please contact us. Note that the default settings will not be changed for the tagged versions. Note that for the trunk or for branches a new ROOTPACK is required for every update in the src directory and that the new ROOTPACK may not always be available.","category":"page"},{"location":"QuickStart/Fast_start_on_cca/#","page":"Fast start on cca","title":"Fast start on cca","text":"\n  harmonie-38h1\n\n   tags:\n\n   ~hlam/harmonie_release/tags/harmonie-38h1.1 ROOTPACK=/project/hirlam/harmonie/pack/38h1_main.01.gnu.x\n   ~hlam/harmonie_release/tags/harmonie-38h1.2 ROOTPACK=/project/hirlam/harmonie/pack/38h1_main.02.gnu.x\n\n   branches:\n\n   ~hlam/harmonie_release/branches/harmonie-38h1 ROOTPACK=/scratch/ms/spsehlam/hlam/common_rootpack/38h1_branch_harmonie_38h1_13749.01.gnu.x\n   ~hlam/harmonie_release/branches/harmonEPS-38h1.1 ROOTPACK=/project/harmonie/pack/38h1_harmonEPS_13706.01.gnu.x\n                                                    ROOTPACK=/project/harmonie/pack/38h1_harmonEPS_13827.01.gnu.x\n\n  harmonie-40h1\n\n   tags:\n   ~hlam/harmonie_release/tags/harmonie-40h1.1.beta.1 ROOTPACK=/project/hirlam/harmonie/pack/40h1_beta_1.01.gnu.x\n\n   branches:\n\n   ~hlam/harmonie_release/trunk ROOTPACK=/scratch/ms/spsehlam/hlam/common_rootpack/40h1_trunk_NNNNN.01.gnu.x\n\n","category":"page"},{"location":"QuickStart/TheHarmonieScript/#The-Harmonie-main-script-1","page":"The Harmonie script","title":"The Harmonie main script","text":"","category":"section"},{"location":"QuickStart/TheHarmonieScript/#","page":"The Harmonie script","title":"The Harmonie script","text":"The Harmonie script is the main user interface to the harmonie system. It is used to setup, start, check and control your experiment and environment. Below follows the most useful commands. There are other commands inherited from the HIRLAM environment that may or may not work. For a full list check scr/Start, scr/Actions, scr/Actions.pl.","category":"page"},{"location":"QuickStart/TheHarmonieScript/#","page":"The Harmonie script","title":"The Harmonie script","text":"Harmonie setup [ -r REVISION] [ -h HOST] [ -d DOMAIN] [ -c CONFIGURATION] [ -l LEVELS] where:\nREVISION is the path to the version of harmonie you are working with.\nHOST is the name of the host you are working on. There should exist corresponding config-sh/config.HOST. \nCONFIGURATION is one of the predefined configurations in scr/Harmonie_testbed.pl. It a fast way to setup your favourite configuration.\nDOMAIN is one of the predefined domains in ecf/config_exp.h \nLEVELS is one of the predefined level definitions in scr/Vertical_levels.pl\nHarmonie start DTG=YYYYMMDDHH [ DTGEND=YYYYMMDDHH] [ optional environment variables] launches a cold start run.\nDTG is the initial time of your experiment\nSeveral other optional variables can be given like\nPLAYFILE=FILENAME use a different ecflow suite definition file. Default is harmonie.tdf\nBUILD=yes|no to turn on and off compilation\nCREATE_CLIMATE=yes|no to turn on and off generation of climate files\nAny environment variable that you would like to send to the system.\nHarmonie prod will continue from the DTG given in your progress.log file. The rest of the arguments is as for Harmonie start. This should be used to continue and experiment. It is assumed that a first guess file is available and the run will fail if this is not found.    \nHarmonie mon will restart your ecflow_ui window and try to connect to an existing ecflow server.\nHarmonie co [FILE|PATH/FILE] will copy the request file from the version chosen in your setup ( as pointed out in the config-sh/hm_rev file ) to your local directory. If the PATH is not given a search will be done. If the name matches several files you will be given a list to choose from.\nHarmonie install will build your libraries and binaries but not start any experiment\nHarmonie testbed will launch the Harmonie testbed\nHarmonie diff [--xxdiff] will look for differences between the revision in config-sh/hm_rev and HM_LIB.\nHarmonie CleanUp -ALL -go will clean the following directories: HM_DATA, HM_LIB, HM_EXP. Instructions from src/Actions.pl:","category":"page"},{"location":"QuickStart/TheHarmonieScript/#","page":"The Harmonie script","title":"The Harmonie script","text":"# args: if -go: remove, (default is to list but not remove the matching files)\n#       if -k*: do not do the long term archive HM_EXP - so keep it\n#       if -d*: combination of -k and -ALL (-d* means: disks)\n#       if -ALL: treat all files and also (if -go) remove the directories\n#       a pattern is usually a string without meta-characters. To this\n#       a * is appended (so e.g. ob will affect all files ob*); this can\n#       be inhibited by appending ~ (so ob~! translates to ob).\n#       Also, files in all subdirectories P*_* will be affected\n#       where P is the pattern [0-9][0-9], This resembles\n#       a `CYCLEDIR'. So ob will result in 'ob* P*_*/ob*'.\n#       The pattern P*_* will be prepended to every / in the pattern,\n#       unless the / is preceded by ~ (which will be removed).\n#       Hence, to remove e.g. all analyses from 1995, use 1995/an,\n#       which translates to 1995[0-9][0-9]*_*/an*\n#       (to be precise: use: CleanUp(\"REMOVE:1995/an\", \"-go\");","category":"page"},{"location":"#Harmonie-System-Documentation-1","page":"Home","title":"Harmonie System Documentation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This is a draft version of the Harmonie System Documentation. Not all pages have been converted from trac/wiki to markdown yet. Suggestions for improvements are very welcome. ","category":"page"},{"location":"#","page":"Home","title":"Home","text":"We are optimistic that our Harmonie documentation will be nominated for the Nobel Prize in Literature in 2022 but winning requires a bit more work. ","category":"page"},{"location":"#","page":"Home","title":"Home","text":"If you spot small typos that don't require review, click the \"Edit on Github\" button at the top right of the page. You can then edit the markdown file on github.com and commit directly to the pre-CY46h1 branch.\nFor larger edits that could benefit from review or to add new pages it is recommend to use the fork-and-branch workflow. ","category":"page"},{"location":"#How-to-add-a-new-page-1","page":"Home","title":"How to add a new page","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"If you want to add a new page to the documentation:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Clone your fork and create a new branch from the pre-CY46h1 branch\nPut the markdown file in one of the subdirectories in docs/src/ (or create a new subdirectory). \nDecide where in the navigation bar your page should appear and edit docs/pages.jl accordingly. \ncd docs and run \njulia --project make.jl\nThis will check that, e.g. url's are valid and ensures that when we merge the pull request our github action won't fail. Note that the pages won't be deployed to the HarmonieSystemDocumentation repository when running locally. Julia can be downloaded here. The first time run cd docs\njulia --project -e 'using Pkg; Pkg.instantiate()'\nto install the Documenter dependency. \nThe html files will be put in docs/build/. Open docs/build/index.html and check everything looks fine. \nCommit your changes and push them to your fork on github\nCreate a pull request using the pre-CY46h1 branch as merge base and add Roel Stappers (and/or others) as a reviewer.\nWait for the email from the Nobel committee","category":"page"},{"location":"QuickStart/ECMWF/ECMWF_teleport/#Teleport-1","page":"Teleport","title":"Teleport","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#Parameter-list-and-GRIB-definitions-1","page":"Output List","title":"Parameter list and GRIB definitions","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#HARMONIE-system-output-1","page":"Output List","title":"HARMONIE system output","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"The HARMONIE system writes its primary output, in FA format, to the upper air history files ICMSHHARM+llll and the SURFEX history files ICMSHHARM+llll.sfx, where HARM is the four-character experiment identifier set in the configuration file config_exp.h, and llll is normally the current timestep in hours. The files are designed to be complete snapshots of respective model state described by the system for a particular time point. In addition more model output including post-processing/diagnostic fields can be written out during the forecast model integration, such as those model diagnostics or pressure level diagnostics, also in FA format, as PFHARMDOMAIN+llll. The FA files can be considered to be internal format files. All of them can be converted to GRIB files during the run for external usage. The name convention is as follows:","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Forecast upper air history files: ICMSHHARM+llll -> fcYYYYMMDDHH+lll_grib (GRIB1) or fcYYYYMMDDHH+lll_grib2 (GRIB2) \nForecast Surfex history files: ICMSHHARM+llll.sfx -> fcYYYYMMDDHH+lll_grib_sfx (GRIB1 only) \nForecast Surfex selected output: ICMSSELE+llll.sfx -> fcYYYYMMDDHH+lll_grib_sfxs (GRIB1 only) \nPostprocess files: PFHARMDOMAIN+llll.hfp -> fcYYYYMMDDHH+lllgrib_fp (GRIB1) or fcYYYYMMDDHH+lllgrib2_fp (GRIB2) \nAnalysis upper air history files: ICMSHANAL+0000 -> anYYYYMMDDHH+000grib (GRIB1) or anYYYYMMDDHH+000grib2 (GRIB2) (1)\nAnalysis SURFEX history files: ICMSHANAL+0000.sfx -> sa2019041600+000grib_sfx (only GRIB1 for the time being)","category":"page"},{"location":"ForecastModel/Outputlist/#GRIB1-table-2-version-in-HARMONIE-1","page":"Output List","title":"GRIB1 table 2 version in HARMONIE","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"To avoid conflicts with archived HIRLAM data HARMONIE uses version 253 of table 2. The table is based on the standard WMO version 3 of table 2 and postion 000-127 is kept the same as in the WMO. Note that accumulated and instantaneous versions of the same parameter differ only by the time range indicator. It is thus not sufficient to specify parameter, type and level when you refer to an accumulated parameter, but the time range indicator has to be included as well.","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"The translation of SURFEX files to GRIB1 is still incomplete and contains several WMO violations. This is not changed in the current release but will revised later. However, the upper air history file also includes the most common surface parameters and should be sufficient for most users.","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"The current table 2 version 253 definition files for gribapi can be found in `util/glgrib_api/definitions/`. These local definition files assume centre=233 (Dublin) and should be copied to your own GRIB-API installation. You are strongly recommended to set your own code for generating centre fore operational usage of the data.","category":"page"},{"location":"ForecastModel/Outputlist/#GRIB2-in-HARMONIE-1","page":"Output List","title":"GRIB2 in HARMONIE","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"The possibility to convert to GRIB2 has been introduced in release-43h2. So far the conversion is restricted to atmospheric history and fullpos files only. To get the output in GRIB2 set ARCHIVE_FORMAT=GRIB2 in ecf/config_exp.h. Please notice that if ARCHIVE_FORMAT=GRIB2 is selected, SURFEX files will be converted to GRIB1 anyway (for the time being). To convert from GRIB1 with GRIB2 using grib_filter we have to tell EcCodes how to translate the parameters. This is done by using the internal HARMONIE tables and setting","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"export ECCODES_DEFINITION_PATH=$SOME_PATH_TO_GL/gl/definitions:$SOME_PATH_TO_ECCODES/share/eccodes/definitions","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Note that there are a few parameters that are not translated to GRIB2 to and those has to be excluded explicitly.","category":"page"},{"location":"ForecastModel/Outputlist/#List-of-parameters-1","page":"Output List","title":"List of parameters","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#D-model-state-variables-on-model-levels-(1-NLEV),-levelTypehybrid-1","page":"Output List","title":"3D model state variables on model levels (1-NLEV), levelType=hybrid","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#indicatorOfParameter-1","page":"Output List","title":"indicatorOfParameter","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#parameterCategory-1","page":"Output List","title":"parameterCategory","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#parameterNumber-1","page":"Output List","title":"parameterNumber","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#discipline-1","page":"Output List","title":"discipline","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"FA name shortName iOP d pC pN stepType unit Description\nSNNNHUMI.SPECIFI q 51 0 1 0 instant kg kg**-1 Specific humidity\nSNNNLIQUID_WATER cwat_cond 76 0 1 83 instant kg kg**-1 Specific cloud liquid water content\nSNNNSOLID_WATER ciwc_cond 58 0 1 84 instant kg kg**-1 Specific cloud ice water content\nSNNNSNOW snow_cond 184 0 1 86 instant kg kg**-1 Specific snow water content\nSNNNRAIN rain_cond 181 0 1 85 instant kg kg**-1 Specific rain water content\nSNNNGRAUPEL grpl_cond 201 0 1 32 instant kg kg**-1 Specific graupel\nSNNNTKE tke 200 0 19 11 instant J kg**-1 Turbulent Kinetic Energy\nSNNNCLOUD_FRACTI tcc 71 0 6 192 instant 0-1 Total cloud cover\nSNNNPRESS.DEPART pdep 212 0 3 8 instant Pa Pressure departure\nSNNNTEMPERATURE t 11 0 0 0 instant K Temperature\nSNNNVERTIC.DIVER vdiv 213 0 2 192 instant s**-1 Vertical Divergence\nSNNNWIND.U.PHYS u 33 0 2 2 instant m s**-1 u-component of wind\nSNNNWIND.V.PHYS v 34 0 2 3 instant m s**-1 v-component of wind","category":"page"},{"location":"ForecastModel/Outputlist/#D-Surface,-prognostic/diagnostic-near-surface-and-soil-variables-1","page":"Output List","title":"2D Surface, prognostic/diagnostic near-surface and soil variables","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"FA name Unit Parameter Type Level Note\nSURFPRESSION Pa 1 105 0 Surface pressure\nSURFTEMPERATURE K 11 105 0 Surface temperature\nCLSTEMPERATURE K 11 105 2 T2m\nCLSMAXI.TEMPERAT K 15 105 2 Max temperature over 3h\nCLSMINI.TEMPERAT K 16 105 2 Min temperature over 3h\nCLSVENT.ZONAL ms-1 33 105 10 U10m, relative to the model coordinates\nCLSVENT.MERIDIEN ms-1 34 105 10 V10m, relative to the model coordinates\nCLSHUMI.SPECIFIQ kg kg-1 51 105 2 Q2m\nCLSHUMI.RELATIVE % 52 105 2 RH2m\nSURFRESERV.NEIGE kg m-2 65 105 0 Snow depth in water equivalent\n[[Color(green, CLPMHAUT.MOD.XFU)]] m 67 105 0 Height (in meters) of the PBL out of the model\nSURFNEBUL.TOTALE % 71 105 0 Total cloud cover\nSURFNEBUL.CONVEC % 72 105 0 Convective cloud cover\nSURFNEBUL.BASSE % 73 105 0 Low cloud cover\nSURFNEBUL.MOYENN % 74 105 0 Medium cloud cover\nSURFNEBUL.HAUTE % 75 105 0 High cloud cover\nSURFRAYT.SOLAIRE W/m2 117 105 0 [[Color(blue, Parameter identifier was 116)]] Instantaneous surface solar radiation (SW down global)\nSURFRAYT.TERREST W/m2 115 105 0 Instantaneous surface thermal radiation (LW down)\n[[Color(green, SURFCAPE.MOD.XFU)]] J kg-1 160 105 0 Model output CAPE (not calculated by AROME physics)\n~~SURFCAPE.MOD.F04~~ J kg-1 160 105 0 Postprocessed CAPE\n[[Color(green, SURFCAPE.MOD.F00)]] J kg-1 160 105 0 Postprocessed CAPE\n[[Color(green, SURFDIAGHAIL)]] % 161 105 0 AROME  hail diagnostic. LXXDIAGH = .TRUE.\nCLSU.RAF.MOD.XFU m/s 162 105 10 U-momentum of gusts from the model. LXXGST = .TRUE. in NAMXFU. gives gust between current and previous output time step.\nCLSV.RAF.MOD.XFU m/s 163 105 10 V-momentum of gusts from the model. LXXGST = .TRUE. in NAMXFU. gives gust between current and previous output time step.\nSURFINSPLUIE kg/m2 181 105 0 Instantaneous rain.\nSURFINSNEIGE kg/m2 184 105 0 Instantaneous snow.\nSURFINSGRAUPEL kg/m2 201 105 0 Instantaneous graupel.\n[[Color(green, CLSMINI.HUMI.REL)]] % 241 105 2 Min relative humidity over 3h\n[[Color(green, CLSMAXI.HUMI.REL)]] % 242 105 2 Max relative humidity over 3h\n[[Color(green, CLSRAFALES.POS)]] m/s 228 105 10 Gust wind speed","category":"page"},{"location":"ForecastModel/Outputlist/#D-Surface,-accumulated-near-surface-and-soil-variables-1","page":"Output List","title":"2D Surface, accumulated near-surface and soil variables","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Note that all these are coded with stepType=accum","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"FA name shortName iOP d pC pN level unit Description\nS065RAYT SOL CL cssw 130 0 4 11 65 J m**-2 SW net clear sky rad\nS065RAYT THER CL cslw 131 0 5 6 65 J m**-2 LW net clear sky rad\nSURFACCGRAUPEL grpl 201 0 1 75 0 kg m-2 s-1 Graupel precipitation rate\nSURFACCNEIGE snow 184 0 1 53 0 kg m**-2 Total snowfall rate water equivalent\nSURFACCPLUIE rain 181 0 1 65 0 kg m**-2 Rain\nSURFDIR NORM IRR dni 140 3 6 2 0 J m**-2 Direct normal irradiance\nSURFFLU.CHA.SENS shf 122 0 0 11 0 J m**-2 Sensible heat flux\nSURFFLU.LAT.MEVA lhe 132 0 1 193 0 J m**-2 Latent heat flux through evaporation\nSURFFLU.LAT.MSUB lhsub 244 0 1 202 0 J kg**-1 Latent Heat Sublimation\nSURFFLU.MEVAP.EA wevap 245 0 1 6 0 kg m**-2 Water evaporation\nSURFFLU.MSUBL.NE snsub 246 0 1 62 0 kg m**-2 Snow sublimation\nSURFFLU.RAY.SOLA nswrs 111 0 4 9 0 J m**-2 Net shortwave radiation flux (surface)\nSURFFLU.RAY.THER nlwrs 112 0 5 5 0 J m**-2 Net longwave radiation flux (surface)\nSURFRAYT DIR SUR swavr 116 0 4 7 0 J m**-2 Shortwave radiation flux\nSURFRAYT SOLA DE grad 117 0 4 3 0 J m**-2 Global radiation flux\nSURFRAYT THER DE lwavr 115 0 5 4 0 J m**-2 Longwave radiation flux\nSURFTENS.TURB.ME uflx 124 0 2 198 0 N m**-2 Momentum flux, u-component\nSURFTENS.TURB.ZO vflx 125 0 2 199 0 N m**-2 Momentum flux, v-component\n# tp 61 0 1 8 0 kg m**-2 Total precipitation","category":"page"},{"location":"ForecastModel/Outputlist/#D-TOA,-diagnostic-and-accumulated-variables-1","page":"Output List","title":"2D TOA, diagnostic and accumulated variables","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Note that all these are coded with time range indicator = 4.","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"     \nSOMMFLU.RAY.SOLA J m-2 113 8 0 TOA SW net radiation (Accumulated Top Solar radiation)\nSOMMFLU.RAY.THER J m-2 114 8 0 TOA LW net radiation (Accumulated Top Thermal radiation)\nSOMMRAYT.SOLAIRE W m-2 113 8 0 TOA Instantaneous SW net radiation\nSOMMRAYT.TERREST W m-2 114 8 0 TOA Instantaneous LW net radiation\nTOPRAYTDIRSOM W m-2 116 8 0 [[Color(blue, Parameter identifier was 117)]]  TOA Accumulated SW down radiation\n[[Color(green, SOMMTBOZCLEAR)]] K 170 8 0 Brightness temperature OZ clear\n[[Color(green, SOMMTBOZCLOUD)]] K 171 8 0 Brightness temperature OZ cloud\n[[Color(green, SOMMTBIRCLEAR)]] K 172 8 0 Brightness temperature IR clear\n[[Color(green, SOMMTBIRCLOUD)]] K 173 8 0 Brightness temperature IR cloud\n[[Color(green, SOMMTBWVCLEAR)]] K 174 8 0 Brightness temperature WV clear\n[[Color(green, SOMMTBWVCLOUD)]] K 175 8 0 Brightness temperature WV cloud","category":"page"},{"location":"ForecastModel/Outputlist/#D-Surface,-Postprocessed-variables-1","page":"Output List","title":"2D Surface, Postprocessed variables","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"|  FA name         | Unit  | Parameter | Type | Level | Note | | :– | :– | :– | :– | :– | :– |","category":"page"},{"location":"ForecastModel/Outputlist/#D-Surface,-constant-near-surface-and-soil-variables-1","page":"Output List","title":"2D Surface, constant near-surface and soil variables","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"FA name Unit Parameter Type Level Note\nSPECSURFGEOPOTEN m-2 s-2 6 105 0 Geopotential relative to mean sea level. \"... contains a GRID POINT orography which is the interpolation of the departure orography\"\nSURFIND.TERREMER % 81 105 0 Fr. Land (Land/sea mask)\nSURFAEROS.SEA  251 105 0 Surface aerosol  sea (Marine aerosols, locally defined GRIB)\nSURFAEROS.LAND  252 105 0 Surface aerosol land (Continental aerosols, locally defined GRIB)\nSURFAEROS.SOOT  253 105 0 Surface aerosol soot (Carbone aerosols, locally defined GRIB)\nSURFAEROS.DESERT  254 105 0 Surface aerosol desert (Desert aerosols, locally defined GRIB)\nSURFAEROS.VOLCAN  197 105 0 Surface aerosol volcan (Stratospheric ash, to be locally defined GRIB)\nSURFAEROS.SULFAT  198 105 0 Surface aerosol desert (Stratospheric sulfate, to be locally defined GRIB)\nSURFA.OF.OZONE  248 105 0 SURFA OZONE. First ozone profile (A), locally defined GRIB\nSURFB.OF.OZONE  249 105 0 SURFB OZONE. Second ozone profile (B), locally defined GRIB\nSURFC.OF.OZONE  250 105 0 SURFC OZONE. Third ozone profile (C), locally defined GRIB\nPROFTEMPERATURE K 11 112 0 Soil temperature\nSURFCAPE.POS.F00 J kg-1 160 105 0 Convective available potential energy (CAPE)\nSURFCIEN.POS.F00 J kg-1 165 105 0 Convective inhibition (CIN)\nSURFLIFTCONDLEV m 167 105 0 Lifting condensation level (LCL)\nSURFFREECONVLEV m 168 105 0 Level of free convection (LFC)\nSURFEQUILIBRLEV m 169 105 0 Level of neutral buoyancy (LNB)\nPROFRESERV.EAU kg m-2 86 112 0 Soil Wetness\nPROFPROP.RMAX.EA kg m-2 238 112 0 Deep soil wetness\nPROFRESERV.GLACE kg m-2 193 112 0 Soil ice\nPROFRESERV.GLACE kg m-2 193 112 0 Soil ice","category":"page"},{"location":"ForecastModel/Outputlist/#D-variables-on-special-surfaces-1","page":"Output List","title":"2D variables on special surfaces","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"FA name Unit Parameter Type Level Note\nKT273ISOT_ALTIT m 8 20 27315 Altitude of 0-degree isotherm\nKT263ISOT_ALTIT m 8 20 26315 Altitude of -0-degree isotherm\nSURFISOTPW0.MAL m 8 5 0 Altitude of T'w=0 isotherm\nSURFTOT.WAT.VAPO kg m-2 54 200 0 Total column water vapour","category":"page"},{"location":"ForecastModel/Outputlist/#Postprocessed-variables-on-different-surface-types-1","page":"Output List","title":"Postprocessed variables on different surface types","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Through the postprocessing sofware fullpos HARMONIE offers a number of variables postprocessed on different surface types. For the current choice of variables, surfaces and levels please see scr/Select_postp.pl.","category":"page"},{"location":"ForecastModel/Outputlist/#State-variables-and-diagnostics-on-pressure-levels,-leveltype-1","page":"Output List","title":"State variables and diagnostics on pressure levels,  leveltype","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"FA name shortName iOP d pC pN stepType unit Description\nPNNNNNWIND.U.PHY u 33 0 2 2 instant m s**-1 u-component of wind\nPNNNNNWIND.V.PHY v 34 0 2 3 instant m s**-1 v-component of wind\nPNNNNNTEMPERATUR t 11 0 0 0 instant K Temperature\nPNNNNNHUMI.SPECI q 51 0 1 0 instant kg kg**-1 Specific humidity\nPNNNNNLIQUID_WAT cwat_cond 76 0 1 83 instant kg kg**-1 Specific cloud liquid water content\nPNNNNNSOLID_WATE ciwc_cond 58 0 1 84 instant kg kg**-1 Specific cloud ice water content\nPNNNNNCLOUD_FRAC tcc 71 0 6 192 instant 0-1 Total cloud cover\nPNNNNNSNOW snow_cond 184 0 1 86 instant kg kg**-1 Specific snow water content\nPNNNNNRAIN rain_cond 181 0 1 85 instant kg kg**-1 Specific rain water content\nPNNNNNGRAUPEL grpl_cond 201 0 1 32 instant kg kg**-1 Specific graupel\nPNNNNNGEOPOTENTI z 6 0 3 4 instant m2 s-2 Geopotential\nPNNNNNHUMI_RELAT r 52 0 1 192 instant 0-1 Relative humidity\nPNNNNNTHETA_PRIM papt 14 0 0 3 instant K Pseudo-adiabatic potential temperature\nPNNNNNVERT.VELOC w 40 0 2 9 instant m s**-1 Geometrical vertical velocity\nPNNNNNPOT_VORTIC pv 4 0 2 14 instant K m2 kg-1 s**-1 Potential vorticity\nPNNNNNABS_VORTIC absv 41 0 2 10 instant s**-1 Absolute vorticity","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"NNNNN is in Pascals. \nFrom FullPos documentation: \"Warning: fields on pressure levels bigger or equal to 1000 hPa are written out with truncated names; for example, temperature at 1000 hPa is P00000TEMPERATURE while P00500TEMPERATURE could be as well the temperature at 5 hPa or the temperature at 1005 hPa!\"","category":"page"},{"location":"ForecastModel/Outputlist/#State-variables-and-diagnostics-on-height-levels,-levelTypeheightAboveGround-1","page":"Output List","title":"State variables and diagnostics on height levels, levelType=heightAboveGround","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"FA name shortName iOP d pN pN stepType unit Description\nHNNNNNWIND.U.PHY u 33 0 2 2 instant m s**-1 u-component of wind\nHNNNNNWIND.V.PHY v 34 0 2 3 instant m s**-1 v-component of wind\nHNNNNNTEMPERATUR t 11 0 0 0 instant K Temperature\nHNNNNNLIQUID_WAT cwat_cond 76 0 1 83 instant kg kg**-1 Specific cloud liquid water content\nHNNNNNSOLID_WATE ciwc_cond 58 0 1 84 instant kg kg**-1 Specific cloud ice water content\nHNNNNNCLOUD_FRAC tcc 71 0 6 192 instant 0-1 Total cloud cover\nHNNNNNSNOW snow_cond 184 0 1 86 instant kg kg**-1 Specific snow water content\nHNNNNNRAIN rain_cond 181 0 1 85 instant kg kg**-1 Specific rain water content\nHNNNNNGRAUPEL grpl_cond 201 0 1 32 instant kg kg**-1 Specific graupel\nHNNNNNHUMI_RELAT r 52 0 1 192 instant 0-1 Relative humidity\nHNNNNNPRESSURE pres 1 0 3 0 instant Pa Pressure","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"NNNNN is in meters. ","category":"page"},{"location":"ForecastModel/Outputlist/#State-variables-and-diagnostics-on-PV-levels,-GRIB1-level-type-117-1","page":"Output List","title":"State variables and diagnostics on PV levels, GRIB1 level type 117","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"FA name Unit Par Typ Lev gl name Note\nVNNNGEOPOTENTI m2 s-2 6 117 nnn Geo.Pot. \nVNNNTEMPERATUR K 11 117 nnn Temp. \nVNNNWIND.U.PHY m s-1 33 117 nnn U-comp. \nVNNNWIND.V.PHY m s-1 34 117 nnn V-comp. \nVNNNVITESSE_VE Pa s-1 39 117 nnn !VertVel. DYNAMICS=h\nVNNNVERT.VELOC m s-1 40 117 nnn !VertVel. DYNAMICS=nh\nVNNNABS_VORTIC s-1 41 117 nnn A.Vort. \nVNNNDIVERGENCE s-1 44 117 nnn Div. \nVNNNHUMI.SPECI kg kg-1 51 117 nnn Sp.Hum. \nVNNNHUMI_RELAT % 52 117 nnn Rel.Hum. ","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"\"pv\" stream is not available by default\nNNN is in deci-PVU (1PVU = 1x10-6 K m2 kg-1 s-1) in FA files. PV levels must be in SI units in namelists\nGRIB1 levels are in milli-PVU. Currently gl does not convert devi-PVU (FA) to milli-PVU (GRIB1)","category":"page"},{"location":"ForecastModel/Outputlist/#State-variables-and-diagnostics-on-Theta-levels,-GRIB1-level-type-113-1","page":"Output List","title":"State variables and diagnostics on Theta levels, GRIB1 level type 113","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"FA name Unit Par Typ Lev gl name Note\nTNNNPOT_VORTIC s-1 4 113 nnn Pot. Vorticity \nTNNNGEOPOTENTI m2 s-2 6 113 nnn Geo.Pot. \nTNNNTEMPERATUR K 11 113 nnn Temp. \nTNNNWIND.U.PHY m s-1 33 113 nnn U-comp. \nTNNNWIND.V.PHY m s-1 34 113 nnn V-comp. \nTNNNVITESSE_VE Pa s-1 39 113 nnn !VertVel. DYNAMICS=h\nTNNNVERT.VELOC m s-1 40 113 nnn !VertVel. DYNAMICS=nh\nTNNNABS_VORTIC s-1 41 113 nnn A.Vort. \nTNNNDIVERGENCE s-1 44 113 nnn Div. \nTNNNHUMI.SPECI kg kg-1 51 113 nnn Sp.Hum. \nTNNNHUMI_RELAT % 52 113 nnn Rel.Hum. ","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"\"th\" stream is not available by default\nNNN is in Kelvin.","category":"page"},{"location":"ForecastModel/Outputlist/#FA-fields-without-any-default-GRIB1-translation-1","page":"Output List","title":"FA fields without any default GRIB1 translation","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Some very special fields are left without any default translation. Please see in the gl documentation on how to add you own translation.","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"FA name Unit Comment\nCUF1PRESSURE  Coupling error field.\nTHETAPWP_FLUX K m-4 s-1 Instantaneous thetaprimwprim surface flux\nCLPMOCON.MOD.XFU kg kg-1 s-1 MOCON model output\nATMONEBUL.TOTALE  Accumulated Total cloud cover.\nATMONEBUL.CONVEC  Accumulated Convective cloud cover.\nATMONEBUL.BASSE  Accumulated Low cloud cover.\nATMONEBUL.MOYENN  Accumulated Medium cloud cover.\nATMONEBUL.HAUTE  Accumulated High cloud cover.\nSURFCFU.Q.TURBUL  Accumulated contribution of Turbulence to Q.\nSURFCFU.CT.TURBUL  Accumulated contribution of Turbulence to CpT.\nSUNSHI. DURATION  Sunshine duration.\nSURFFL.U  TURBUL  Contribution of Turbulence to U.\nSURFFL.V  TURBUL  Contribution of Turbulence to V.\nSURFFL.Q  TURBUL  Contribution of Turbulence to Q.\nSURFFL.CT TURBUL  Contribution of Turbulence to CpT.\nSNNNSRC  Second order flux.","category":"page"},{"location":"ForecastModel/Outputlist/#Variables-postprocessed-by-gl-1","page":"Output List","title":"Variables postprocessed by gl","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"The following fields are can be generated by gl from a history file and are thus not necessarily available as FA fields.","category":"page"},{"location":"ForecastModel/Outputlist/#Single-level-fields-1","page":"Output List","title":"Single level fields","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Name Unit Par Typ Lev Note\nMSLPRESSURE Pa 1 103 0 MSLP. gl calculates MSLP independent of AROME/!FullPos\n# m 20 105 0 Visibility\n#  31 ttt lll Wind direction. gl calculates based on u[33,ttt,lll] and v[34,ttt,lll] wind components\n# m s-1 32 ttt lll Wind speed. gl calculates based on u[33,ttt,lll] and v[34,ttt,lll] wind components\n# kg m-2 61 105 0 Total precipitation (TP).  gl calculates TP![61,105,0]=rain![181,105,0]+snow![184,105,0]+graupel![201,105,0]+hail![204,105,0]\n# m 67 105 0 Boundary layer height\n# - 71 105 2 [[Color(Red, Fog)]]\n# ? 135 105 0 Icing index\n# K 136 105 0 Pseudo satellite image, cloud top temperature (infrared)\n# K 137 105 0 Pseudo satellite image, water vapour brightness temperature\n# K 138 105 0 Pseudo satellite image, water vapour br. temp. + correction for clouds\n# ? 139 105 0 Pseudo satellite image, cloud water reflectivity (visible)\n# ? 144 105 0 Precipitation type\n# kg m-2 185 105 0 Total solid precipitation.  gl calculates ![185,105,0]=snow![184,105,0]+graupel![201,105,0]+hail![204,105,0]\n# m/s 228 105 10 Wind gust speed","category":"page"},{"location":"ForecastModel/Outputlist/#Integrated-quantities-1","page":"Output List","title":"Integrated quantities","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"| Name | Unit   | Par | Typ | Lev | Note                                        |                                                         | | :–  | :–    | :– | :– | :– | :–                                         |                                                         | | #    | kg m-2 | 58  | 200 | 0   | Vertical integral of cloud ice              |                                                         | | #    | kg m-2 | 76  | 200 | 0   | Vertical integral of cloud water            |                                                         | | #    | ?      | 133 | 200 | 0   | Mask of significant cloud amount            |                                                         | | #    | J kg-1 | 160 | 200 | 0   | CAPE, comes in two flavours, capeversion=1 | 2 where the second is compatible with the ECMWF version | | #    | J kg-1 | 165 | 200 | 0   | CIN, comes in two flavours, capeversion=1  | 2 where the second is compatible with the ECMWF version | | #    | kg m-2 | 181 | 200 | 0   | Vertical integral of rain                   |                                                         | | #    | kg m-2 | 184 | 200 | 0   | Vertical integral of snow                   |                                                         | | #    | kg m-2 | 201 | 200 | 0   | Vertical integral of graupel                |                                                         | | #    | m      | 186 | 200 | 0   | Cloud base                                  |                                                         | | #    | m      | 187 | 200 | 0   | Cloud top                                   |                                                         | | #    | -      | 209 | 200 | 0   | [[Color(Red, Lightning)]]                   |                                                         |","category":"page"},{"location":"ForecastModel/Outputlist/#GRIB-encoding-information-1","page":"Output List","title":"GRIB encoding information","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#Time-units,-WMO-code-table-4-1","page":"Output List","title":"Time units, WMO code table 4","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"The following time units are used to encode GRIB edition 1 data","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Code Unit\n0 Minute\n1 Hour\n13 15 minutes\n14 30 minutes","category":"page"},{"location":"ForecastModel/Outputlist/#Time-range-indicator,-WMO-code-TABLE-5-1","page":"Output List","title":"Time range indicator, WMO code TABLE 5","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Code Definition\n0 Forecast product valid for reference time + P1 (P1 > 0), or Uninitialized analysis product for reference time (P1 = 0)\n2 Product with a valid time ranging between reference time + P1 and reference time + P2. Used for min/max values\n4 Accumulation (reference time + P1 to reference time + P2) product considered valid at reference time + P2","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Note that fields available as both instanteous and accumulated values like e.g. rain has the same parameter values and can only be distinguished by the time range indicator.","category":"page"},{"location":"ForecastModel/Outputlist/#Level-types,-WMO-Code-table-3-1","page":"Output List","title":"Level types, WMO Code table 3","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"level type WMO/HIRLAM type definition Units notes\n001 Ground or water surface 0 WMO\n002 Cloud base level 0 WMO\n004 Level of 0°C isotherm 0 WMO\n006 Maximum wind level 0 WMO\n007 Tropopause 0 WMO\n008 Top-of-atmosphere  WMO\n020 Isothermal level Temperature in 1/100 K WMO\n100 Isobaric level hPa WMO\n103 Specified altitude above mean sea level Altitude in m WMO\n105 Specified height above ground Altitude in m WMO\n107 Sigma level Sigma value in 1/10000 WMO\n109 Hybrid level  WMO\n113 Isentropic (theta) level Potential temperature in K WMO\n117 Potential vorticity surface 10-9 K m2 kg-1 s-1 WMO\n200 Entire atmosphere (considered as a single layer)  WMO, vertically integrated","category":"page"},{"location":"ForecastModel/Outputlist/#Harmonie-GRIB1-code-table-2-version-253-Indicator-of-parameter-1","page":"Output List","title":"Harmonie GRIB1 code table 2 version 253 - Indicator of parameter","text":"","category":"section"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Below the indicator of parameter code table for the Harmonie model. It is based on the WMO code table 2 version 3 with local parameters added. Parameter indicators 128-254 are reserved for originating center use. Parameter indicators 000-127 should not be altered. In HARMONIE, radiation fluxes are assumed positive downwards (against the recommendation by WMO).","category":"page"},{"location":"ForecastModel/Outputlist/#","page":"Output List","title":"Output List","text":"Par Description SI Units\n000 Reserved n/a\n001 Pressure Pa\n002 Pressure reduced to MSL Pa\n003 Pressure tendency Pa s-1\n004 Potential vorticity K m2 kg-1 s-1\n005 ICAO Standard Atmosphere reference height m\n006 Geopotential m2 s-2\n007 Geopotential height gpm\n008 Geometrical height m\n009 Standard deviation of height m\n010 Total ozone Dobson\n011 Temperature K\n012 Virtual temperature K\n013 Potential temperature K\n014 Pseudo-adiabatic potential temperature K\n015 Maximum temperature K\n016 Minimum temperature K\n017 Dew-point temperature K\n018 Dew-point depression (or deficit) K\n019 Lapse rate K m-1\n020 Visibility m\n021 Radar spectra (1) -\n022 Radar spectra (2) -\n023 Radar spectra (3) -\n024 Parcel lifted index (to 500 hPa) K\n025 Temperature anomaly K\n026 Pressure anomaly Pa\n027 Geopotential height anomaly gpm\n028 Wave spectra (1) -\n029 Wave spectra (2) -\n030 Wave spectra (3) -\n031 Wind direction Degree true\n032 Wind speed m s-1\n033 u-component of wind m s-1\n034 v-component of wind m s-1\n035 Stream function m2 s-1\n036 Velocity potential m2 s-1\n037 Montgomery stream function m2 s-1\n038 Sigma coordinate vertical velocity s-1\n039 Vertical velocity Pa s-1\n040 Vertical velocity m s-1\n041 Absolute vorticity s-1\n042 Absolute divergence s-1\n043 Relative vorticity s-1\n044 Relative divergence s-1\n045 Vertical u-component shear s-1\n046 Vertical v-component shear s-1\n047 Direction of current Degree true\n048 Speed of current m s-1\n049 u-component of current m s-1\n050 v-component of current m s-1\n051 Specific humidity kg kg-1\n052 Relative humidity %\n053 Humidity mixing ratio kg kg-1\n054 Precipitable water kg m-2\n055 Vapor pressure Pa\n056 Saturation deficit Pa\n057 Evaporation kg m-2\n058 Cloud ice kg m-2\n059 Precipitation rate kg m-2 s-1\n060 Thunderstorm probability %\n061 Total precipitation kg m-2\n062 Large scale precipitation kg m-2\n063 Convective precipitation kg m-2\n064 Snowfall rate water equivalent kg m-2 s-1\n065 Water equivalent of accumulated snow depth kg m-2\n066 Snow depth m\n067 Mixed layer depth m\n068 Transient thermocline depth m\n069 Main thermocline depth m\n070 Main thermocline anomaly m\n071 Total cloud cover %\n072 Convective cloud cover %\n073 Low cloud cover %\n074 Medium cloud cover %\n075 High cloud cover %\n076 Cloud water kg m-2\n077 Best lifted index (to 500 hPa) K\n078 Convective snow kg m-2\n079 Large scale snow kg m-2\n080 Water temperature K\n081 Land cover (1 = land, 0 = sea) Proportion\n082 Deviation of sea level from mean m\n083 Surface roughness m\n084 Albedo %\n085 Soil temperature K\n086 Soil moisture content kg m-2\n087 Vegetation %\n088 Salinity kg kg-1\n089 Density kg m-3\n090 Water run-off kg m-2\n091 Ice cover (1 = ice, 0 = no ice) Proportion\n092 Ice thickness m\n093 Direction of ice drift Degree true\n094 Speed of ice drift m s-1\n095 u-component of ice drift m s-1\n096 v-component of ice drift m s-1\n097 Ice growth rate m s-1\n098 Ice divergence s-1\n099 Snow melt kg m-2\n100 Significant height of combined wind waves and swell m\n101 Direction of wind waves Degree true\n102 Significant height of wind waves m\n103 Mean period of wind waves s\n104 Direction of swell waves Degree true\n105 Significant height of swell waves m\n106 Mean period of swell waves s\n107 Primary wave direction Degree true\n108 Primary wave mean period s\n109 Secondary wave direction Degree true\n110 Secondary wave mean period s\n111 Net short-wave radiation flux (surface) W m-2\n112 Net long-wave radiation flux (surface) W m-2\n113 Net short-wave radiation flux (top of atmosphere) W m-2\n114 Net long-wave radiation flux (top of atmosphere) W m-2\n115 Long-wave radiation flux W m-2\n116 Short-wave radiation flux W m-2\n117 Global radiation flux W m-2\n118 Brightness temperature K\n119 Radiance (with respect to wave number) W m-1 sr-1\n120 Radiance (with respect to wave length) W m-3 sr-1\n121 Latent heat flux W m-2\n122 Sensible heat flux W m-2\n123 Boundary layer dissipation W m-2\n124 Momentum flux, u-component N m-2\n125 Momentum flux, v-component N m-2\n126 Wind mixing energy J\n127 Image data -\n128 Analysed RMS of PHI (CANARI) m2 s-2\n129 Forecasted RMS of PHI (CANARI) m2 s-2\n130 SW net clear sky rad W m-2\n131 LW net clear sky rad W m-2\n132 Latent heat flux through evaporation W m-2\n133 Mask of significant cloud amount #\n134 Available #\n135 Icing index Code table\n136 Pseudo satellite image, cloud top temperature (infrared) K\n137 Pseudo satellite image, water vapour brightness temperature K\n138 Pseudo satellite image, water vapour br. temp. + correction for clouds K\n139 Pseudo satellite image, cloud water reflectivity (visible) ?\n140 Direct normal irradiance J m-2\n141 Available #\n142 Available #\n143 Available #\n144 Precipition Type Code table\n145 Available #\n146 Available #\n147 Available #\n148 Available #\n149 Available #\n150 Available #\n151 Available #\n152 Available #\n153 Available #\n154 Available #\n155 Available #\n156 Available #\n157 Available #\n158 Surface downward moon radiation W m-2\n159 Available #\n160 CAPE J kg-1\n161 AROME hail diagnostic %\n162 U-momentum of gusts out of the model m s-1\n163 V-momentum of gusts out of the model m s-1\n164 Available #\n165 Convective inhibition (CIN) J kg-1\n166 MOCON out of the model kg/kg s-1\n167 Lifting condensation level (LCL) m\n168 Level of free convection (LFC) m\n169 Level of neutral boyancy (LNB) m\n170 Brightness temperature OZ clear K\n171 Brightness temperature OZ cloud K\n172 Brightness temperature IR clear K\n173 Brightness temperature IR cloud K\n174 Brightness temperature WV clear K\n175 Brightness temperature WV cloud K\n176 Virtual potential temperature K\n177 Available #\n178 Available #\n179 Available #\n180 Available #\n181 Rain kg m-2\n182 Stratiform Rain kg m-2\n183 Convective Rain kg m-2\n184 Snow kg m-2\n185 Total solid precipitation kg m-2\n186 Cloud base m\n187 Cloud top m\n188 Fraction of urban land Proportion\n189 Available #\n190 Snow Albedo Proportion\n191 Snow density kg/m3\n192 Water on canopy kg/m2\n193 Soil ice kg/m2\n194 Available #\n195 Gravity wave stress U-comp N/m2\n196 Gravity wave stress V-comp N/m2\n197 Available #\n198 Available #\n199 Vegetation type -\n200 TKE m2 s-2\n201 Graupel kg m-2\n202 Stratiform Graupel kg m-2\n203 Convective Graupel kg m-2\n204 Hail kg m-2\n205 Stratiform Hail kg m-2\n206 Convective Hail kg m-2\n207 Available #\n208 Available #\n209 Lightning -\n210 Simulated reflectivity dBz\n211 Available #\n212 Pressure departure Pa\n213 Vertical divergence s-1\n214 UD_OMEGA ms-1?\n215 DD_OMEGA ms-1?\n216 UDMESHFRAC -\n217 DDMESHFRAC -\n218 PSHICONVCL -\n219 Surface albedo for non snow covered areas Proportion\n220 Standard deviation of orography * g J/Kg\n221 Anisotropy coeff of topography -\n222 Direction of main axis of topography rad\n223 Roughness length of bare surface * g m2 s-2\n224 Roughness length for vegetation * g m2 s-2\n225 Fraction of clay within soil Proportion\n226 Fraction of sand within soil Proportion\n227 Maximum proportion of vegetation Proportion\n228 Gust wind speed m s-1\n229 Albedo of bare ground Proportion\n230 Albedo of vegetation Proportion\n231 Stomatal minimum resistance s/m\n232 Leaf area index m2/m2\n233 Thetaprimwprim surface flux Km/s\n234 Dominant vegetation index -\n235 Surface emissivity -\n236 Maximum soil depth m\n237 Soil depth m\n238 Surface liquid water content kg/m2\n239 Thermal roughness length * g m2 s-2\n240 Resistance to evapotransiration s/m\n241 Minimum relative moisture at 2 meters %\n242 Maximum relative moisture at 2 meters %\n243 Duration of total precipitations s\n244 Latent Heat Sublimation W/m2\n245 Water evaporation kg/m2\n246 Snow sublimation kg/m2\n247 Snow history ???\n248 A OZONE ???\n249 B OZONE ???\n250 C OZONE ???\n251 Surface aerosol sea ???\n252 Surface aerosol land ???\n253 Surface aerosol soot ???\n254 Surface aerosol desert ???\n255 Missing value n/a","category":"page"},{"location":"DataAssimilation/Screening/#","page":"Screening","title":"Screening","text":"EditURL=\"https://hirlam.org/trac//wiki//Screening?action=edit\"","category":"page"},{"location":"DataAssimilation/Screening/#Screening-1","page":"Screening","title":"Screening","text":"","category":"section"},{"location":"DataAssimilation/Screening/#Introduction-1","page":"Screening","title":"Introduction","text":"","category":"section"},{"location":"DataAssimilation/Screening/#","page":"Screening","title":"Screening","text":"Screening (configuration 002 of ARPEGE/IFS model) carries out quality control of observations. ","category":"page"},{"location":"DataAssimilation/Screening/#","page":"Screening","title":"Screening","text":"A useful presentation (Martin Ridal) from the \"Hirlam-B Training Week on HARMONIE system\" training course is available here: MR_screenandminim.pdf. Most of the information on this page is based on his presentation.","category":"page"},{"location":"DataAssimilation/Screening/#Inputs-1","page":"Screening","title":"Inputs","text":"","category":"section"},{"location":"DataAssimilation/Screening/#","page":"Screening","title":"Screening","text":"First guess (the same file with 5  different names):\nICMSHMIN1INIT\nICMSHMIN1IMIN\nICMRFMIN10000\nELSCFMIN1ALBC000\nELSCFMIN1ALBC\nInput/output ODB directory structure\n${d_DB}/ECMA\n${d_DB}/ECMA.${base1}\nConstants and statistics (MAY NEED TO BE UPDATED)\ncorrel.dat\nsigmab.dat\nrszcoef_fmt\nerrgrib\nrt_coef_atovs_newpred_ieee.dat\nbcor_noaa.dat\nchanspec_noaa.dat\nrmtberr_noaa.dat\ncstlim_noaa.dat\nNamelist: See %screening in nam/harmonie_namelists.pm","category":"page"},{"location":"DataAssimilation/Screening/#Screening-tasks-1","page":"Screening","title":"Screening tasks","text":"","category":"section"},{"location":"DataAssimilation/Screening/#","page":"Screening","title":"Screening","text":"(Based on Martin Ridal's presentation).","category":"page"},{"location":"DataAssimilation/Screening/#","page":"Screening","title":"Screening","text":"Preliminary check of observations\nCheck of completeness of the reports\nCheck if station altitude is present\nCheck of the reporting practice for SYNOP & TEMP mass observations \nBlacklisting: A blacklist is applied to discard observations of known poor quality and/or that cannot be properly handled by the data assimilation. A selection of variables for assimilation is done using the data selection part of the blacklist file and the information hard-coded in Arpege/Aladin (orographic rejection limit, land-sea rejection...). Decisions based on the blacklist are feedback to the CMA. Blacklisting is defined in src/bla/mf_blacklist.b \nBackground quality control: flags are assigned to observations – 1 =>  probably correct, 2 => probably incorrect, 3 => incorrect.\nVertical consistency of multilevel report:\nThe duplicated levels, in multi-level reports, are removed from the reports\nIf 4 consecutive layers are found to be of suspicious quality then these layers are rejected\nRemoval of duplicated reports\nIn case of co-located airep reports of the same observation types (time, position), some or all of the content of one of the reports is rejected\nRedundancy check\nperformed for active reports that are co-located and originate from the same station\nLAND SYNOP: the report closest to the centre of the screening time window with most active data is retained\nSHIP SYNOP: redundant if the moving platforms are within a circle of 1^o^ radius src/arpifs/obs_preproc/sufglim.F90 RSHIDIS = 111000._JPRB\nTEMP and PILOT: same stations are considered at the same time in the redundancy check\nA SYNOP mass observation is redundant if there are any TEMP geopotential height observations (made in the same time and the same station) that are no more than 50hPa above the SYNOP mass observation\nThinning: High resolution data needs to be reduced to reduce correlated errors and reduce the amount of data","category":"page"},{"location":"DataAssimilation/Screening/#Output-1","page":"Screening","title":"Output","text":"","category":"section"},{"location":"DataAssimilation/Screening/#","page":"Screening","title":"Screening","text":"The quality control information will be put into the input ECMA ODB(s) and a newly created CCMA to used by the 3DVAR minimization.","category":"page"},{"location":"DataAssimilation/Screening/#","page":"Screening","title":"Screening","text":"A valuable summary about screening decisions can be found in HM_Date_YYYYMMDDHH.html:","category":"page"},{"location":"DataAssimilation/Screening/#","page":"Screening","title":"Screening","text":"Look for “SCREENING STATISTICS” to get:\nSTATUS summary\nEVENT summary\nNumber of variables, departures and missing departures\nDiagnostic JO-table\nCCMA ODB and updated ECMA ODB","category":"page"},{"location":"DataAssimilation/Screening/#","page":"Screening","title":"Screening","text":"Screening Events listed under \"EVENT SUMMARY OF REPORTS:\"","category":"page"},{"location":"DataAssimilation/Screening/#","page":"Screening","title":"Screening","text":" Description\n1 NO DATA IN THE REPORT\n2 ALL DATA REJECTED\n3 BAD REPORTING PRACTICE\n4 REJECTED DUE TO RDB FLAG\n5 ACTIVATED DUE TO RDB FLAG\n6 ACTIVATED BY WHITELIST\n7 HORIZONTAL POSITION OUT OF RANGE\n8 VERTICAL POSITION OUT OF RANGE\n9 TIME OUT OF RANGE\n10 REDUNDANT REPORT\n11 REPORT OVER LAND\n12 REPORT OVER SEA\n13 MISSING STATION ALTITUDE\n14 MODEL SUR. TOO FAR FROM STAT. ALT.\n15 REPORT REJECTED THROUGH THE NAMELIST\n16 FAILED QUALITY CONTROL","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#HARMONIE-Vertical-Model-Level-Definitions-1","page":"Vertical Grid","title":"HARMONIE Vertical Model Level Definitions","text":"","category":"section"},{"location":"ExperimentConfiguration/VerticalGrid/#HARMONIE-vertical-coordinate-1","page":"Vertical Grid","title":"HARMONIE vertical coordinate","text":"","category":"section"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"HARMONIE model, similar to that of HIRLAM, is constructed for a general pressure based and terrain following vertical coordinate eta(pp_s), where","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"eta(0P_s) = 0","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"and ","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"eta(p_sp_s) = 1","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"The formulation corresponds to the ECMWF hybrid system. The model is formulated for a spherical coordinate system (lambda, theta), but in the code two metric coefficients (h_xh_y) have been introduced. This is done to prepare the model for any orthogonal coordinate system or map projection with axes (x,y). ","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"To represent the vertical variation of the dependent variables (U, V, T and Q), the atmosphere is divided into \"nlev\" layers. These layers are defined by the pressures at the interfaces between them (the `half-levels'). From the general expression","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"p_k+12 = A_k+12 (n) + B_k+12(n) * p_s(xy)","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"for  k=01nlev","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"the vertical surfaces for half-levels are defined. Pure pressure surfaces are obtained for B=0 and pure sigma surfaces for A=0. `full-level' pressure associated with each model level (middle of two half layers) is then determined accordingly.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#Definition-of-model-levels-in-HARMONIE-1","page":"Vertical Grid","title":"Definition of model levels in HARMONIE","text":"","category":"section"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"The script src/Vertical_levels.pl contains definition of vertical levels that have been used in the HIRLAM community for research and/or operational purposes. Currently the default model setup defines 65-level structure as derived by Per Unden, SMHI. Model level definitions for commonly used vertical structures in HARMONIE are listed below.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"FourtyLevel: HIRLAM_40 model levels (same as Hirlam 6.2.1, Nov 2003 - HIRLAM 7.0, 2006 )\nSixtyLevel: HIRLAM-60 model levels (same as Hirlam 7.1, March 2007 - 2012 )\n[wiki:MFSixtyLevel MF_60]: MF-60 model levels (same as Meteo France AROME since 2010 )\nSixtyfiveLevel: 65 model levels (same as Hirlam 7.4, March 2012 - )\nother levels: Prague87, MF70, 40 (ALADIN-40), ECMWF_60.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"Note that VLEV is the name of the set of A/B coefficients defining your levels set in ecf/config_exp.h. There are e.g. more than one definition for 60 levels. To print the levels just run  scr/Vertical_levels.pl","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"Usage: scr/Vertical_levels.pl [VLEV PRINT_OPTION] where:","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"VLEV: name of your level definition\nPRINT_OPTION=AHALF: print A coefficients for VLEV\nPRINT_OPTION=BHALF: print B coefficients for VLEV\nPRINT_OPTION=NLEV: print number of levels for VLEV\nPRINT_OPTION=NRFP3S: print NRFP3S namelist values for VLEV","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"For reference, we provide links detailing structure of the ECMWF 62 level (ensemble and seasonal forecast),  91 level (deterministic forecast) and the 137-level deterministic forecast (starting June 25 2013, 38r2)]","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"When performing HARMONIE experiment, users can select vertical levels by changing VLEV in ecf/config_exp.h. If a non-standard level number is to be chosen, the script scr/Vertical_levels.pl needs to be edited to add layer definition.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#Define-new-eta-levels-1","page":"Vertical Grid","title":"Define new eta levels","text":"","category":"section"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"A brief description and some code on how to create new eta levels can be found in New_eta.tar.gz on hirlam.org.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"There is also an interactive tool that can help you in creating a new set of levels.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"The method is based on a program by Pierre Bénard, Meteo France, that is described in this gmapdoc article.","category":"page"},{"location":"ExperimentConfiguration/VerticalGrid/#Relevant-corresponding-data-set-for-different-vertical-structure-1","page":"Vertical Grid","title":"Relevant corresponding data set for different vertical structure","text":"","category":"section"},{"location":"ExperimentConfiguration/VerticalGrid/#","page":"Vertical Grid","title":"Vertical Grid","text":"HARMONIE 3D-VAR and 4DVAR upper air data assimilation needs background error structure function for each given vertical layer structure. It is noted that the structure function data included in the reference HARMONIE repository const/jb_data is only useful for reference configuration. Users that runs 3DVAR/4DVAR are strongly recommended to derive proper structure function data following instructions in the Harmonie wiki using own data archive to avoid improper use of structure function.","category":"page"},{"location":"Overview/Binaries/#HARMONIE-binaries-1","page":"Binaries","title":"HARMONIE binaries","text":"","category":"section"},{"location":"Overview/Binaries/#","page":"Binaries","title":"Binaries","text":"An installation of HARMONIE produces the following binaries:","category":"page"},{"location":"Overview/Binaries/#","page":"Binaries","title":"Binaries","text":"ACADFA1D : Tool to generate initial and boundary data for MUSC\nADDPERT : Create initial perturbations\nADDSURF : Allows you to mix different files and add different fields\nALTO : Also known as PINUTS. Contains several diagnostic tools.\nBATOR : Generate ODB from observations in various formats\nbl95.x : Blacklist compiler, help program to generate object files from the blacklist\nBLEND : Mixes to files\nBLENDSUR : Mixes to files\ncluster : Cluster ensemble members\nCONVERT_ECOCLIMAP_PARAM : Generate binary files from ECOCLIMAP ascii files\ndcagen : ODB handling tool\ndomain_prop : Helper program to return various model domain properties\nFESTAT : Background error covariance calculations.\nfldextr : Extracts data for verification from model history files. Reads FA from HARMONIE and GRIB from ECMWF/HIRLAM.\ngl : Converts/interpolates between different file formats and projections. Used for boundary interpolation.\nIOASSIGN/ioassign : ODB IO setup\nLSMIX : Scale dependent mixing of two model states.\njbconv : Interpolates/extrapolates background error statistics files. For technical experimentation\nlfitools : FA/LFI file manipulation tool\nMASTERODB : The main binary for the forecast model, surface assimilation, climate generation, 3DVAR, fullpos and much more.\nMTEN : Computation of moist tendencies\nobsextr : Extract data for verification from BUFR files. \nobsmon : Extract data for observation monitoring\nodb98.x : ODB manipulation program\nOFFLINE : The SURFEX offline model. Also called SURFEX\noulan : Converts observations in BUFR to OBSOUL format used by BATOR\nPERTCMA : Perturbation of observations in ODB\nPERTSFC : Surface perturbation scheme\nPGD : Generates physiography files for SURFEX.\nPREGPSSOL : Processing of GNSS data\nPREP : Generate SURFEX initial files. Interpolates/translates between two SURFEX domains.\nSFXTOOLS : Converts SURFEX output between FA and LFI format.\nshuffle : Manipulation of ODB. Also called ODBTOOLS\nShuffleBufr : Split bufr data according to observation type, used in the observation preprocessing.\nSODA : Surfex offline data assimilation\nSPG : Stochastic pattern generator, https://github.com/gayfulin/SPG\nSURFEX : The SURFEX offline model. Also called OFFLINE\ntot_energy : Calculates the total energy of a model state. Is used for boundary perturbation scaling.\nxtool : Compares two FA/LFI/GRIB files.","category":"page"},{"location":"SuiteManagement/ECFLOW/#Running-Harmonie-under-ecFlow-1","page":"ECFLOW","title":"Running Harmonie under ecFlow","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#Introduction-1","page":"ECFLOW","title":"Introduction","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"This document describes how to run Harmonie under ecFlow scheduler at ECMWF. ecFlow is the ECMWF workflow manager and it has been written using python to improve maintainability, allow easier modification and introduce object orientated features as compared to the old scheduler SMS. ecFlow can be used in any HARMONIE version in and above harmonie-40h1.1.beta.1.","category":"page"},{"location":"SuiteManagement/ECFLOW/#Start-your-experiment-supervised-by-ecFlow-1","page":"ECFLOW","title":"Start your experiment supervised by ecFlow","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"Launch the experiment in the usual manner by giving start time, DTG, end time, DTGEND and other optional arguments","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"      ~hlam/Harmonie start DTG=YYYYMMDDHH","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"If successful, ecFlow will identify your experiment name and start building your binaries and run your forecast. If not, you need to examine the ecFlow log file $HM_DATA/ECF.log. $HM_DATA is defined in your Env_system file. At ECMWF $HM_DATA=$SCRATCH/hm_home/$EXP where $EXP is your experiment name.","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"The ecflow viewer stars automatically. To view any suite for your server or other servers, the server must be added to ecflowview Edit/Preferences/Servers and selected in Servers. See below on how to find the port and server name.","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"More than one experiment is not allowed with the same name monitored in the same server so Harmonie will start the server and delete previous non-active suite for you.\nFor deleting a suite manually using ecflow_client --port XXXX --host XXXX --delete force yes /suite or using the GUI Collector node+CTRL+click1 selecting ###ecflow_client --delete force yes <full_name>\nIf other manual intervention in server or client is needed you can use ecflow commands. See here.","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"At ECMWF there are two server options ECF_HOST=ecgate or ECF_HOST=ecgb-vecf where the latter available since release-43h2.beta.5. Set ECF_HOST in Env_system to choose between the servers.","category":"page"},{"location":"SuiteManagement/ECFLOW/#ecFlow-control-1","page":"ECFLOW","title":"ecFlow control","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#Finding-the-port-and-host-of-the-ecFlow-server-1","page":"ECFLOW","title":"Finding the port and host of the ecFlow server","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"Information about server variables can be found by running","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"      ecflow_server status ","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"At ECMWF you can also find ECF_PORT/ECF_HOST by checking the files under /hpc/perm/ms/$GROUP/$USER/HARMONIE, like ","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"hlam@ecgb11:~/hm_home/43_aug> ls -rlt /hpc/perm/ms/spsehlam/hlam/HARMONIE/*.ecf.*\n-rw-r-----. 1 hlam hirald     10443 Aug  8 13:11 /hpc/perm/ms/spsehlam/hlam/HARMONIE/ecgb-vecf.4531.ecf.log\n-rw-r-----. 1 hlam hirald      8804 Aug  8 13:12 /hpc/perm/ms/spsehlam/hlam/HARMONIE/ecgate.4531.ecf.log","category":"page"},{"location":"SuiteManagement/ECFLOW/#Check-the-status-of-your-server-1","page":"ECFLOW","title":"Check the status of your server","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"To check the status of your server you can use","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"ecflow_client --stats  --port ECF_POST  --host ECF_HOST","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"or","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"ecflow_client --port ECF_PORT  --host ECF_HOST  --ping","category":"page"},{"location":"SuiteManagement/ECFLOW/#Open-the-viewer-of-a-running-ecFlow-server-1","page":"ECFLOW","title":"Open the viewer of a running ecFlow server","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"If you know that your ecFlow server is running but you have no viewer attached to it you can restart the viewer:","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"ecflow_ui &","category":"page"},{"location":"SuiteManagement/ECFLOW/#Stop-your-ecFlow-server-1","page":"ECFLOW","title":"Stop your ecFlow server","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"If you are sure you're running the server on the login node of your machine you can simply run","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"ecflow_stop.sh","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"A more complete and robust way is","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"export ECF_PORT=<your port>\nexport ECF_HOST=<your server name>\necflow_client  --halt=yes\necflow_client  --check_pt\necflow_client  --terminate=yes","category":"page"},{"location":"SuiteManagement/ECFLOW/#Restart-your-ecFlow-server-1","page":"ECFLOW","title":"Restart your ecFlow server","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"If the server is not running you can start again using the script","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":" ecflow_start.sh -d /hpc/perm/ms/$GROUP/$USER/HARMONIE/","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"Again, if ecFlow is running on a different machine you have to login and start it on that machine. For the virtual server ecgb-vecf you would do","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":" ssh ecgb-vecf\n module load ecflow\n ecflow_start.sh -d /hpc/perm/ms/$GROUP/$USER/HARMONIE/","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"As an alternative you can let Harmonie start the server for you when starting your next experiment.","category":"page"},{"location":"SuiteManagement/ECFLOW/#Keep-your-ecFlow-server-alive-1","page":"ECFLOW","title":"Keep your ecFlow server alive","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"The ecFlow server running on ecgate will eventually die causing an unexpected disruption in you experiments. To prevent this you can add a cron job restarting the server e.g. every fifth minute.","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"> crontab -l\n*/5 * * * * /home/ms/$GROUP/$USER/bin/cronrun.sh ecflow_start.sh -d /hpc/perm/ms/$GROUP/$USER/HARMONIE > ~/ecflow_start.out 2>&1","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"where to small script cronrun.sh make sure you get the right environment","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"#!/bin/bash\nsource ~/.bash_profile\nmodule unload ecflow\nmodule load ecflow/4.12.0\n$@","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"The ecFlow server version may change over time.","category":"page"},{"location":"SuiteManagement/ECFLOW/#Add-another-user-to-your-ecflowviewer-1","page":"ECFLOW","title":"Add another user to your ecflowviewer","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"Sometimes it's handy to be able to follow, and control, your colleagues experiments. To be able to do this do the following steps:","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"Find the port number of your colleague as described above.\nIn the ecflowviewer choose edit->preferences->servers and fill in the appropriate host and port and give it a useful name. Click on add to save it.\nIf you click on Servers in the viewer the name should appear and you can make it visible by clicking on it.","category":"page"},{"location":"SuiteManagement/ECFLOW/#Changing-the-port-1","page":"ECFLOW","title":"Changing the port","text":"","category":"section"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"By default, the port is set by ","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"export ECF_PORT=$((1500+usernumber))","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"in mSMS.job (40h1.1), or Start_ecFlow.sh (trunk). ","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"If you want to change this number (for example, if that port is in use already), you will also need to add a -p flag when calling ecflow_start.sh as follows:","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"ecflow_start.sh -p $ECF_PORT -d $JOBOUTDIR","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"Otherwise, ecflow_start.sh tries to open the default port. ","category":"page"},{"location":"SuiteManagement/ECFLOW/#","page":"ECFLOW","title":"ECFLOW","text":"Note: if you already have an ecFlow server running at your new port number before launching an experiment, this won't be an issue. ","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Platform-Configuration-1","page":"Platform","title":"Platform Configuration","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Overview-1","page":"Platform","title":"Overview","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"This wiki page outlines the configuration files required by HARMONIE for successful compilation and running of the system.","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Basic-requirements-1","page":"Platform","title":"Basic requirements","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"All experiments require a valid host to \"setup\" an experiment using the Harmonie script. Recall from the quick start instructions that in order to setup a new experiment on your platform, called YOURHOST, using HARMONIE downloaded to PATH_TO_HARMONIE one must issue the following command:","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"cd hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie setup -r PATH_TO_HARMONIE -h YOURHOST","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"hm_home/my_exp contains:","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"Env_submit -> config-sh/submit.YOURHOST           ## YOURHOST platform specific settings\nEnv_system -> config-sh/config.YOURHOST           ## YOURHOST task submission settings\n./config-sh/hm_rev                                ## contains PATH_TO_HARMONIE\n./config-sh/Main                                  ## The script used to run HARMONIE\n./config-sh/submit.YOURHOST                       ## YOURHOST platform specific settings\n./config-sh/config.YOURHOST                       ## YOURHOST task submission settings\n./suites/harmonie.pm                              ## perl module to define ensemble settings\n./ecf/config_exp.h                                ## your experiment definition (scientific type options)\n./scr/include.ass                                 ## assimilation specific settings","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"But, what if your host configuration is not available in the HARMONIE system? Host specific configuration files in PATH_TO_HARMONIE/config-sh must be available for your host and configuration files for the compilation of the code must be available. This documentation attempts to describe what is required.","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Host-config-files-1","page":"Platform","title":"Host config files","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Env_system-config-sh/config.YOURHOST-1","page":"Platform","title":"Env_system -> config-sh/config.YOURHOST","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"The config.YOURHOST file defines host specific variables such as some input directory locations. If your YOURHOST is not already included in HARMONIE it may be work looking at config.* files in config-sh/ to see what other people have done. The table below outlines variables set in config-sh/config-sh.YOURHOST and what the variables do:","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"Variable name Description\nCOMPCENTRE controls special ECMWF solutions (such as MARS) where required. Set to LOCAL if you are unsure\nHARMONIE_CONFIG defines the config file used by Makeup compilation\nMAKEUP_BUILD_DIR location of where Makeup compiles the HARMONIE code\nMAKE_OWN_PRECOMPILED yes=>install pre-compiled code in $PRECOMPILED\nPRECOMPILED location of (optional) pre-compiled HARMONIE code\nE923_DATA_PATH location of input data for E923, climate generation\nPGD_DATA_PATH location of input data for PGD, surfex climate generation\nECOSG_DATA_PATH location of input data for ECOCLIMAP2G\nGMTED2010_DATA_PATH location of HRES DEM\nSOILGRID_DATA_PATH location of SOILGRID data\nHM_SAT_CONST location of constants for satellite assimilation\nRTTOV_COEFDIR location of RTTOV coefficients\nHM_DATA location of top working directory for the experiment\nHM_LIB location of src/scripts and compiled code\nTASK_LIMIT Maximum number of jobs submitted by ECFLOW\nRSYNC_EXCLUDE used to exclude .git* sub-directories from copy of source code for compilation\nDR_HOOK_IGNORE_SIGNALS environment variable used by Dr Hook to ignore certain \"signals\"\nHOST0 define primary host name\nHOSTN define other host name(s)\nHOST_INSTALL 0=> install on HOST0, 0:...:N => install on HOST0,...,HOSTN\nMAKE make command may need to be explicity defined. Set to make for most platforms\nMKDIR mkdir command (default: mkdir -p)\nJOBOUTDIR where ECFLOW writes its log files\nECCODES_DEFINITION_PATH location of local ecCodes definition files\nBUFR_TABLES location of local BUFR tables","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Env_submit-config-sh/submit.YOURHOST-1","page":"Platform","title":"Env_submit -> config-sh/submit.YOURHOST","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"The Env_submit file uses perl to tell the HARMONIE scheduler how to execute programs - which programs should be run on multiple processors and define batch submissions if required.","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"perl description\n%backg_job defines variables for jobs run in the background on HOST0\n%scalar_job defines variables for single processor batch jobs\n%par_job defines variables for multi-processor batch jobs\n@backg_list list of tasks to be submitted as a background job\n@scalar_list list of tasks to be submitted as a scalar job\n@par_list list of tasks to be submitted as parallel job\ndefault \"wildcard\" task name to defined default type of job for unlisted tasks","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Host-summary-1","page":"Platform","title":"Host summary","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"YOURHOST Host type batch Contact\nKNMI-Altix KNMI SGI HPC none \nLinuxPC General Linux PC no MPI none \nLinuxPC-MPI General Linux PC with MPI none \nLinuxPC-MPI-ubuntu Ubuntu Linux PC with MPI none \nMETIE.LinuxPC METIE CentOS 6 PC with MPI none Eoin Whelan\nMETIE.LinuxRH7gnu METIE Redhat 7 server with MPI none Eoin Whelan\nMETIE.fionn METIE SGI HPC PBS Eoin Whelan\nSMHI.Linda4 SMHI ???  \nSMHI.LinuxPC SMHI PC  \nbi   \ncrayx1   \ncrayxt5m   \necgb   \necgb-cca ECMWF HPC with MPI dual host slurm/PBS \nfmisms   \njumbo   \nxt5intel   \nxtpathscale   ","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Compilation-config-files-1","page":"Platform","title":"Compilation config files","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Makeup-1","page":"Platform","title":"Makeup","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"config files required for compilation of code using Makeup ...","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"More information on Makeup is available here: Build with Makeup","category":"page"},{"location":"ExperimentConfiguration/PlatformConfiguration/#Obsmon-1","page":"Platform","title":"Obsmon","text":"","category":"section"},{"location":"ExperimentConfiguration/PlatformConfiguration/#","page":"Platform","title":"Platform","text":"For config files required for compilation of obsmon check util/obsmon/config","category":"page"},{"location":"Verification/Extract4verification/#Verification-preparation-1","page":"Extract4verification","title":"Verification preparation","text":"","category":"section"},{"location":"Verification/Extract4verification/#Introduction-1","page":"Extract4verification","title":"Introduction","text":"","category":"section"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"Before we can run the verification we need to extract data for each geographical point and produce files in a format that the verification program can use. In HARMONIE there are two programs, one fore extracting model data (fldextr_grib_api) and one for observations ( obsextr ). Both are part of the util/gl_grib_api. ","category":"page"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"fldextr is capable of extracting data from several sources (HARMONIE/HIRLAM/IFS and produces so called vfld-files in ASCII format. The main tasks of the program is to:","category":"page"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"Recalculates rh,td to be over water\nInterpolates to geographical points according to a synop.list and temp.list\nDoes MSLP,RH2M,TD2M calculations if the are not available in the input file\nOptional fraction of land check.\nInterpolates to pressure levels for TEMP data.","category":"page"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"obsextr extracts conventional observations from BUFR data and creates a vobs file similar to the vfld file. It:","category":"page"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"Reads SYNOP and TEMP\nLUSE_LIST controls the usage of a station list","category":"page"},{"location":"Verification/Extract4verification/#Station-lists-used-by-verification-1","page":"Extract4verification","title":"Station lists used by verification","text":"","category":"section"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"src/Fldextr links  synop.list to $HM_LIB/util/gl_grib_api/scr/allsynop.list  and temp.list to $HM_LIB/util/gl_grib_api/scr/alltemp.list. These station lists are based on information in WMO's ''Publication No. 9, Volume A, Observing Stations and WMO Catalogue of Radiosondes. This is regularly updated by the WMO. allsynop.list and alltemp.list are updated less frequently. There is also scope to include local stations in these lists that are not included in WMO'sPublication No. 9''. The following 7-digit station identifiers are available to HIRLAM countries:","category":"page"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"Country identifier\nNorway 1000000 - 1099999\nSweden 2000000 - 2099999\nEstonia 2600000 - 2649999\nLithuania 2650000 - 2699999\nFinland 2700000 - 2799999\nIreland 3900000 - 3900000\nIceland 4000000 - 4099999\nGreenland 4200000 - 4299999\nDenmark 6000000 - 6999999\nNetherlands 6200000 - 6299999\nSpain 8000000 - 8099999","category":"page"},{"location":"Verification/Extract4verification/#Field-extraction-1","page":"Extract4verification","title":"Field extraction","text":"","category":"section"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"scr/Fldextr This script goes through all forecast files and  collects all the variables (T2m, V10m, mean sea level pressure, RH2m, Q2m, total cloudiness, precipitation + profiles)  needed in basic verification.","category":"page"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"Input parameters: none.\nData: Forecast files.\nNamelists: Station lists for surface data (ewglam.list) and radiosounding data (temp.list).\nExecutables: fldextr.\nOutput: Field extraction files (vfld${EXP}${DTG}), which are placed in EXTRARCH.","category":"page"},{"location":"Verification/Extract4verification/#Extract-observations-1","page":"Extract4verification","title":"Extract observations","text":"","category":"section"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"scr/FetchOBS scripts takes care of the observation  extraction for verification. First, the observation BUFR-file is fetched from the MARS (ExtractVEROBSfromMARS),  then all the needed data is extracted from the BUFR-files.","category":"page"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"Input parameters: none.  \nData:  Station lists for surface data (ewglam.list) and radiosounding data (temp.list). These shoud be found from SCRDIR  \nExecutables: mars, obsextr.  \nOutput: Field extraction files (vobs*), which are placed in EXTRARCH.","category":"page"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"For the continuous monitoring on hirlam.org the most recent data are kept online at ECMWF under ecgb:/scratch/ms/dk/nhz/OBS.","category":"page"},{"location":"Verification/Extract4verification/#A-general-input-format-1","page":"Extract4verification","title":"A general input format","text":"","category":"section"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"The file format for verification is a simple ascii file with a header that allows an arbitrary number of different types of point data to be included in the model vfld- or observation vobs- files.","category":"page"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"The generalized input format is defined as ","category":"page"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"nstation_synop nstation_temp version_flag  # written in fortran format '(1x,3I6)' )\n# where version_flag == 4\n# If ( nstation_synop > 0 ) we read the variables in the file, their descriptors and\n# their accumulation time\n#\nnvar_synop\nDESC_1 ACC_TIME_1\n...\nDESC_nvar_synop ACC_TIME_nvar_synop\n# Station information and data N=nstation_synop times\nstid_1 lat lon hgt val(1:nvar_synop)\n...\nstid_N lat lon hgt val(1:nvar_synop)\n\n# If ( nstation_temp > 0 )\nnlev_temp\nnvar_temp\nDESC_1 ACC_TIME_1\n..\nDESC_nvar_temp ACC_TIME_nvar_temp\n# Station information and data nstation_temp times\n# and station data nlev_temp times for each station\nstid_1 lat lon hgt\npressure(1) val(1:nvar_temp)\n...\npressure(nlev_temp) val(1:nvar_temp)\nstid_2 lat lon hgt\n...","category":"page"},{"location":"Verification/Extract4verification/#","page":"Extract4verification","title":"Extract4verification","text":"The accumulation time allows us to e.g. easily include different precipitation accumulation intervals.","category":"page"},{"location":"Overview/FileFormats/#File-formats-in-HARMONIE-1","page":"File Formats","title":"File formats in HARMONIE","text":"","category":"section"},{"location":"Overview/FileFormats/#Introduction-1","page":"File Formats","title":"Introduction","text":"","category":"section"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"The HARMONIE system reads and writes a number of different formats. ","category":"page"},{"location":"Overview/FileFormats/#FA-files-1","page":"File Formats","title":"FA files","text":"","category":"section"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"Default internal format input/output for HARMONIE for gridpoint, spectral and SURFEX data. GRIB is used as a way to pack data, but the grib record cannot be used as such.","category":"page"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"The header contains information about model domain, projection, spectral truncation, extension zone, boundary zone, vertical levels. \nOnly one date/time per file.\nFA routines are found under ifsaux/fa\nList or convert a file with gl\nOther listing tool PINUTS","category":"page"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"Read more","category":"page"},{"location":"Overview/FileFormats/#GRIB/GRIB2-1","page":"File Formats","title":"GRIB/GRIB2","text":"","category":"section"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"All FA files may be converted to GRIB after the forecast run. For the conversion between FA names and GRIB parameters check this table.","category":"page"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"List or convert a GRIB file with gl","category":"page"},{"location":"Overview/FileFormats/#NETCDF-1","page":"File Formats","title":"NETCDF","text":"","category":"section"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"In climate mode all FA files may converted to NETCDF after the forecast run. For the conversion between FA names and NETCDF parameters check util/gl/inc/nc_tab.h.","category":"page"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"For the manipulation and listing of NETCDF files we refer to standard NETCDF tools.\nNETCDF is also used as output data from some SURFEX tools.","category":"page"},{"location":"Overview/FileFormats/#BUFR-and-ODB-1","page":"File Formats","title":"BUFR and ODB","text":"","category":"section"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"BUFR is the archiving/exchange format for observations. Observation Database is used for efficient handling of observations on IFS. ODB used for both input data and feedback information.","category":"page"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"Read more about observations in HARMONIE here.","category":"page"},{"location":"Overview/FileFormats/#DDH-(LFA-files-)-1","page":"File Formats","title":"DDH (LFA files )","text":"","category":"section"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"Diagnostics by Horizontal Domains allows you to accumulate fluxes from different packages over different areas/points. ","category":"page"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"LFA files ( Autodocumented File Software )\ngmapdoc\nunder util/ddh","category":"page"},{"location":"Overview/FileFormats/#Misc-1","page":"File Formats","title":"Misc","text":"","category":"section"},{"location":"Overview/FileFormats/#","page":"File Formats","title":"File Formats","text":"vfld/vobs files in a simple ASCII format used by the verification.\nObsmon files are stored in sqlite format.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#Generation-of-climate-and-physiography-files-1","page":"Climate Generation","title":"Generation of climate and physiography files","text":"","category":"section"},{"location":"ClimateGeneration/ClimateGeneration/#Introduction-1","page":"Climate Generation","title":"Introduction","text":"","category":"section"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"The generation of climate files includes two parts. The first part is the generation of climate files for the atmospheric model, the so called  e923 configuration. The second part is the generation of the physiography information for  SURFEX. In the following we describe how it is implemented in HARMONIE.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#Input-data-for-climate-generation-1","page":"Climate Generation","title":"Input data for climate generation","text":"","category":"section"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"The location of your input data for the climate generation is defined by the HM_CLDATA environment variable defined in the config-sh/config.yourhost. At ECMWF the climate data is stored on cca here: cca:/project/hirlam/harmonie/climate_in_order","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"Information on what data to download is available here. The input data contains physiography data, topography information and climatological values determined from a one year ARPEGE assimilation experiment with a resolution of T79. ","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"In the current version the option to use pre-generated climate files has been introduced to save time for quick experiments. To use pre-generated domains you need to set USE_REF_CLIMDIR=yes in Env_system. The regenerated domains location is defined in config_exp.h and in ECMWF are located in REF_CLIMDIR=ec:/hlam/harmonie_climdir/release-43h2.1.rc1/$DOMAIN/$ECOCLIMAP_VERSION.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#Preparation-of-SURFEX-physiography-file-1","page":"Climate Generation","title":"Preparation of SURFEX physiography file","text":"","category":"section"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"SURFEX needs information about the distribution of different available tiles like nature, sea, water and town. The nature tile also needs information about type of vegetation and soiltypes. The main input sources for this are found at SURFEX physiographic maps.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"The data base for SURFEX-file preparation is located under HM_CLDATA/PGD","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"ecoclimats_v2.* : Landtypes\ngtopo30.* : Topography\nsand_fao.* : Soil type distribution\nclay_fao.* : Soil type distribution","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"The generation of SURFEX physiography file (PGD.lfi) is done in scr/Prepare_pgd. The script creates the namelist OPTIONS.nam based on the DOMAIN settings in scr/Harmonie_domains.pm. Note that the SURFEX domain is only created over the C+I area. In the namelist we set which scheme that should be activated for each tile.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"     Tile\nPHYSICS Nature Sea Water Town \nAROME ISBA SEAFLX WATFLX TEB \nALARO ISBA SEAFLX WATFLX Town as rock ","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"The program PGD produces one SURFEX physiography file PGD.lfi, which is stored in CLIMDIR directory.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"To make sure we have the same topography input for the atmospheric part we call Prepare_pgd two times. One time to produce a PGD.lfi for SURFEX and a second time to produce a PGD.fa file that can be used as input for the climate generation described below. Note that for the atmosphere the topography will be spectrally filtered and the resulting topography will be imposed on SURFEX again.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#Generation-of-a-non-SURFEX-climate-file-1","page":"Climate Generation","title":"Generation of a non SURFEX climate file","text":"","category":"section"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"scr/Climate is a script, which prepares climate file(s) for  prefered forecast range. Climate files are produced for past, present and following month. The outline of Climate is as follows:","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"Check if climate files already exists.\nCreation of namelists. The definition of domain and truncation values is taken from src/Harmonie_domains.pm.\nPart 0: Read the PGD.fa file generated by SURFEX and write it to Neworog\nPart 1: Filter  Neworog to target grid with spectral smoothing to remove 2dx waves.\nPart 2: generation of surface, soil and vegetation variables, without annual variation.\nPart 3: creation of monthly climatological values and  modification of albedo and emissivity according to the climatology of sea-ice limit.\nPart 4: definition and modification of the vegetation and surface characteristics\nPart 5: modification of fields created by step 2 and 4 over land from high resolution datasets (for each month)\nPart 6: modification of climatological values","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"The result is climate files for the previous, current and next month. The files are named after their month like m01, m02 - m12 and stored in CLIMDIR.","category":"page"},{"location":"ClimateGeneration/ClimateGeneration/#","page":"Climate Generation","title":"Climate Generation","text":"Further reference e923","category":"page"},{"location":"DataAssimilation/SingleObs/#Single-observation-impact-experiment-1","page":"Single Obs","title":"Single observation impact experiment","text":"","category":"section"},{"location":"DataAssimilation/SingleObs/#General-1","page":"Single Obs","title":"General","text":"","category":"section"},{"location":"DataAssimilation/SingleObs/#","page":"Single Obs","title":"Single Obs","text":"The results of single observation impact experiment provide useful information of the observation operator and error statistics. Among others, it is a useful tool for diagnosing background error statistics. The procedure described below is the recommended one and it has been tested on HARMONIE harmonie-43h21, with some modifications. The example below with the new system is for a AROME domain covering Denmark (DOMAIN=DKCOEXP). Three TEMP observation types have been implemented in scr/Create_singe_obs as deviations to the background:","category":"page"},{"location":"DataAssimilation/SingleObs/#","page":"Single Obs","title":"Single Obs","text":"A temperature increase of 1K\nA wind speed increase of 1 m/s\nA specific humidity reduction to 90% of the background.","category":"page"},{"location":"DataAssimilation/SingleObs/#Illustrative-example-of-single-observation-impact-experiment-on-ecgb/cca-1","page":"Single Obs","title":"Illustrative example of single observation impact experiment on ecgb/cca","text":"","category":"section"},{"location":"DataAssimilation/SingleObs/#","page":"Single Obs","title":"Single Obs","text":"create hm_home/sinob directory. Then cd hm_home/sinob.\ncreate experiment by typing ~hlam/Harmonie setup -r ~hlam/harmonie_release/tags/harmonie-43h2.1.\nEdit ecf/config_exp.h  as follows:\nset ANASURF=none,\nset SINGLEOBS=yes,\nset LSMIXBC=no,\nEdit scr/include.ass  as follows:\nset USEOBSOUL=1,\nCopy a correction of the file Createsingleobs by typing \ncp ~hlam/harmone_release/git/develop/scr/Create_single_obs scr/Create_single_obs\nCopy a correction for gl\nmkdir -p util/gl/mod/ ; cp ~hlam/harmone_release/git/develop/util/gl/mod/module_rotations.f90 util/gl/mod\nLaunch the single observation impact experiment by standing in hm_home/sinob typing:\n~hlam/Harmonie start DTG=2012061003 DTGEND=2012061006\nThe resulting analysis file be found as cca:$SCRATCH/hm_home/sinob/archive/2012/06/10/06/MXMIN1999+0000. You can now diagnose the 3D-VAR analysis increments of the sinob-experiment taking the difference between the analysis  MXMIN1999+0000 (analysis) and the first guess, cca:$SCRATCH/hm_home/sinob/archive/2012/06/10/03/ICMSHHARM+0003. Plot horizontal and vertical cross-sections of temperature and other variables using your favorite software (EpyGram for example). ","category":"page"},{"location":"DataAssimilation/SingleObs/#","page":"Single Obs","title":"Single Obs","text":"Note that you can change position of observation, observation error, variable to be observed etc. Investigate these options by taking a closer look at the script Createsingleobs.","category":"page"},{"location":"DataAssimilation/SingleObs/#","page":"Single Obs","title":"Single Obs","text":"Read more about radiance single observation experiments here. In ec:/smx/sinob_wiki_ml you will also find OBSOUL_amsua7, a file for generating a satellati radiance amsu a channel 7 single observation impact experiment.","category":"page"},{"location":"DataAssimilation/CHKEVO/#ECHKEVO-1","page":"CHKEVO","title":"ECHKEVO","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/#Introduction-1","page":"CHKEVO","title":"Introduction","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"This page describes how to activate CHKEVO for diagnosing forecast model spin-up of pressure. This diagnostic is available in trunk from r16488. Yann Michel (MF) kindly suggested some of the changes required. The diagnostics are generated as part of a forecast model run up to 3 h or 6 h. A known problem is that the method fails when the first lateral boundary conditions are read by the model. The suggestion is to use BDINT=3 and forecast length 3 h. FULL-POS should also be deactivated in config_exp.h.","category":"page"},{"location":"DataAssimilation/CHKEVO/#Preparations-1","page":"CHKEVO","title":"Preparations","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"It is assumed you already have a well defined experiment called your_exp. The following instructions are valid for a 3h diagnostic forecast.","category":"page"},{"location":"DataAssimilation/CHKEVO/#NAMCHK-namelist-1","page":"CHKEVO","title":"NAMCHK namelist","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"Enable CHKEVO in the namelist (in the %arome entries):\ncd $HOME/hm_home/your_exp\n~hlam/Harmonie co nam/harmonie_namelists.pm\nEdit NAMCHK:\nNAMCHK=>{\n'LECHKEVO' => '.TRUE.,',\n'LECHKTND' => '.TRUE.,',\n'LECHKPS' => '.TRUE.,',\n},","category":"page"},{"location":"DataAssimilation/CHKEVO/#ecf/config_exp.h-1","page":"CHKEVO","title":"ecf/config_exp.h","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"Edit your ecf/config_exp.h  as follows:\nPOSTP=\"none\"                          # Postprocessing by Fullpos (inline|offline|none).\nBDINT=3\nHH_LIST=\"00-21:3\"                     # Which cycles to run, replaces FCINT\nLL_LIST=\"3\"                           # Forecast lengths for the cycles [h], replaces LL, LLMAIN\nAlternatively for a 6 h diagnostic forecast:\nPOSTP=\"none\"                          # Postprocessing by Fullpos (inline|offline|none).\nBDINT=6\nHH_LIST=\"00-18:6\"                     # Which cycles to run, replaces FCINT\nLL_LIST=\"6\"                           # Forecast lengths for the cycles [h], replaces LL, LLMAIN","category":"page"},{"location":"DataAssimilation/CHKEVO/#Results-1","page":"CHKEVO","title":"Results","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"After running the forecast with CHKEVO activated the statistics of surface pressure tendencies are written to NODE.001_01 log file. This log file is included in the HM_Date_YYYYMMDDHH.html log file (written to $SCRATCH/hm_home/your_exp/archive/log on ecgate). The results can be obtained by grepping the log file as follows:","category":"page"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"grep \"^ CHKEVO : \" HM_Date_2013041118.html | tail -n +2","category":"page"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"This gives the RMS and AVG of pressure tendency for each time step. (The first line is removed as the reading of the start file produces zeros):","category":"page"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":" CHKEVO :   2.5683273661035013       0.42575646791552352     \n CHKEVO :   2.5432078820872874       0.36700119757663685     \n CHKEVO :   1.4402533781888094       0.23533175032737094     \n CHKEVO :   1.3677546254375832       0.22965677860570116     \n CHKEVO :   1.1506125378848564       0.20575065246468008     \n CHKEVO :  0.98597708942270756       0.19299583141063531\n.....","category":"page"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"The first column contains the string CHKEVO :\nSecond column contains the RMS of dps/dt averaged over the domain.\nSecond column contains the AVG of dps/dt averaged over the domain.","category":"page"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"The RMS of dps/dt alone can be extracted with:","category":"page"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"grep \"^ CHKEVO : \" HM_Date_2013041118.html | tail -n +2 | awk '{print $3}'","category":"page"},{"location":"DataAssimilation/CHKEVO/#Plotting-1","page":"CHKEVO","title":"Plotting","text":"","category":"section"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"Some instructions here ...","category":"page"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"grep \"^ CHKEVO : \" HM_Date_2013041118.html | tail -n +2 > dps.dat\nwget https://hirlam.org/trac/raw-attachment/wiki/HarmonieSystemDocumentation/CHKEVO/plotCHKEVO.py\nchmod 755 ./plotCHKEVO.py\n./plotCHKEVO.py -h\n./plotCHKEVO.py -i ps.dat  && eog plot.png\n# OR\n./plotCHKEVO.py -i ps.dat -t 75  && eog plot.png","category":"page"},{"location":"DataAssimilation/CHKEVO/#","page":"CHKEVO","title":"CHKEVO","text":"To get the unit of hPa/3h the time-step is taken into account using the -t option. If, for example, the time-step is 60 s the values in the second column will be multiplied with 60.*3. and then divided by 100.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#Controlling-the-namelists-in-HARMONIE-1","page":"Namelist","title":"Controlling the namelists in HARMONIE","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/#Introduction-1","page":"Namelist","title":"Introduction","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"IFS is largely driven by namelists and has thousands of options. For each configuration a number of namelists controlling different parts are read. To make the maintenance of the namelists manageable and to assure consistency in terms on e.g. name conventions for fields, packing accuracy, physics settings and parallel options all namelists are generated as they are needed during the run. All the basic settings are defined in a perl dictionary nam/harmonie_namelists.pm for IFS and nam/surfex_namelists.pm and nam/surfex_selected_output.pm for SURFEX. surfex reference namelist settings. In future versions the commented namelists will be included and maintained as part of the code.   The IFS dictionary is structured in several sections:","category":"page"},{"location":"ExperimentConfiguration/Namelists/#harmone_namelists.pm-1","page":"Namelist","title":"harmone_namelists.pm","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"Global settings\nTechnical settings\nHost specific settings\nMPP options\nFile settings\nDYNAMICS SETTINGS\nMain dynamics switches\nNon-hydrostatic settings\nVertical finite element\nDFI\nMain physics options. NB! These may contain switches for dynamics as well\nALADIN\nAROME\nEDMFM switches, to be applied after AROME\nAlaro\nOld surface\nSURFEX\nDDH\nE927 Interpolation settings\nMain fullpos settings\nE927\nE927 nh\nSURFEX initial file generation\nAladin e927\nALARO e927\nArome e927\nGeneral postprocessing switches \nDefault fullpos settings\nNH postprocessing\nSwitches for postprocessing with surfex\nSpecial cases for arome\nAssimilation\nCanari\nArome canari\nVarbc_rad\nVarbc_coldstart\nScreening\nArome screening\nAlaro screening\nMinimization\nAlaro minimization\n4DVAR \n4DVAR minimization\n4DVAR screening\n4DVAR trajectory\nClimate generation\nClimate generations (e923)\nMisc\nGeneral namelist settings for Tangent-Linear and Adjoint tests \nExtra Adjoint test options \nOulan\nBator","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"The final namelists are build through the rules given in scr/Get_namelist and are generated by nam/gen_namelists.pl. Note that in several cases environment variables are still parsed in the scripts.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#surfex_namelists.pm-1","page":"Namelist","title":"surfex_namelists.pm","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"All possible SURFEX namelist setting are documented at the SURFEX web site. Use the search text area in the upper right corner to search for a specific namelist or namelist option. Please, keep in mind that the SURFEX web site documents the latest SURFEX version, i.e. SURFEXv8, while in cy40h SURFEXv7.3 is used. Therefore, some of the settings may be different or not available.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#PGD-1","page":"Namelist","title":"PGD","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"PGD represents in general the preparation of physiography data. The default PGD settings are listed here. Some modifications can be done for specific model configurations and will be specified as e.g. alaro_pgd or arome_pgd.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"# 2 layer ISBA scheme\n%isba_2L=(\n NAM_ISBA=>{\n  CISBA          => '\"2-L\",',\n  NGROUND_LAYER  => '2,',\n },\n);\n\n# 3 layer ISBA scheme\n%isba_3L=(\n NAM_ISBA=>{\n  CISBA          => '\"3-L\",',\n  NGROUND_LAYER  => '3,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"The section used here is decided by setting CISBA=\"3-L\" (default) in config_exp.h.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"%isba_pgd=(\n NAM_ISBA=>{\n   YCLAY         => 'YCLAY,',\n   YCLAYFILETYPE => '\"DIRECT\",' ,\n   YSAND         => 'YSAND,',\n   YSANDFILETYPE => '\"DIRECT\",'\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"The clay/sand used is decided by setting SOIL_TEXTURE_VERSION=FAO (default) in config_exp.h. FAO corresponds to 10 km resolution clay/sand according to SURFEX Soil texture description. The 1 km HWSD_v2 is not used by default since is shows strange vales over Scandinavia.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"%pgd=(\n NAM_IO_OFFLINE=>{\n  CSURF_FILETYPE => 'CSURF_FILETYPE,',\n   CPGDFILE      => 'CPGDFILE,',\n },\n NAM_PGD_GRID=>{\n   CGRID         => '\"CONF PROJ\",',\n },\n NAM_CONF_PROJ=>{\n   XLAT0         => $ENV{LAT0},\n   XLON0         => $ENV{LON0},\n   XRPK          => $ENV{SINLAT0},\n   XBETA         => 0.0,\n },\n NAM_CONF_PROJ_GRID=>{\n   XLATCEN       => $ENV{LATC},\n   XLONCEN       => $ENV{LONC},\n   NIMAX         => 'BNIMAX,',\n   NJMAX         => 'BNJMAX,',\n   XDX           => 'BXDX,', \n   XDY           => 'BXDY,',\n },\n NAM_COVER=> {\n   YCOVER        => 'YCOVER,',\n   YCOVERFILETYPE=> '\"DIRECT\",',\n },\n NAM_ZS=>{\n   YZS           => 'YTOPO,',\n   YZSFILETYPE   => '\"DIRECT\",',\n },\n NAM_SEABATHY=>{\n   XUNIF_SEABATHY=>'0.,', \n },\n NAM_PGD_SCHEMES=>{\n    CNATURE => '\"ISBA \",',\n    CSEA    => '\"SEAFLX\",',\n    CWATER  => '\"WATFLX\",',\n    CTOWN   => '\"TEB \",',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"Most options here are specified in different scripts and are normally not supposed to be modified. Exceptions are e.g.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"ECOCLIMAP version used (YCOVER) is decided by setting ECOCLIMAP_VERSION=2.2 (default) in config_exp.h. Available ECOCLIMAP versions provided via the SURFEX team are documented in SURFEX Land use description. Some of these can be available on your system, see your $HM_CLDATA setting.\nOrography version used (YTOPO) is decided by setting TOPO_SOURCE=gmted2010 (default) in config_exp.h.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#PREP-1","page":"Namelist","title":"PREP","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"PREP represents in general initialisation of prognostic variables. The default PREP settings are listed here. Some modifications can be done for specific model configurations and will be specified as e.g. alaro_prep or arome_prep.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"%sice_prep=(\n NAM_SEAFLUXn=>{\n    LHANDLE_SIC            =>'.TRUE.,',\n    LSIC_FROM_FILE         =>'.TRUE.,',\n    CSEA_ICE               =>'\"SICE\",',\n    NICE_LAYER             =>'4,',\n },\n NAM_PREP_SEAFLUX=>{\n    CFILE_SIC           =>'\"CFILE_SIC\",',\n    CTYPE_SIC           =>'\"GRIB  \"',\n },\n NAM_SIMPLE_ICE=>{\n    XICE_THICKNESS      =>'.75,',\n    LICE_HAS_SNOW       =>'.FALSE.,',\n    CICE_SNOW           =>'\"S-D\",',\n    NICE_SNOW_NLAYERS   =>'4,',\n    XICE_SNOW_HEIGHT    =>'.3,',\n    LSIC_DRIVEN_THICKNESS=>'.FALSE.',\n    XSIC_DRIVEN_MAX_THICKNESS=>'1.',\n },\n NAM_PREP_SIMPLE_ICE=>{\n    LINIT_FROM_SST => '.TRUE.,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"CSEA_ICE : selects preferred sea ice scheme.\nNONE – use default SURFEX ICEFLUX diagnostic scheme \nSICE – use SICE\nLHANDLE_SIC : activates ice fraction handling\nLSIC_FROM_FILE : ice fraction data from an external source\n.TRUE. – use the externally provided ice fraction data\n.FALSE. – assume that ice fraction data is encoded within SST field\nNICE_LAYER : number of ice layers (2 < NICE_LAYER < 100)\nCFILE_SIC : external data file that stores ice fraction data\nCTYPE_SIC : type of external data source\nXICE_THICKNESS : uniform thickness of the ice field\nLICE_HAS_SNOW : snow upon the ice\n.TRUE. – preform snow-enabled run\n.FALSE. – use bare ice configuration\nCICE_SNOW : snow scheme that used to parametrize snow upon the ice\nS-D – test heat diffusion scheme\nNICE_SNOW_NLAYERS : number of snow layers for S-D snow scheme\nXICE_SNOW_HEIGHT : thickness of snow pack for S-D snow scheme\n3-L – use 3-L explicit snow scheme \nLSIC_DRIVEN_THICKNESS : use ice fraction to estimate ice thickness. Untested, should be set to .FALSE.\nXSIC_DRIVEN_MAX_THICKNESS : unused if LSIC_DRIVEN_THICKNESS == .FALSE.\nLINIT_FROM_SST : use composite SST/SIST field to initialize sea ice temperatures","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"This part represents settings connected to the Simple Sea-ice Scheme (SICE) by Yurii Batrak. Please note that SICE is not yet an official contribution to SURFEX and therefore you will not find any documentation of SICE via the SURFEX web site. ","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"%isba_prep=(\n NAM_PREP_ISBA=>{\n   LISBA_CANOPY  => '.TRUE.,',\n   LEXTRAP_TG    => '.TRUE.,',\n   LEXTRAP_WG    => '.TRUE.,',\n   LEXTRAP_WGI   => '.TRUE.,',\n   LEXTRAP_SN    => '.TRUE.,',\n   NDIM_EXTRAP   => '20,',\n },\n NAM_PREP_ISBA_SNOW=>{\n   LSWEMAX       => '.TRUE.,',\n },\n NAM_DIAG_ISBAn=>{\n   LPATCH_BUDGET=>'.TRUE.,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"LISBA_CANOPY : activates surface boundary multi layer scheme over vegetation\nLEXTRAP_XX : extrapolate XX points where LSM < 0.5 (buffer only)\nNDIM_EXTRAP : Size of search domain for extrapolation (not in official SURFEX)\nLSWEMAX : logical switch to set an upper limit on initial snow water equivalent (set by XSWEMAX (=500 kg/m2 default) ).","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"%fullpos_prep=(\n NAM_FILE_NAMES=>{\n   HPGDFILE      =>'\"PGDFILE\",',\n   CINIFILE      =>'\"SURFXINI\",',\n },\n);\n\n%offline_prep=(\n NAM_IO_OFFLINE=>{\n   CSURF_FILETYPE       => '\"LFI   \",',\n   CPREPFILE            => '\"SURFXINI\",',\n   CPGDFILE             => '\"PGD\",',\n },\n NAM_PREP_SURF_ATM=>{\n   CFILEPGD      =>'\"PGD_host\",',\n   CFILEPGDTYPE  =>'\"LFI\",',\n   CFILE      =>'\"INFILE\",',\n   CFILETYPE  =>'CFILETYPE,',\n   NYEAR      =>'NYEAR,',\n   NMONTH     =>'NMONTH,',\n   NDAY       =>'NDAY,',\n   XTIME      =>'XTIME,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"Most options here are specified in different scripts and are normally not supposed to be modified.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"%arome_prep=(\n NAM_PREP_SEAFLUX=>{\n   LSEA_SBL     => '.FALSE.,',\n },\n NAM_SEAFLUXn=>{\n    CSEA_ICE               =>'\"NONE\",',\n    LHANDLE_SIC            =>'.FALSE.,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"Please note these special settings for the AROME model configuration.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#FORECAST-1","page":"Namelist","title":"FORECAST","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"FORECAST represents in general the namelist settings used during Forecast. The default FORECAST settings are listed here. Some modifications can be done for specific model configurations and will be specified as e.g. alaro_forecast or arome_forecast.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"%sice_forecast=(\n NAM_SEAFLUXn=>{\n    LHANDLE_SIC            =>'.TRUE.,',\n    LSIC_FROM_FILE         =>'.TRUE.,',\n    CSEA_ICE               =>'\"SICE\",',\n    NICE_LAYER             =>'4,',\n },\n NAM_SIMPLE_ICE=>{\n    XICE_THICKNESS            => '.75,',\n    LICE_HAS_SNOW             => '.FALSE.,',\n    CICE_SNOW                 => '\"S-D\",',\n    NICE_SNOW_NLAYERS         => '4,',\n    XICE_SNOW_HEIGHT          => '.3,',\n    LSIC_DRIVEN_THICKNESS     => '.FALSE.',\n    XSIC_DRIVEN_MAX_THICKNESS => '1.',\n },\n NAM_DIAG_SURFn=>{\n   LCOEF             => '.TRUE.,',\n   LSURF_VARS        => '.TRUE.,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"This part represents settings connected to the Simple Sea-ice Scheme (SICE) by Yurii Batrak. Please note that SICE is not yet an official contribution to SURFEX and therefore you will not find any documentation of SICE via the SURFEX web site.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"%isba_forecast=(\n NAM_ISBAn=>{\n   CROUGH        => '\"NONE\",',\n },\n NAM_DIAG_ISBAn=>{\n   LPGD     => '.TRUE.,',\n   LSURF_MISC_BUDGET=> '.TRUE.,',\n },\n);\n\n%forecast=(\n NAM_IO_OFFLINE=>{\n  'CSURF_FILETYPE'      => '\"LFI    \",',\n  'CFORCING_FILETYPE'   => '\"ASCII\",',\n  'CTIMESERIES_FILETYPE'=> '\"LFI\",',\n  'XTSTEP_SURF'         => $ENV{TSTEP}.\",\",\n  'XTSTEP_OUTPUT'       => '3600.,',\n  'LRESTART'            => '.TRUE.,',\n  'CPREPFILE'           => '\"PREP\",',\n  'CPGDFILE'            => '\"PGD\",',\n },\n NAM_SURF_ATM=>{\n   XRIMAX=>'0.0,',\n },\n NAM_DIAG_SURFn=>{\n   LSURF_BUDGET      => '.TRUE.,',\n   N2M      =>'2,',\n },\n NAM_DIAG_SURF_ATMn=>{\n   LT2MMW            => '.TRUE.,',\n },\n NAM_DIAG_ISBAn=>{\n   LPATCH_BUDGET  => '.TRUE.,',\n },\n NAM_SSOn=>{\n   CROUGH   => \"'\".$ENV{CROUGH}.\"'\",\n   XFRACZ0  => '15.,',\n },\n NAM_SEAFLUXn=>{\n  CSEA_FLUX => '\"ECUME\",',\n  LPWG      => '.FALSE.,',\n  LPRECIP   => '.FALSE.,',\n  LPWEBB    => '.FALSE.,',\n  CSEA_ICE    =>'\"NONE\",',\n  LHANDLE_SIC =>'.FALSE.,',\n  LPERTFLUX   => 'LPERTSURF,',\n },\n NAM_ISBAn=>{\n  LCANOPY_DRAG => '.TRUE.,',\n  XCDRAG       => '0.01,',\n  LPERTSURF    => 'LPERTSURF,',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"XRIMAX: limitation of Richardson number in drag computation (=0 default).\nN2M : flag to compute surface boundary layer characteristics (=0 default). N2M=2 computes temperature at 2 m, specific humidity at 2 m, relative humidity, zonal and meridian wind at 10 m, and Richardson number. 2m and 10m quantities are calculated interpolating between atmospheric forcing variables and surface temperature and humidity. Please note that if the surface boundary multi layer scheme is activated over any tile (as with LISBA_CANOPY=T over land) it overrides the diagnostic N2M method. \nLT2MMW : Alternative weighting of grid average T2M giving more weight to the land tile (=FALSE default).\nCROUGH: type of orographic roughness length. CROUGH is decided by setting CROUGH=\"NONE\" in config_exp.h which means that no orographic treatment is applied.\nXFRACZ0 : Z0=Min(Z0, Href/XFRACZ0). Not applied here since CROUGH=\"NONE\".\nLPERTFLUX: multiplicative perturbation of Ecume fluxes for ensemble forecasting. In HARMONIE this is set in a number of scripts under scr.\nLPERTSURF: if .True. modification of surface fluxes for ensemble forecasting. In HARMONIE this is set in a number of scripts under scr.\nLCANOPY_DRAG: drag activated in SBL scheme within the canopy.\nXCDRAG: drag coefficient in canopy (=0.15 default in SURFEX).","category":"page"},{"location":"ExperimentConfiguration/Namelists/#ASSIMILATION-1","page":"Namelist","title":"ASSIMILATION","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"ASSIMILATION represents in general the surface assimilation namelist settings used for CANARI and SURFEX. The default ASSIMILATION settings are listed here. The SURFEX assimilation method used (OI or EKF) is decided by setting ANASURF=CANARI_OI_MAIN in config_exp.h. OI is default and EKF is still experimental.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"%sice_assim=(\n NAM_SEAFLUXn=>{\n    LHANDLE_SIC            =>'.TRUE.,',\n    LSIC_FROM_FILE         =>'.TRUE.,',\n    CSEA_ICE               =>'\"SICE\",',\n    NICE_LAYER             =>'4,',\n },\n NAM_SIMPLE_ICE=>{\n   LICE_HAS_SNOW       =>'.FALSE.,',\n   XICE_THICKNESS      =>'.75,',\n   CICE_SNOW           =>'\"S-D\",',\n   NICE_SNOW_NLAYERS   =>'4,',\n   XICE_SNOW_HEIGHT    =>'.3,',\n   LSIC_DRIVEN_THICKNESS      =>'.FALSE.',\n   XSIC_DRIVEN_MAX_THICKNESS  =>'1.',\n \n},\n NAM_PREP_SEAFLUX=>{\n    CFILE_SIC          => 'CFILE_SIC,',\n    CTYPE_SIC          => '\"GRIB  \"',\n    LSEA_SBL           => '.FALSE.',\n },\n NAM_PREP_SIMPLE_ICE=>{\n    LINIT_FROM_SST             =>'.TRUE.,',\n    LPREP_ONLY_NEW_ICE         =>'.TRUE.,',\n    LEXTRAPOLATE_FROM_FORECAST =>'.TRUE.,',\n    CFORECAST_GRIB             =>'CFORECAST_GRIB',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"LPREP_ONLY_NEW_ICE : perform prep in current grid cell only if ice state is undefined\nLEXTRAPOLATE_FROM_FORECAST : use previous forecast to fill areas with new ice. If LINIT_FROM_SST == .TRUE. .AND. LEXTRAPOLATE_FROM_FORECAST == .TRUE. option LINIT_FROM_SST is forced to .FALSE.\nCFORECAST_GRIB : previous forecast in GRIB format","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"This part represents settings connected to the Simple Sea-ice Scheme (SICE) by Yurii Batrak. Please note that SICE is not yet an official contribution to SURFEX and therefore you will not find any documentation of SICE via the SURFEX web site. ","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"%assim_surfex=(\n NAM_NACVEG=>{\n   NECHGU      => ''.$ENV{FCINT}.',',\n   RCLIMCA     => '0.,',\n   RCLISST     => '0.05,',\n   SIGH2MO     => '0.10,',\n   SIGT2MO     => '1.0,',\n   LOBS2M      => '.TRUE.,',\n   LOBSWG      => '.FALSE.,',\n },\n NAM_IO_OFFLINE=>{\n   CSURF_FILETYPE       => 'CSURF_FILETYPE,',\n   CTIMESERIES_FILETYPE => '\"LFI \",',\n   CFORCING_FILETYPE    => '\"ASCII\",',\n   LRESTART             => '.TRUE.,',\n   XTSTEP_SURF          => '3600.,',\n   XTSTEP_OUTPUT        => '3600.,',\n },\n NAM_ASSIM=>{\n   LASSIM              => '.TRUE.,',\n   LEXTRAP_WATER       => '.TRUE.,',\n   LEXTRAP_SEA         => '.FALSE.,',\n   LEXTRAP_NATURE      => '.FALSE.',\n   LREAD_SST_FROM_FILE => '.TRUE.,',\n   LWATERTG2           => '.TRUE.,',\n   LAESNM              => 'LAESNM,',\n   LECSST              => 'LECSST,',\n   LAROME              => 'LAROME,',\n   NPRINTLEV           => '1,',\n },\n);\n\n%oi_main=(\n NAM_ASSIM=>{\n   CASSIM_ISBA         => '\"OI\",',\n },\n);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#surfex_selected_output.pm-1","page":"Namelist","title":"surfex_selected_output.pm","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"The output from SURFEX to the .sfx fa-files (e.g. ICMSHHARM+0002.sfx) is in general decided by SURFEX NAM_DIAG namelist settings. These settings activate or deactivate groups of variables in output files. When one or more such groups are activated it is possible to limit the output to a specific list of variables by the use of the LSELECT/CSELECT options in SURFEX. This way of specifying output from SURFEX is the default way in cy40h. The setting is SURFEX_LSELECT=\"yes\" in config_exp.h. When SURFEX_LSELECT=\"yes\" the namelist nam/surfex_selected_output.pm is used to specify the output variables. The style of surfex_selected_output.pm is","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"%surfex_output=(\n NAM_WRITE_DIAG_SURFn=>{\n   NSTEP_DUMP_STATE => 'NSTEP_DUMP_STATE,',\n   LSELECT     => '.TRUE.,',\n   CSELECT     => '\n\"Z0\",\n\"RNC\",\n\"HC\",\n\n\n\"T2M_P\",\n\"T2MMIN_P\",\n\"T2MMAX_P\",'\n },);","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"Please note the \" ' \" character at the end of the last variable line \"T2MMAX_P\",'. Don't miss it, it is very important! The naming convention for variables in this file follows the SURFEX naming convention which is not exactly how they appear in fa-files. A fa-file output example from a .sfx file may look:","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"SFX.SST         > 001:011-  000-102@20160301_15:00+002h00m tri:000 000 SST\nSFX.TS_WATER    > 001:011-  770-105@20160301_15:00+002h00m tri:000 000 TS_WATER\nX001TG1         >                   20160301_15:00+002h00m         000\nX001TG2         >                   20160301_15:00+002h00m         000\nX001WG1         >                   20160301_15:00+002h00m         000\nX001WG2         >                   20160301_15:00+002h00m         000\nSFX.TROAD1      >                   20160301_15:00+002h00m         000\nSFX.WS_ROAD     > 001:024-  950-105@20160301_15:00+002h00m tri:000 000 WS_ROAD","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"You get the corresponding SURFEX names by removing \"SFX.\" or \"X001\" at the beginning of these fa-names.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"The .sfx files are used as first guess for next forecast. However, the first guess needs ALL SURFEX variables and not only a subset as defined by LSELECT/CSELECT. Therefore, a full .sfx file (e.g. ICMSHFULL+0003.sfx) is created for each assimilation cycle hour in addition to the corresponding limited file ICMSHHARM+0003.sfx. The output frequency of full files is defined by SURFEX_DUMP_STATE_STEPS=\"\" in config_exp.h.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"The content of the FULL-files can be used to identify additional variables to add to surfex_selected_output.pm.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#Change-your-namelists-1","page":"Namelist","title":"Change your namelists","text":"","category":"section"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"There are several ways of changing namelists generated from the dictionary.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"Copy the harmonie_namelist.pm file to your local experiment directory and change the right section like for any source or script modification.\nIf you feel uncertain where to change in the dictionary you can copy the actual namelist used in your run. Every namelist used is listed in the logfile so copy it from there and put it under the nam directory in your local experiment. Make sure you give it a unique name. You must then also change the script(s) using this namelist like in the Forecast script scr/Forecast.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"#  Get namelist name\n#NAMELIST=$WRK/$WDIR/namelist_forecast\n#Get_namelist forecast $NAMELIST\nNAMELIST=$HM_LIB/nam/namelist_forecast_with_a_unique_name\n","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"For namelists not present in the dictionary you just copy them to you local nam directory.","category":"page"},{"location":"ExperimentConfiguration/Namelists/#","page":"Namelist","title":"Namelist","text":"There is also a description on how to generate new namelist dictionaries here.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#Model-Domain-1","page":"Domain","title":"Model Domain","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/#Introduction-1","page":"Domain","title":"Introduction","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"There are four projections available in HARMONIE, polar stereographic, lambert, mercator and rotated mercator. The model itself chooses the best (least distortion) projection among the first three given your domain specifications. The rotated mercator projection is selected through the variable LROTMER. Note that the polar stereographic project is defined at 90^o N(S) whereas in GRIB1 it is defined at 60^o N(S).","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"(Image: projections)","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"Polar stereographic, Lambert and Mercator projection.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"(Image: Rotated Mercator)","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"Rotated mercator projection","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#Model-domain-settings-1","page":"Domain","title":"Model domain settings","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"For each domain we set variables related to the geometry and the resolution like:","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"HARMONIE model domains are defined in settings in scr/Harmonie_domains.pm. The following variables related to the geometry and the resolution are required:","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"TSTEP is model timestep in seconds\nNLON is number of points in x-direction.\nNLAT is number of points in y-direction.\nLONC is the longitude of domain centre in degrees.\nLATC is the latitude of domain center in degrees.\nLON0 is the reference longitude of the projection in degrees.\nLAT0 is the reference latitude of the projection in degrees. If LAT0 is set to 90, the projection is polar stereographic. If LAT0 < 90, the projection is lambert unless LMRT=.TRUE.  \nGSIZE is grid size in meters in both x- and y-direction.\nEZONE is number of points over extension zone in both x- and y-direction. Default value 11. \nLMRT switch for rotated Mercator projection. If LMRT=.TRUE. LAT0 should be zero.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"NLON and NLAT should satisfy the equation 5^b * 3^d * 2^e, where a-e are integers leq 0.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"~~BDNLON is number of points in x-direction for intermediate climate file. BDNLON > NLON.~~\n~~BDNLAT is number of points in y-direction for intermediate climate file. BDNLAT > NLAT.~~","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"The default area is the Denmark domain (DKCOECP). The following values for C+I zone and truncation are calculated in src/Harmonie_domains.pm from the values above. ","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"NDLUXG is number of points in x-direction without extension (E) zone.\nNDGUXG is number of points in y-direction without extension (E) zone.\nNMSMAX_LINE is truncation order in longitude. By default (NLON-2)/2. \nNSMAX_LINE is truncation order in latitude. By default (NLAT-2)/2. \nNMSMAX_QUAD is truncation order in longitude. By default (NLON-2)/3. It is used to create filtered orography with lower resolution.\nNSMAX_QUAD is truncation order in latitude. By default (NLAT-2)/3. It is used to create filtered orography with lower resolution.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"(Image: cie domain)","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"~~Note that to run with LSPSMORO=yes you have to use a linear grid. I.e. NLON/NLAT must satisfy~~","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#Domain-creation-tool-1","page":"Domain","title":"Domain creation tool","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"To help with the design of a new domain, there is an interactive tool that lets you experiment with the grid parameters described above, and visualize the resulting domain immediately on a map, see figure below.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"(Image: domain200)","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"At present, it only works for Lambert and polar stereographic projection, not rotated mercator.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#Creating-a-new-domain-1","page":"Domain","title":"Creating a new domain","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"If you are happy with your new domain created with the help of the domain creation tool you can add it to scr/Harmonie_domains.pm for your experiment, my_exp (assuming you have set up the experiment):","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie co scr/Harmonie_domains.pm\n#\n# add domain information for new domain called MYNEWDOM in this file\n#\nvi scr/Harmonie_domains.pm\n#\n# set DOMAIN=MYNEWDOM in the experiment config file\n#\nvi ecf/config_exp.h ","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"You can now start a new experiment with a newly defined domain called MYNEWDOM.","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#Create-a-test-domain-with-gl-1","page":"Domain","title":"Create a test domain with gl","text":"","category":"section"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"Before you go through the full climate generation process you can generate a test domain using gl. Define your domain in the namelist like:","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"&NAMINTERP\nOUTGEO%NLON = 300 ,\nOUTGEO%NLAT = 300,\nOUTGEO%PROJECTION = 3,\nOUTGEO%WEST = 17.0,\nOUTGEO%SOUTH = 58.0,\nOUTGEO%DLON = 2500.0\nOUTGEO%DLAT = 2500.0\nOUTGEO%PROJLAT = 60.0\nOUTGEO%PROJLAT2 = 60.0\nOUTGEO%PROJLON = 0.0,\n/","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"Running gl using this namelist by","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"gl -n namelist_file","category":"page"},{"location":"ExperimentConfiguration/ModelDomain/#","page":"Domain","title":"Domain","text":"will create an GRIB file with a constant orography which you can use for plotting. ","category":"page"},{"location":"QuickStart/QuickStartLocal/#Running-Harmonie-on-your-local-platform-1","page":"Local","title":"Running Harmonie on your local platform","text":"","category":"section"},{"location":"QuickStart/QuickStartLocal/#Introduction-1","page":"Local","title":"Introduction","text":"","category":"section"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"These \"quick start instructions\" assumes that someone has already put in place a valid configuration for your local platform, CONFIG=linux.local for example.","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"The Harmonie system runs through a number of steps to help you complete your experiment. The chain can be summarized like:","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Configure and start the experiment: This is where you define your domain, choose your settings and specify the period for your experiment.","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Once you have done this you can start the system and let it create the basic infrastructure","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Setup the necessary directories and copy the system files needed (InitRun, Prepare_cycle)\nCompile the binaries you need to run your experiment (Build)\nCreate the constant climate files specifying your domain (Climate)","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"With the basic setup and files in place we can proceed to the integration part where we have three loops taking care of ","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Prepare boundaries and observations (MakeCycleInput)\nRun assimilation and forecasts (Date)\nPost process and archive the result (Postprocessing)","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"The three different task are allowed to run ahead/after each other to get a good throughput.","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"The configuration, the full suite and the relation between different tasks is controlled by the scheduler. This documentation describes how to get started with your first experiment. The description is general for a single host. (The reference Harmonie system on ECMWF platform assumes a dual-hosts setup with the front-end ecgb used to configure and launch experiments and cca is used for all computations except those for operations related to observation verification and monitoring. ","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Following example shows the steps to launch an Harmonie experiment my_exp.","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"If this is the first time to install HARMONIE on your local platform please take a look at the basic install instructions here. ","category":"page"},{"location":"QuickStart/QuickStartLocal/#Configure-your-experiment-1","page":"Local","title":"Configure your experiment","text":"","category":"section"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Create an experiment directory under HOME/hmhome and use the master script Harmonie to set up a minimum environment for your experiment.  ```bash  mkdir -p HOME/hmhome/myexp  cd HOME/hmhome/myexp  PATHTOHARMONIE/config-sh/Harmonie setup -r PATHTO_HARMONIE -h YOURHOST  ```","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"where","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"-r is the path to your downloaded version of HARMONIE\n-h tells which configuration files to use. At ECMWF config.ecgb is the default one. List PATH_TO_HARMONIE/config-sh/config.* for available HOST configurations\nThis setup command  provides the default setup which currently is AROME physics with CANARI+OI_MAIN surface assimilation and 3DVAR upper air assimilations with 3h cycling on a domain covering Denmark using 2.5km horizontal resolution and 65 levels in the vertical.\nNow you can edit the basic configuration file ecf/config_exp.h to configure your experiment scenarios. Modify specifications for model domain, physics (AROME, ALARO), data locations, settings for dynamics, physics, domain, coupling host model etc. Read more about the options in here. You can also use some of the predefined configurations by calling Harmonie with the -c option:","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"mkdir $HOME/hm_home/my_exp\ncd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie setup -r PATH_TO_HARMONIE -h YOURHOST -c CONFIG ","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"where CONFIG is one of the setups defined in scr/Harmonie_configurations.pm. If you give -c with out an argument or a non existing configuration a list of configurations will be printed.","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"In some cases you might have to edit the general system configuration file, Env_system. See here for further information\nThe rules for how to submit jobs are defined in Env_submit. See here for further information\nIf you experiment in data assimilation you might also want to change scr/include.ass","category":"page"},{"location":"QuickStart/QuickStartLocal/#Start-your-experiment-1","page":"Local","title":"Start your experiment","text":"","category":"section"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Launch the experiment by giving start time, DTG, end time, DTGEND, and forecast length, LL","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie start DTG=YYYYMMDDHH DTGEND=YYYYMMDDHH LL=12\n# e.g., PATH_TO_HARMONIE/Harmonie start DTG=2012122400 DTGEND=2012122406 LL=12","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"If you would like to only run long forecasts at 00/12 UTC and short (3h) at 03/06/09/15/18/21, you specify the longer forecast length as LLMAIN. ","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie start DTG=2012122400 LLMAIN=24","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"If successful, ecflow will identify your experiment name and start building your binaries and run your forecast. If not, you need to examine the mSMS log file $HM_DATA/mSMS.log. $HM_DATA is defined in your Env_system file. At ECMWF $HM_DATA=$SCRATCH/hm_home/$EXP where $EXP is your experiment name. Read more about where things happen further down.","category":"page"},{"location":"QuickStart/QuickStartLocal/#Continue-your-experiment-1","page":"Local","title":"Continue your experiment","text":"","category":"section"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"If your experiment have successfully completed and you would like to continue for another period you should write","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie prod DTGEND=YYYYMMDDHH LL=12 ","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"By using prod you tell the system that you are continuing the experiment and using the first guess from the previous cycle. The start date is take from a file progress.log created in your $HOME/hm_home/my_exp directory. If you would have used start the initial data would have been interpolated from the boundaries, a cold start in other words.","category":"page"},{"location":"QuickStart/QuickStartLocal/#Start/Restart-of-mXCdp-1","page":"Local","title":"Start/Restart of mXCdp","text":"","category":"section"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"To start the graphical window for mSMS on ecgb type","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie mon","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"The graphical window, mXCdp runs independently of the mSMS job and can be closed and restarted again with the same command. With the graphical interface you can control and view logfiles of each task. ","category":"page"},{"location":"QuickStart/QuickStartLocal/#Making-local-changes-1","page":"Local","title":"Making local changes","text":"","category":"section"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Very soon you will find that you need to do changes in a script or in the source code. Once you have identified which file to edit you put it into the current $HOME/hm_home/my_exp directory, with exactly the same subdirectory structure as in the reference. e.g, if you want to modify a namelist setting ","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"cd $HOME/hm_home/my_exp\nPATH_TO_HARMONIE/config-sh/Harmonie co nam/harmonie_namelists.pm         # retrieve default namelist harmonie_namelists.pm\nvi nam/harmonie_namelists.pm                        # modify the namelist","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Next time you run your experiment the changed file will be used. You can also make changes in a running experiment. Make the change you wish and rerun the InitRun task in the mXCdp window. The !InitRun task copies all files from your local experiment directory to your working directory $HM_DATA. Once your InitRun task is complete your can rerun the task you are interested in. If you wish to recompile something you will also have to rerun the Build tasks. Read more about how to control and rerun tasks in mini-SMS from mXCdp.","category":"page"},{"location":"QuickStart/QuickStartLocal/#Directory-structure-1","page":"Local","title":"Directory structure","text":"","category":"section"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"On most platforms HARMONIE compiles and produces all its output data under HM_DATA (defined in ~/hmhome/myexp/Env_system)","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"= Description                            = = Location                                                                                  =\nBinaries $BINDIR (set in ecf/config_exp.h ), default is $HM_DATA/bin\nlibraries, object files & source code $HM_DATA/lib/src if MAKEUP=yes, $HMDATA/gmkpack_build if MAKEUP=no\nScripts $HM_DATA/lib/scr\nconfig files (Envsystem & Envsystem $HM_DATA/lib linked to files in $HM_DATA/config-sh\nsms $HM_DATA/lib/sms\nmsms definitions $HM_DATA/lib/msms\nUtilities such as gmkpack, gl & monitor $HM_DATA/lib/util\nClimate files $HM_DATA/climate\nWorking directory for the current cycle $HM_DATA/YYYYMMDD_HH\nArchived files $HM_DATA/archive\nArchived cycle output $HM_DATA/archive/YYYY/MM/DD/HH\nArchived log files $HM_DATA/archive/log/HM_TaskFamily_YYYYMMDDHH.html where TaskFamily=MakeCycleInput,Date,Postprocessing\nTask log files $JOBOUTDIR (set in Env_system) usually $HM_DATA/sms_logfiles\nVerification data (vfld/vobs/logmonitor) $HM_DATA/archive/extract\nVerification (monitor) results $HM_DATA/archive/extract/WebgraF\n\"Fail\" directory $HM_DATA/YYYYMMDD_HH/Failed_Family_Task (look at ifs.stat,NODE.001_01, fort.4","category":"page"},{"location":"QuickStart/QuickStartLocal/#Archive-contents-1","page":"Local","title":"Archive contents","text":"","category":"section"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"$HM_DATA/archive/YYYY/MM/DD/HH is used to store \"archived\" output from HARMONIE cycles. The level of archiving depends on ARSTRATEGY in ecf/config_exp.h . The default setting is medium which will keep the following cycle data:","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Surface analysis: ICMSHANAL+0000\nAtmospheric analysis result: MXMIN1999+0000\nBlending between surface/atmospheric analysis and cloud variable from the first guess: ANAB1999+0000\nICMSHHARM+NNNN and ICMSHHARM+NNNN.sfx are atmospheric and surfex forecast output files\nPFHARM* files produced by the inline postprocessing \nGRIB files produced by the conversion of FA output files to GRIB if MAKEGRIB=yes in ecf/config_exp.h \nODB databases and feedback information in odb_stuff.tar","category":"page"},{"location":"QuickStart/QuickStartLocal/#Cleanup-of-old-experiments-1","page":"Local","title":"Cleanup of old experiments","text":"","category":"section"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Once you have complete your experiment you may wish to remove code, scripts and data from the disks. Harmonie provides some simple tools to do this. First check the content of the different disks by","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":" Harmonie CleanUp -ALL","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"Once you have convinced yourself that this is OK you can proceed with the removal.","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":" Harmonie CleanUp -ALL -go ","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"If you would like to exclude the data stored  HM_DATA ( as defined in Env_system ) you run ","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":" Harmonie CleanUp -d","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"to list the directories intended for cleaning. Again, convince yourself that this is OK and proceed with the cleaning by","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":" Harmonie CleanUp -d -go","category":"page"},{"location":"QuickStart/QuickStartLocal/#","page":"Local","title":"Local","text":"NOTE that these commands may not work properly in all versions. Do not run the removal before you're sure it's OK","category":"page"},{"location":"System/ECMWF_teleport/#Monitoring-Harmonie-suites-1","page":"Teleport","title":"Monitoring Harmonie suites","text":"","category":"section"},{"location":"System/ECMWF_teleport/#","page":"Teleport","title":"Teleport","text":"In order to monitor the progress of your Harmonie suite(s) at ECMWF the ecFlow GUI ecflow_ui can be used directly from your local PC/server. This relies on teleport and ssh port forwarding which is described in more detail below. ","category":"page"},{"location":"System/ECMWF_teleport/#Open-Teleport-connection-1","page":"Teleport","title":"Open Teleport connection","text":"","category":"section"},{"location":"System/ECMWF_teleport/#","page":"Teleport","title":"Teleport","text":"This relies on a Teleport connection to ECMWF. Further details on Teleport are available here:","category":"page"},{"location":"System/ECMWF_teleport/#","page":"Teleport","title":"Teleport","text":"[itops@reaserve ~]$ tsh status\n> Profile URL:  https://shell.ecmwf.int:443\n  Logged in as: itops@met.ie\n  Cluster:      shell.ecmwf.int\n  Roles:        *\n  Logins:       duit\n  Valid until:  2021-03-23 22:00:35 +0000 UTC [valid for 11h21m0s]\n  Extensions:   permit-X11-forwarding, permit-agent-forwarding, permit-port-forwarding, permit-pty\n\n\n* RBAC is only available in Teleport Enterprise\n  https://gravitational.com/teleport/docs/enterprise\n[itops@reaserve ~]$ ","category":"page"},{"location":"System/ECMWF_teleport/#","page":"Teleport","title":"Teleport","text":"In order to open a new Teleport connection execute the following and submit credential via browser:","category":"page"},{"location":"System/ECMWF_teleport/#","page":"Teleport","title":"Teleport","text":"[ewhelan@reaserve ~]$ tsh login --proxy=shell.ecmwf.int:433","category":"page"},{"location":"System/ECMWF_teleport/#Log-in-1","page":"Teleport","title":"Log in","text":"","category":"section"},{"location":"System/ECMWF_teleport/#","page":"Teleport","title":"Teleport","text":"To log in to ecgate:","category":"page"},{"location":"System/ECMWF_teleport/#","page":"Teleport","title":"Teleport","text":"[itops@reaserve ~]$ ssh -X ecgate","category":"page"},{"location":"System/ECMWF_teleport/#","page":"Teleport","title":"Teleport","text":"The Teleport connection to ecgate (and cca/ccb/tems) is configured as follows:","category":"page"},{"location":"System/ECMWF_teleport/#","page":"Teleport","title":"Teleport","text":"[ewhelan@reaserve ~]$ cat .ssh/config \nHost ecgate cca ccb tems\n  User dui\n  IdentityFile ~/.tsh/keys/shell.ecmwf.int/eoin.whelan@met.ie\n  ProxyCommand bash -c \"tsh login; ssh -W %h:%p %r@shell.ecmwf.int\"\n[ewhelan@reaserve ~]$ ","category":"page"},{"location":"System/ECMWF_teleport/#Open-ecFlow-ports-1","page":"Teleport","title":"Open ecFlow ports","text":"","category":"section"},{"location":"System/ECMWF_teleport/#","page":"Teleport","title":"Teleport","text":"The following opens ports to ECMWF (dui, ECF_PORT=3710) ecFlow server. Based on instructions provided by [https://confluence.ecmwf.int/display/ECFLOW/Teleport+-+using+local+ecflow_ui]. In a new terminal:","category":"page"},{"location":"System/ECMWF_teleport/#","page":"Teleport","title":"Teleport","text":"ssh ecgate -C -N -L 3710:ecgb-vecf:3710","category":"page"},{"location":"Overview/Source/#Harmonie-Source-Code-1","page":"Source","title":"Harmonie Source Code","text":"","category":"section"},{"location":"Overview/Source/#Introduction-1","page":"Source","title":"Introduction","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"This wiki page summaries the ARPEGE/IFS source code made available in the HARMONIE system. It is based on documents made available by YESSAD K. (METEO-FRANCE/CNRM/GMAP/ALGO). The relevant document for cycle 40 is available here (or directly here). Documents for other versions are available in the \"References and documentation\" section. This page used the 40T1 document.","category":"page"},{"location":"Overview/Source/#HARMONIE-Source-Library-Structure-1","page":"Source","title":"HARMONIE Source Library Structure","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"The main source of HARMONIE system originates from IFS/ARPEGE and it consists of a number of \"project\" sources. These are:","category":"page"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"aeolus: Aeolous source code, a package for pre-processing satellite lidar wind data. Inactive for us.\naladin: specific routines only relevant to LAM, (limited area models, in particular ALADIN and AROME).\nalgor: application routines, e.g. to read LFI or Arpege files,interface routines for distributed memory environment, some linear algebra routines, such as lanczos algorithm, minimizers.\narpifs: global model routines (ARPEGE, IFS), and routines common to global and LAM models. This is the core of the ARPEGE/IFS software. The core of ARPEGE/IFS software.\nbiper: Biperiodization routines for the LAM\nblacklist: package for blacklisting\ncoupling: lateral coupling and spectral nudging for LAM models\netrans: spectral transforms for plane geometry, used for LAM\nifsaux: some application routines, for example reading or writing on “LFI” or ARPEGE files, interface routines for distributed memory environment\nmpa: upper air meso-NH/AROME physics (also used in ARPEGE/ALADIN)\nmse: surface processes in meso-NH/AROME (interface for SURFEX)\nodb: ODB (Observational Data Base software), needed by ARPEGE/ALADIN for their analysis or their assimilation cycle\nsatrad: satellite data handling package, needed to run the model analysis/assimilation\nsurf: ECMWF surface scheme\nsurfex: surface processes in meso-NH/AROME - the externalized surface scheme SURFEX\ntrans: spectral transforms for spherical geometry, used for ARPEGE/IFS\nutilities: utility packages, for operational FA to GRIB (PROGRID), OULAN, BATOR, or programs to operate on ODB and radiances bias correction","category":"page"},{"location":"Overview/Source/#Dependencies-and-hierarchy-between-each-project-1","page":"Source","title":"Dependencies and hierarchy between each project","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"Note: these project names are no longer valid – need to update","category":"page"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"ARP+TFL+XRD+XLA+MPA+MSE+SURFEX: for ARPEGE forecasts with METEO-FRANCE physics.\nARP+ALD+TFL+TAL+XRD+XLA+BIP+MPA+MSE+SURFEX: for ALADIN or AROME forecasts.\nARP+TFL+XRD+XLA+SUR: for IFS forecasts with ECMWF physics.\nARP+TFL+XRD+XLA+MPA+MSE+SURFEX+BLA+ODB+SAT+AEO: for ARPEGE assimilations with METEO-FRANCE physics.\nARP+ALD+TFL+TAL+XRD+XLA+BIP+MPA+MSE+SURFEX+BLA+ODB+SAT+AEO: for ALADIN or AROME assimilations.\nARP+TFL+XRD+XLA+SUR+BLA+ODB+SAT+OBT+SCR+AEO: for IFS assimilations with ECMWF physics.","category":"page"},{"location":"Overview/Source/#Libraries-under-each-project-1","page":"Source","title":"Libraries under each project","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"Note: this information made need to be updated for CY40","category":"page"},{"location":"Overview/Source/#ARPIFS-1","page":"Source","title":"ARPIFS","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"adiab\nAdiabatic dynamics\nAdiabatic diagnostics and intermediate quantities calculation, for example the geopotential height (routines GP... or GNH...).\nEulerian advections\nSemi-Lagrangian advection and interpolators (routines LA...)\nSemi-implicit scheme and linear terms calculation (routines SI..., SP..SI..)\nHorizontal diffusion (routines SP..HOR..)\nald inc\nfunction: functions used only in ALADIN\nnamelist: namelists read by ALADIN.\nc9xx: specific configurations 901 to 999 routines (mainly configuration 923). Routines INCLI.. are used in configuration 923. Routines INTER... are interpolators used in configurations 923, 931, 932.\ncanari: routines used in the CANARI optimal interpolation. Their names generally starts by CA.\ncanari common: empty directory to be deleted.\nclimate: some specific ARPEGE-CLIMAT routines.\ncommon: often contains includes\ncontrol: control routines. Contains in particular STEPO and CNT... routines.\ndfi: routines used in the DFI (digital filter initialisation) algorithm\ndia: diagnostics other than FULL-POS. One finds some setup SU... routines specific to some diagnostics and some WR... routines doing file writing.\nfunction: functions (in includes). The qa....h functions are used in CANARI, the fc....h functions are used in a large panel of topics.\ninterface: not automatic interfaces (currently empty).\nkalman: Kalman filter.\nmodule: all the types of module (variables declarations, type definition, active code).\nmwave: micro-wave observations (SSM/I) treatment.* namelist: all namelists.\nnmi: routines used in the NMI (normal mode initialisation) algorithm.\nobs error: treatment of the observation errors in the assimilation.\nobs preproc: observation pre-processing (some of them are called in the screening).\nocean: oceanic coupling, for climatic applications.\nonedvar: 1D-VAR assimilation scheme used at ECMWF.\nparallel: parallel environment, communications between processors.\nparameter: empty directory to be deleted.\nphys dmn: physics parameterizations used at METEO-FRANCE, and HIRLAM physics, ALARO physics.\nphys ec: ECMWF physics. Some of these routines (FMR radiation scheme, Lopez convection scheme) are now also used in the METEO-FRANCE physics.\npointer: empty directory to be deleted.\npp obs: several applications\nobservation horizontal and vertical interpolator. \nFULL-POS. \nvertical interpolator common to FULL-POS and the observation interpolator; some of these routines may be used elsewhere.\nsetup: setup routines not linked with a very specific domain. More specific setup routines are spread among some other subdirectories.\nsinvect: singular vectors calculation (configuration 601).\nsupport: empty directory to be deleted.\ntransform: hat routines for spectral transforms.\nutility: miscellaneous utilitaries, linear algebra routines, array deallocation routines.\nvar: routines involved in the 3DVAR and 4DVAR assimilation, some minimizers (N1CG1, CONGRAD), some specific 3DVAR and 4DVAR setup routines.\nwave: empty directory to be deleted.","category":"page"},{"location":"Overview/Source/#ALADIN-1","page":"Source","title":"ALADIN","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"adiab: adiabatic dynamics.\nblending: blending scheme (currently only contains the procedure blend.ksh).\nc9xx: specific configurations E901 to E999 routines (mainly configuration E923). Routines EINCLI.. are used in configuration E923. Routines EINTER... are interpolators used in configurations E923, E931, E932.\ncontrol: control routines.\ncoupling: lateral coupling by external lateral boundary conditions.\ndia: diagnostics other than FULL-POS.\ninidata: setup routines specific to file reading (initial conditions, LBC).\nmodule: active code modules only used in ALADIN.\nobs preproc: observation pre-processing (some of them are called in the screening).\nparallel: parallel environment, communications between processors.\npp obs: several applications:\nobservation horizontal and vertical interpolator. \nFULL-POS.\nvertical interpolator common to FULL-POS and the observation interpolator; some of these routines may be used elsewhere.\nprograms: probably designed to contain procedures, but currently contains among others some blending routines, the place of which would be probably better in subdirectory \"blending\".\nsetup: setup routines not linked with a very specific domain. More specific setup routines are spread among some other subdirectories.\nsinvect: singular vectors calculation (configuration E601).\ntransform: hat routines for spectral transforms.\nutility: miscellaneous utilitaries, array deallocation routines.\nvar: routines involved in the 3DVAR and 4DVAR assimilation, some specific 3DVAR and 4DVAR setup routines.","category":"page"},{"location":"Overview/Source/#TFL-1","page":"Source","title":"TFL","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"build: contains procedures.\nexternal: routines which can be called from another project.\ninterface: not automatically generated interfaces which match with the \"external\" directory routines.\nmodule: all the types of module (variables declarations, type definition, active code).\ntpm ...F90: variable declaration + type definition modules. \nlt.... mod.F90: active code modules for Legendre transforms. \nft.... mod.F90: active code modules for Fourier transforms. \ntr.... mod.F90: active code modules for transpositions. \nsu.... mod.F90: active code modules for setup.\nprograms: specific entries which can be used for TFL code validation. These routines are not called elsewhere.   ","category":"page"},{"location":"Overview/Source/#TAL-1","page":"Source","title":"TAL","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"external: routines which can be called from another project.\ninterface: not automatically generated interfaces which match with the \"external\" directory routines.\nmodule: all the types of module (variables declarations, type definition, active code). \ntpmald ...F90: variable declaration + type definition modules. \nelt.... mod.F90: active code modules for N-S Fourier transforms. \neft.... mod.F90: active code modules for E-W Fourier transforms. \nsue.... mod.F90: active code modules for setup. \nprograms: specific entries which can be used for TAL code validation. These routines are not called elsewhere.                                                             ","category":"page"},{"location":"Overview/Source/#XRD-1","page":"Source","title":"XRD","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"arpege: empty directory to be deleted.\nbufr io: BUFR format files reading and writing.\ncma: CMA format files reading and writing.\nddh: DDH diagnostics.\nfa: ARPEGE (FA) files reading and writing.\ngrib io: ECMWF GRIB format files reading and writing.\ngrib mf: METEO-FRANCE GRIB format files reading and writing.\nioassign: empty directory to be deleted.\nlanczos: linear algebra routines for Lanczos algorithm.\nlfi: LFI format files reading and writing.\nminim: linear algebra routines for minimizations. Contains the M1QN3 (quasi-Newton) minimizer.\nmisc: miscellaneous decks.* module: all the types of module (variables declarations, type definition, active code). There are a lot of mpl...F90 modules for parallel environment (interface to MPI parallel environment).\nmrfstools: empty directory to be deleted.\nnewbufrio: empty directory to be deleted.\nnewcmaio: empty directory to be deleted.\nnot used: miscellaneous decks (unused decks to be deleted?).\npcma: empty directory to be deleted.\nsupport: miscellaneous routines. Some of them do Fourier transforms, some others do linear algebra.\nsvipc: contains only svipc.c .\nutilities: miscellaneous utilitaries.","category":"page"},{"location":"Overview/Source/#SUR-1","page":"Source","title":"SUR","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"build: contains procedures.\nexternal: routines which can be called from another project.* function: specific functions.\ninterface: not automatically generated interfaces which match with the \"external\" directory routines.\nmodule: all the types of module (variables declarations, type definition, active code).\nyos ...F90: variable declaration + type definition modules. \nsu.... mod.F90 but not surf.... mod.F90: active code modules for setup. \nsurf.... mod.F90, v.... mod.F90: other active code modules.\noffline: specific entries which can be used for SUR code validation. These routines are not called elsewhere.","category":"page"},{"location":"Overview/Source/#BLA-1","page":"Source","title":"BLA","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"compiler.\ninclude: not automatically generated interfaces, functions, and some other includes.\nlibrary: the only containing .F90 decks.\nold2new.\nscripts.","category":"page"},{"location":"Overview/Source/#SAT-1","page":"Source","title":"SAT","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"bias.\nemiss.\ninterface.\nmodule.\nmwave.\nonedvar.\npre screen.\nrtlimb.\nrttov.\nsatim.\ntest. (Not described in detail; more information has to be provided by someone who knows the content of this project, but there is currently no specific documentation about this topic)","category":"page"},{"location":"Overview/Source/#UTI-1","page":"Source","title":"UTI","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"add cloud fields: program to add 4 cloud variables (liquid water, ice, rainfall, snow) in ARPEGE files.\nbator: BATOR software (reads observations data in a ASCII format file named OBSOUL and the blacklist, writes them on a ODB format file with some additional information).\ncombi: combination of perturbations in an ensemble forecast (PEARP).\ncontrodb: control of the number of observations.\nextrtovs: unbias TOVS.\nfcq: does quality control and writes this quality control in ODB files.\ngobptout: PROGRIB? (convert ARPEGE files contained post-processed data into GRIB files).\ninclude: all .h decks (functions, COMMON blocks, parameters).\nmandalay: software MANDALAY.\nmodule: all types of modules.\nnamelist: namelists specific to the applications stored in UTI (for example OULAN, BATOR).\noulan: OULAN software (the step just before BATOR: observation extractions in the BDM, samples data in space and time, and writes the sampled data in an ASCII file called \"OBSOUL\").\npregpssol: Surface GPS processing.\nprescat: Scatterometer data processing.\nprogrid: PROGRID? (convert ARPEGE files contained post-processed data into GRIB files).\nprogrid cadre: cf. progrid?\nsst nesdis: program to read the SST on the BDAP. This project has its own entries.","category":"page"},{"location":"Overview/Source/#MPA-1","page":"Source","title":"MPA","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"It contains first layer of directory","category":"page"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"chem: chemistry.\nconv: convection.\nmicro: microphysics.\nturb: turbulence.","category":"page"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"Each directory contains the following subdirectories","category":"page"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"externals: routines which can be called from another project.\ninclude: all the \"include\" decks (functions, COMMON blocks, parameters).\ninterface: not automatically generated interfaces which match with the \"external\" directory routines.\ninternals: other non-module routines; they cannot be called from another project.\nmodule: all types of modules.","category":"page"},{"location":"Overview/Source/#SURFEX-1","page":"Source","title":"SURFEX","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"ASSIM: Surface assimilation routines (please note that programs soda.F90, oi_main.F90 and varassim.F90 are located under mse/programs).\nOFFLIN: Surface offline routines (please note that programs pgd.F90, prep.F90 and offline.F90 are located under mse/programs).\nSURFEX: Surface routines for physiography (PGD), initialisation (PREP) and physical processes including e.g. land (ISBA), sea, town (TEB) and lakes.\nTOPD: TOPMODEL (TOPography based MODEL) for soil hydrology.\nTRIP: River routing model TRIP","category":"page"},{"location":"Overview/Source/#MSE-1","page":"Source","title":"MSE","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"dummy: empty versions of some routines.\nexternals: routines which can be called from another project.\ninterface: not automatically generated interfaces which match with the \"external\" directory routines.\ninternals: other non-module routines; they cannot be called from another project.\nmodule: all types of modules.\nnew: file conversion routines, e.g. fa2lfi, lfi2fa\nprograms: SURFEX programs","category":"page"},{"location":"Overview/Source/#References-and-documentation-1","page":"Source","title":"References and documentation","text":"","category":"section"},{"location":"Overview/Source/#","page":"Source","title":"Source","text":"Yessad K.: Jul 2015: LIBRARY ARCHITECTURE AND HISTORY OF THE TECHNICAL ASPECTS IN ARPEGE/IFS, ALADIN AND AROME IN THE CYCLE 42 OF ARPEGE/IFS\nYessad K.: Mar 2014: LIBRARY ARCHITECTURE AND HISTORY OF THE TECHNICAL ASPECTS IN ARPEGE/IFS, ALADIN AND AROME IN THE CYCLE 40T1 OF ARPEGE/IFS\nYessad K.: Jul 2013: LIBRARY ARCHITECTURE AND HISTORY OF THE TECHNICAL ASPECTS IN ARPEGE/IFS, ALADIN AND AROME IN THE CYCLE 40 OF ARPEGE/IFS\nYessad K.: Nov 2011: BASICS ABOUT ARPEGE/IFS, ALADIN AND AROME IN THE CYCLE 38 OF ARPEGE/IFS\nYessad K.: Nov 2011: LIBRARY ARCHITECTURE AND HISTORY OF THE TECHNICAL ASPECTS IN ARPEGE/IFS, ALADIN AND AROME IN THE CYCLE 38 OF ARPEGE/IFS","category":"page"},{"location":"Visualization/EPyGrAM/#EPyGrAM-1","page":"EpyGram","title":"EPyGrAM","text":"","category":"section"},{"location":"Visualization/EPyGrAM/#General-1","page":"EpyGram","title":"General","text":"","category":"section"},{"location":"Visualization/EPyGrAM/#","page":"EpyGram","title":"EpyGram","text":"EPyGram wiki\nVortex information","category":"page"},{"location":"Visualization/EPyGrAM/#Using-EPyGrAM-(version-1.3.6)-at-ecgate-1","page":"EpyGram","title":"Using EPyGrAM (version 1.3.6) at ecgate","text":"","category":"section"},{"location":"Visualization/EPyGrAM/#","page":"EpyGram","title":"EpyGram","text":"Set up EPyGrAM configuration (might be outdated, info is for version 1.2.1):\ncd $HOME\nmkdir .epygram\ncp ~hlam/.epygram/* .epygram\nSet up Vortex and EPyGrAM environment information by adding the following lines to the end of $HOME/.user_bashrc (or similar):\n# Vortex & Footprints\nVORTEX_INSTALL_DIR=/home/ms/spsehlam/hlam/local/EPyGrAM/vortex-1.0.0\nexport PYTHONPATH=$PYTHONPATH:$VORTEX_INSTALL_DIR:$VORTEX_INSTALL_DIR/src:$VORTEX_INSTALL_DIR/site\n\n# EPyGrAM\nEPYGRAM_INSTALL_DIR=/home/ms/spsehlam/hlam/local/EPyGrAM/EPyGrAM-1.3.6\nexport PYTHONPATH=$PYTHONPATH:$EPYGRAM_INSTALL_DIR:$EPYGRAM_INSTALL_DIR/site\nexport PATH=$PATH:$EPYGRAM_INSTALL_DIR/apptools\nexport LD_LIBRARY_PATH=$EPYGRAM_INSTALL_DIR/site/arpifs4py:/usr/local/apps/openmpi/2.1.3/GNU/6.3.0/lib:$LD_LIBRARY_PATH\nOpen a new shell or source $HOME/.user_bashrc \nTest:\nepy_plot.py -f shortName:t,level:802 ~hlam/EPyGRAM/examples/fc2020052500+000grib_sfx\nepy_section.py -f shortName:t,typeOfLevel:heightAboveGround -s'-8,53' -e'-7,53' \n~hlam/EPyGRAM/examples/fc2020052500+000grib_sfx\ndomain_maker.py","category":"page"},{"location":"Visualization/EPyGrAM/#","page":"EpyGram","title":"EpyGram","text":"Enjoy!","category":"page"},{"location":"System/GitDeveloperDocumentation/#Developing-in-the-Hirlam-GitHub-organization-1","page":"GitHub","title":"Developing in the Hirlam GitHub organization","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#Introduction-1","page":"GitHub","title":"Introduction","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Since 2018 and CY43 HIRLAM have used git for code revision control and gitolite as the git server on hirlam.org. HIRLAM is now moving to using Github for ''software development and version control''. This page provides information on how to access the GitHub Hirlam organisation and how to commit your developments, specifically Harmonie. As was the case with hirlam.org's gitolite a fork-and-branch workflow will be used to manage developments.","category":"page"},{"location":"System/GitDeveloperDocumentation/#Becoming-a-member-of-Hirlam-1","page":"GitHub","title":"Becoming a member of Hirlam","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Create a GitHub account: https://github.com – click on Sign up. Details here\nAdd your public ssh key(s) to the account. Details here\nContact your friendly System-core to be invited to the GitHub Hirlam organisation","category":"page"},{"location":"System/GitDeveloperDocumentation/#Fork-and-branch-1","page":"GitHub","title":"Fork and branch","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"You can create a user fork of Harmonie by doing the following:","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Go to https://github.com/Hirlam/Harmonie\nClick on Fork to create a fork of Harmonie for your user (USER)\nClone your fork:","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ git clone git@github.com:USER/Harmonie.git $HOME/git_harmonie/USER/Harmonie","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Further information is available here: https://docs.github.com/en/github/getting-started-with-github/fork-a-repo ","category":"page"},{"location":"System/GitDeveloperDocumentation/#Keep-your-fork-synced-1","page":"GitHub","title":"Keep your fork synced","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"In a terminal change directory to the clone of you fork:","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ cd $HOME/git_harmonie/USER/Harmonie","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"List the current configured remote repository for your fork.","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ git remote -v\norigin\tgit@github.com:USER/Harmonie.git (fetch)\norigin\tgit@github.com:USER/Harmonie.git (push)","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Specify a new remote upstream repository that will be synced with the fork.","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ git remote add upstream git@github.com:Hirlam/Harmonie.git","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Verify the new upstream repository you've specified for your fork.","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ git remote -v\norigin\tgit@github.com:ewhelan/Harmonie.git (fetch)\norigin\tgit@github.com:ewhelan/Harmonie.git (push)\nupstream\tgit@github.com:Hirlam/Harmonie.git (fetch)\nupstream\tgit@github.com:Hirlam/Harmonie.git (push)","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Update develop in your fork.","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ git checkout develop\n$ git pull upstream develop\n$ git push origin develop","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Update pre-CY46h1 in your fork.","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ git checkout pre-CY46h1\n$ git pull upstream pre-CY46h1\n$ git push origin pre-CY46h1","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Further information is available here: https://docs.github.com/en/github/getting-started-with-github/fork-a-repo#keep-your-fork-synced","category":"page"},{"location":"System/GitDeveloperDocumentation/#Contributing-developments-–-develop-1","page":"GitHub","title":"Contributing developments – develop","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Contributions for develop should be developed in your Harmonie fork in a development branch (feature/bugfix/...). The following details how you can get your development upstream.","category":"page"},{"location":"System/GitDeveloperDocumentation/#Create-a-feature-branch-1","page":"GitHub","title":"Create a feature branch","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ cd $HOME/git_harmonie/USER/Harmonie\n$ git checkout develop\n$ git checkout -b feature/descriptive_name_for_developments","category":"page"},{"location":"System/GitDeveloperDocumentation/#Keep-up-to-date-with-develop-1","page":"GitHub","title":"Keep up to date with develop","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Sync your fork as described above\nMerge develop updates in to your branch","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ cd $HOME/git_harmonie/USER/Harmonie\n$ git checkout feature/descriptive_name_for_developments\n$ git merge develop","category":"page"},{"location":"System/GitDeveloperDocumentation/#Creating-a-pull-request-1","page":"GitHub","title":"Creating a pull request","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Once you have committed to your feature branch and wanted them included in the upstream repo you should do the following:","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Push your branch to your fork:","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ cd $HOME/git_harmonie/USER/Harmonie\n$ git checkout feature/descriptive_name_for_developments\n$ git push origin feature/descriptive_name_for_developments","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"When you push your branch information on how to create a pull request will be presented:","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"remote: Resolving deltas: 100% (x/x), completed with x local objects.\nremote: \nremote: Create a pull request for 'feature/descriptive_name_for_developments' on GitHub by visiting:\nremote:      https://github.com/USER/Harmonie/pull/new/feature/descriptive_name_for_developments\nremote: ","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Follow this link","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"request a reviewer\nadd labels to the developemnt (feature/enhancement/...)\nadd comments to help with the review process (Testbed members used/Changes expected if any/...)","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Once the pull request has been approved by the System-core team it will be merged in to develop","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Further information is available here: https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request","category":"page"},{"location":"System/GitDeveloperDocumentation/#Contributing-developments-–-pre-CY46h1-1","page":"GitHub","title":"Contributing developments – pre-CY46h1","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Contributions to pre-CY46h1 should be developed in your Harmonie fork in a development branch (feature/bugfix/...). The following details how you can get your development ''upstream''.","category":"page"},{"location":"System/GitDeveloperDocumentation/#Create-a-''feature''-branch-1","page":"GitHub","title":"Create a ''feature'' branch","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ cd $HOME/git_harmonie/USER/Harmonie\n$ git checkout pre-CY46h1\n$ git checkout -b feature/descriptive_name_for_developments","category":"page"},{"location":"System/GitDeveloperDocumentation/#Keep-up-to-date-with-pre-CY46h1-1","page":"GitHub","title":"Keep up to date with pre-CY46h1","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Sync your fork as described above\nMerge develop updates in to your branch","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ cd $HOME/git_harmonie/USER/Harmonie\n$ git checkout feature/descriptive_name_for_developments\n$ git merge pre-CY46h1","category":"page"},{"location":"System/GitDeveloperDocumentation/#Creating-a-pull-request-2","page":"GitHub","title":"Creating a pull request","text":"","category":"section"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Once you have committed to your feature branch and wanted them included in the upstream repo you should do the following:","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Push your branch to your fork:","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"$ cd $HOME/git_harmonie/USER/Harmonie\n$ git checkout feature/descriptive_name_for_developments\n$ git push origin feature/descriptive_name_for_developments","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"When you push your branch information on how to create a pull request will be presented:","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"remote: Resolving deltas: 100% (x/x), completed with x local objects.\nremote: \nremote: Create a pull request for 'feature/descriptive_name_for_developments' on GitHub by visiting:\nremote:      https://github.com/USER/Harmonie/pull/new/feature/descriptive_name_for_developments\nremote: ","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Follow this link","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"request a reviewer\nadd labels to the developemnt (feature/enhancement/...)\nadd comments to help with the review process (Testbed members used/Changes expected if any/...)","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Once the pull request has been approved by the System-core team it will be merged in to develop","category":"page"},{"location":"System/GitDeveloperDocumentation/#","page":"GitHub","title":"GitHub","text":"Further information is available here: https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request","category":"page"},{"location":"System/GitDeveloperDocumentation/#Moving-my-branches-from-hirlam.org-1","page":"GitHub","title":"Moving my branches from hirlam.org","text":"","category":"section"}]
}
